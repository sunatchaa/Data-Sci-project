{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dear/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse JSON response\n",
    "def parse_scopus_response(json_response):\n",
    "    entries = []\n",
    "    for entry in json_response.get('search-results', {}).get('entry', []):\n",
    "        cover_date = entry.get('prism:coverDate')\n",
    "        # Ensure cover_date is present and not in the future\n",
    "        if cover_date and datetime.strptime(cover_date, \"%Y-%m-%d\").date() <= datetime.now().date():\n",
    "            affiliations = entry.get('affiliation', [])\n",
    "            affiliation_info = []\n",
    "            for aff in affiliations:\n",
    "                aff_name = aff.get('affilname', 'N/A')\n",
    "                aff_city = aff.get('affiliation-city', 'N/A')\n",
    "                aff_country = aff.get('affiliation-country', 'N/A')\n",
    "                affiliation_info.append({\n",
    "                    'name': aff_name,\n",
    "                    'city': aff_city,\n",
    "                    'country': aff_country\n",
    "                })\n",
    "\n",
    "            article = {\n",
    "                'title': entry.get('dc:title'),\n",
    "                'author': entry.get('dc:creator'),\n",
    "                'publicationName': entry.get('prism:publicationName'),\n",
    "                'cover_date': cover_date,\n",
    "                'scopus_id': entry.get('dc:identifier'),\n",
    "                'cited_by_count': entry.get('citedby-count'),\n",
    "                'open_access': entry.get('openaccessFlag'),\n",
    "                'eid' : entry.get('eid'),\n",
    "                'aggregationType' : entry.get('prism:aggregationType', 'N/A'),\n",
    "                'affiliations': affiliation_info,\n",
    "                'link': entry.get('link')[2]['@href'] if 'link' in entry and len(entry['link']) > 0 else None\n",
    "            }\n",
    "            entries.append(article)\n",
    "    return entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch data from Scopus API with pagination\n",
    "def fetch_scopus_data(api_key, query, max_records=1000, count=25):\n",
    "    base_url = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    all_entries = []\n",
    "    start_index = 0\n",
    "\n",
    "    while len(all_entries) < max_records:\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'start': start_index,\n",
    "            'count': count,\n",
    "            'apiKey': api_key\n",
    "        }\n",
    "        headers = {'Accept': 'application/json'}\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            json_response = response.json()\n",
    "            articles = parse_scopus_response(json_response)\n",
    "            if not articles:\n",
    "                print(\"No more articles found or all articles have future dates.\")\n",
    "                break  # Stop if no articles are found in the response\n",
    "            all_entries.extend(articles)\n",
    "            start_index += count\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "\n",
    "        # Avoid exceeding the total max_records\n",
    "        if len(all_entries) >= max_records:\n",
    "            all_entries = all_entries[:max_records]\n",
    "            break\n",
    "\n",
    "    return all_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Fetch the abstracts and add them to the DataFrame\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m df_with_abstracts \u001b[38;5;241m=\u001b[39m \u001b[43mextract_abstracts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Display the updated DataFrame\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_with_abstracts)\n",
      "Cell \u001b[0;32mIn[24], line 41\u001b[0m, in \u001b[0;36mextract_abstracts\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     38\u001b[0m         abstracts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Optional delay to avoid rate limiting\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Add abstracts back to the DataFrame\u001b[39;00m\n\u001b[1;32m     44\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m abstracts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to extract abstracts for all links in a DataFrame\n",
    "def extract_abstracts(df):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    }\n",
    "    \n",
    "    abstracts = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        link = row['link']\n",
    "        try:\n",
    "            # Make a GET request to the article link\n",
    "            response = requests.get(link, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Locate the section by id \"abstractSection\"\n",
    "                abstract_section = soup.find('section', id='abstractSection')\n",
    "                \n",
    "                # Find all <p> tags within this section\n",
    "                if abstract_section:\n",
    "                    paragraphs = abstract_section.find_all('p')\n",
    "                    \n",
    "                    # Combine all text from the <p> tags\n",
    "                    abstract_text = \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "                    abstracts.append(abstract_text if abstract_text else \"Abstract not available\")\n",
    "                else:\n",
    "                    abstracts.append(\"Abstract section not found\")\n",
    "            elif response.status_code == 403:\n",
    "                abstracts.append(\"Access forbidden - Possible login required\")\n",
    "            else:\n",
    "                abstracts.append(f\"Failed to fetch page (status {response.status_code})\")\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            abstracts.append(f\"Request failed: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            abstracts.append(f\"Error: {str(e)}\")\n",
    "        \n",
    "        # Optional delay to avoid rate limiting\n",
    "        time.sleep(5)\n",
    "    \n",
    "    # Add abstracts back to the DataFrame\n",
    "    df['abstract'] = abstracts\n",
    "    return df\n",
    "# Fetch the abstracts and add them to the DataFrame\n",
    "df_with_abstracts = extract_abstracts(df)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_with_abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from playwright.sync_api import Page, expect\n",
    "\n",
    "def extract_abstact(page: Page):\n",
    "    page.goto(df['link'][0])\n",
    "    expect(page).to_have_title(df['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly pick articles\n",
    "def pick_random_articles(articles, total=1000):\n",
    "    if len(articles) < total:\n",
    "        print(f\"Warning: Only {len(articles)} articles available. Returning all.\")\n",
    "        return articles\n",
    "    return random.sample(articles, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title        author  \\\n",
      "0  Analysing trends of computational urban scienc...      Kumar D.   \n",
      "1  A landmark federal interagency collaboration t...  Justice A.C.   \n",
      "2  Regional planning: A failed or flawed project ...    Chirisa I.   \n",
      "3  Data Science and Model Predictive Control:: A ...   Morato M.M.   \n",
      "4  Assessment of the relationship between central...    Akabane S.   \n",
      "\n",
      "                        publicationName  cover_date              scopus_id  \\\n",
      "0           Computational Urban Science  2024-12-01  SCOPUS_ID:85209789532   \n",
      "1                            JAMIA Open  2024-12-01  SCOPUS_ID:85208963031   \n",
      "2  Regional Science Policy and Practice  2024-12-01  SCOPUS_ID:85208099811   \n",
      "3            Journal of Process Control  2024-12-01  SCOPUS_ID:85207933325   \n",
      "4                    Scientific Reports  2024-12-01  SCOPUS_ID:85207210995   \n",
      "\n",
      "  cited_by_count  open_access                 eid aggregationType  \\\n",
      "0              0         True  2-s2.0-85209789532         Journal   \n",
      "1              0         True  2-s2.0-85208963031         Journal   \n",
      "2              0         True  2-s2.0-85208099811         Journal   \n",
      "3              0        False  2-s2.0-85207933325         Journal   \n",
      "4              0         True  2-s2.0-85207210995         Journal   \n",
      "\n",
      "                                        affiliations  \\\n",
      "0  [{'name': 'State University of New York Albany...   \n",
      "1  [{'name': 'VA Connecticut Healthcare System', ...   \n",
      "2  [{'name': 'University of the Free State', 'cit...   \n",
      "3  [{'name': 'UniversitÃ© Grenoble Alpes', 'city':...   \n",
      "4  [{'name': 'The University of Tokyo Hospital', ...   \n",
      "\n",
      "                                                link  \n",
      "0  https://www.scopus.com/inward/record.uri?partn...  \n",
      "1  https://www.scopus.com/inward/record.uri?partn...  \n",
      "2  https://www.scopus.com/inward/record.uri?partn...  \n",
      "3  https://www.scopus.com/inward/record.uri?partn...  \n",
      "4  https://www.scopus.com/inward/record.uri?partn...  \n"
     ]
    }
   ],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"c8690de363626d560cf56cc17f9369d6\"\n",
    "    QUERY = 'TITLE(\"data science\")'\n",
    "    TOTAL_ARTICLES = 1000\n",
    "\n",
    "    # Step 1: Fetch data from API\n",
    "    articles = fetch_scopus_data(API_KEY, QUERY, max_records=TOTAL_ARTICLES)\n",
    "\n",
    "    # Step 2: Extract abstracts using EIDs\n",
    "    # articles_with_abstracts = extract_abstracts_via_eid(articles)\n",
    "\n",
    "    # Step 3: Save data to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    # df = pd.DataFrame(articles_with_abstracts)\n",
    "    print(df.head())  # Print the first few rows for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 11)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kv/cjr920_13657jf274w2_22t00000gn/T/ipykernel_1235/340877856.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marticle_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# If the article has a valid link, try to extract the abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'link'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Use the 'link' directly for the abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_abstracts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass the link to fetch abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "article_names = []\n",
    "\n",
    "for article in articles:\n",
    "    # If the article has a valid link, try to extract the abstract\n",
    "    if df['link']:  # Use the 'link' directly for the abstract\n",
    "        abstract = extract_abstracts(df)  # Pass the link to fetch abstract\n",
    "        article_names.append(abstract)  # Collect abstracts\n",
    "    else:\n",
    "        article_names.append(\"No link available\")  # Add a placeholder if no link is present\n",
    "\n",
    "# Print the first 10 abstracts as a preview\n",
    "print(article_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85209789532&origin=inward'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.scopus.com/inward/record.uri?partn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from playwright.sync_api import Page, expect\n",
    "\n",
    "def extract_abstact(page: Page):\n",
    "    page.goto(df['link'][0])\n",
    "    expect(page).to_have_title(df['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Example text\n",
    "text = \"Data Science in Supporting Hotel Management: An Application of Deep Learning Techniques in 2024!\"\n",
    "\n",
    "# Define a regular expression pattern to extract words\n",
    "words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "# Remove stopwords by checking if each word is not in the stopword list\n",
    "filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Display the filtered words\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_title  \\\n",
      "0    [Data, Science, Supporting, Hotel, Management,...   \n",
      "1    [Data, science, Analysis, Using, Deep, Learnin...   \n",
      "2    [Data, science, based, reconstruction, D, memb...   \n",
      "3    [Data, science, sustainable, entrepreneurship,...   \n",
      "4    [Applying, data, science, methodologies, artif...   \n",
      "..                                                 ...   \n",
      "995  [Data, Science, literacy, future, security, in...   \n",
      "996  [Predictive, modeling, heat, formation, sulfur...   \n",
      "997  [Data, Science, Model, Predictive, Control, su...   \n",
      "998  [Machine, learning, data, science, techniques,...   \n",
      "999  [Harnessing, Data, Science, Produced, Water, E...   \n",
      "\n",
      "                               cleaned_publicationName  \n",
      "0           [Smart, Innovation, Systems, Technologies]  \n",
      "1    [International, Conference, Knowledge, Enginee...  \n",
      "2                         [Journal, Membrane, Science]  \n",
      "3         [Technological, Forecasting, Social, Change]  \n",
      "4               [American, Journal, Medical, Genetics]  \n",
      "..                                                 ...  \n",
      "995  [Journal, Policing, Intelligence, Counter, Ter...  \n",
      "996                [European, Physical, Journal, Plus]  \n",
      "997                        [Journal, Process, Control]  \n",
      "998  [Machine, Learning, Data, Science, Techniques,...  \n",
      "999  [Society, Petroleum, Engineers, SPE, Water, Li...  \n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Extract words (alphabetic characters only)\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "    return filtered_words # Join words back into a string\n",
    "\n",
    "# Apply the clean_text function to the 'title' and 'name' columns\n",
    "df['cleaned_title'] = df['title'].apply(lambda x: clean_text(str(x)))\n",
    "df['cleaned_publicationName'] = df['publicationName'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "print(df[['cleaned_title', 'cleaned_publicationName']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"scopus_data.csv\"\n",
    "df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"scopus_data.csv\"\n",
    "df.to_csv(output_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse JSON response\n",
    "def parse_scopus_response(json_response):\n",
    "    entries = []\n",
    "    for entry in json_response.get('search-results', {}).get('entry', []):\n",
    "        cover_date = entry.get('prism:coverDate')\n",
    "        # Ensure cover_date is present and not in the future\n",
    "        if cover_date and datetime.strptime(cover_date, \"%Y-%m-%d\").date() <= datetime.now().date():\n",
    "            affiliations = entry.get('affiliation', [])\n",
    "            affiliation_info = []\n",
    "            for aff in affiliations:\n",
    "                aff_name = aff.get('affilname', 'N/A')\n",
    "                aff_city = aff.get('affiliation-city', 'N/A')\n",
    "                aff_country = aff.get('affiliation-country', 'N/A')\n",
    "                affiliation_info.append({\n",
    "                    'name': aff_name,\n",
    "                    'city': aff_city,\n",
    "                    'country': aff_country\n",
    "                })\n",
    "\n",
    "            article = {\n",
    "                'title': entry.get('dc:title'),\n",
    "                'author': entry.get('dc:creator'),\n",
    "                'publicationName': entry.get('prism:publicationName'),\n",
    "                'cover_date': cover_date,\n",
    "                'scopus_id': entry.get('dc:identifier'),\n",
    "                'cited_by_count': entry.get('citedby-count'),\n",
    "                'open_access': entry.get('openaccessFlag'),\n",
    "                'eid' : entry.get('eid'),\n",
    "                'aggregationType' : entry.get('prism:aggregationType', 'N/A'),\n",
    "                'affiliations': affiliation_info,\n",
    "                'link': entry.get('link')[2]['@href'] if 'link' in entry and len(entry['link']) > 0 else None\n",
    "            }\n",
    "            entries.append(article)\n",
    "    return entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch data from Scopus API with pagination\n",
    "def fetch_scopus_data(api_key, query, max_records=1000, count=25):\n",
    "    base_url = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    all_entries = []\n",
    "    start_index = 0\n",
    "\n",
    "    while len(all_entries) < max_records:\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'start': start_index,\n",
    "            'count': count,\n",
    "            'apiKey': api_key\n",
    "        }\n",
    "        headers = {'Accept': 'application/json'}\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            json_response = response.json()\n",
    "            articles = parse_scopus_response(json_response)\n",
    "            if not articles:\n",
    "                print(\"No more articles found or all articles have future dates.\")\n",
    "                break  # Stop if no articles are found in the response\n",
    "            all_entries.extend(articles)\n",
    "            start_index += count\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "\n",
    "        # Avoid exceeding the total max_records\n",
    "        if len(all_entries) >= max_records:\n",
    "            all_entries = all_entries[:max_records]\n",
    "            break\n",
    "\n",
    "    return all_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title        author  \\\n",
      "0  Analysing trends of computational urban scienc...      Kumar D.   \n",
      "1  A landmark federal interagency collaboration t...  Justice A.C.   \n",
      "2  Regional planning: A failed or flawed project ...    Chirisa I.   \n",
      "3  Data Science and Model Predictive Control:: A ...   Morato M.M.   \n",
      "4  Assessment of the relationship between central...    Akabane S.   \n",
      "\n",
      "                        publicationName  cover_date              scopus_id  \\\n",
      "0           Computational Urban Science  2024-12-01  SCOPUS_ID:85209789532   \n",
      "1                            JAMIA Open  2024-12-01  SCOPUS_ID:85208963031   \n",
      "2  Regional Science Policy and Practice  2024-12-01  SCOPUS_ID:85208099811   \n",
      "3            Journal of Process Control  2024-12-01  SCOPUS_ID:85207933325   \n",
      "4                    Scientific Reports  2024-12-01  SCOPUS_ID:85207210995   \n",
      "\n",
      "  cited_by_count  open_access                 eid aggregationType  \\\n",
      "0              0         True  2-s2.0-85209789532         Journal   \n",
      "1              0         True  2-s2.0-85208963031         Journal   \n",
      "2              0         True  2-s2.0-85208099811         Journal   \n",
      "3              0        False  2-s2.0-85207933325         Journal   \n",
      "4              0         True  2-s2.0-85207210995         Journal   \n",
      "\n",
      "                                        affiliations  \\\n",
      "0  [{'name': 'State University of New York Albany...   \n",
      "1  [{'name': 'VA Connecticut Healthcare System', ...   \n",
      "2  [{'name': 'University of the Free State', 'cit...   \n",
      "3  [{'name': 'Universit√© Grenoble Alpes', 'city':...   \n",
      "4  [{'name': 'The University of Tokyo Hospital', ...   \n",
      "\n",
      "                                                link  \n",
      "0  https://www.scopus.com/inward/record.uri?partn...  \n",
      "1  https://www.scopus.com/inward/record.uri?partn...  \n",
      "2  https://www.scopus.com/inward/record.uri?partn...  \n",
      "3  https://www.scopus.com/inward/record.uri?partn...  \n",
      "4  https://www.scopus.com/inward/record.uri?partn...  \n"
     ]
    }
   ],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"c8690de363626d560cf56cc17f9369d6\"\n",
    "    QUERY = 'TITLE(\"data science\")'\n",
    "    TOTAL_ARTICLES = 1100\n",
    "\n",
    "    # Step 1: Fetch data from API\n",
    "    articles = fetch_scopus_data(API_KEY, QUERY, max_records=TOTAL_ARTICLES)\n",
    "\n",
    "    # Step 3: Save data to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    # df = pd.DataFrame(articles_with_abstracts)\n",
    "    print(df.head())  # Print the first few rows for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PangSunatcha\\OneDrive - Chulalongkorn University\\Documents\\Y2S1 files\\Data Sci\\proj\\Data-Sci-project\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "print (script_dir)\n",
    "path = os.path.join(script_dir, \"../results/fetched_scopus.csv\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch abstract from a link\n",
    "def fetch_abstract(link):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    }\n",
    "    try:\n",
    "        # Request the page\n",
    "        response = requests.get(link, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find the section with id=\"abstractSection\"\n",
    "            abstract_section = soup.find('section', id='abstractSection')\n",
    "            if abstract_section:\n",
    "                # Find all <p> tags within this section\n",
    "                paragraphs = abstract_section.find_all('p')\n",
    "                # Combine all text from the <p> tags\n",
    "                return \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "            else:\n",
    "                return \"Abstract section not found\"\n",
    "        elif response.status_code == 403:\n",
    "            return \"Access forbidden - Possible login required\"\n",
    "        else:\n",
    "            return f\"Failed to fetch page (status {response.status_code})\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request failed: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban computing with a data science approaches can play a pivotal role in understaning and analyzing the potential of these methods for strategic, short-term, and sustainable planning. The recent development in urban areas have progressed towards the data-driven smart sustainable approaches to resolve the complexities around urban areas. The urban system faces severe challenges and these are complicated to capture, predict, resolve and deliver. The current study advances an unconventional decision-support framework to integrate the complexities of science, urban sustainability theories, and data science, with a data-intensive science to incorporate grassroots initiatives for a top-down policies. This work will¬†influence¬†the urban data analytics to optimize the designs and solutions to enhance sustainability, efficiency, resilience, equity, and quality of life. This work emphasizes the significant trends of data-driven and model-driven decision support systems. This will help to address and create an optimal solution¬†for¬†multifaceted challenges of an urban setup¬†within the analytical framework. The analytical¬†investigations includes¬†the research about land use prediction, environmental monitoring, transportation modelling, and social equity analysis. The fusion of urban computing, intelligence, and sustainability science is expected to resolve and contribute in shaping resilient, equitable, and future environmentally sensible eco-cities. It examines the emerging trends in the domain of computational urban science and data science approaches for sustainable development being utilized to address urban challenges including resource management, environmental impact, and social equity. The analysis of recent improvements and case studies highlights the potential of data-driven insights with computational models for promoting resilient sustainable urban environments, towards more effective and informed policy-making. Thus, this work explores the integration of computational urban science and data science methodologies to advance sustainable development. ¬© The Author(s) 2024.\n",
      "Objectives: In 2016, the Department of Veterans Affairs (VA) and the Department of Energy (DOE) established an Interagency Agreement (IAA), the Million Veteran Program-Computational Health Analytics for Medical Precision to Improve Outcomes Now (MVP-CHAMPION) research collaboration. Materials and Methods: Oversight fell under the VA Office of Research Development (VA ORD) and DOE headquarters. An Executive Committee and 2 senior scientific liaisons work with VA and DOE leadership to optimize efforts in the service of shared scientific goals. The program supported centralized data management and genomic analysis including creation of a scalable approach to cataloging phenotypes. Cross-cutting methods including natural language processing, image processing, and reusable code were developed. Results: The 79.6 million dollar collaboration has supported centralized data management and genomic analysis including a scalable approach to cataloging phenotypes and launched over 10 collaborative scientific projects in health conditions highly prevalent in veterans. A ground-breaking analysis on the Summit and Andes supercomputers at the Oak Ridge National Laboratory (ORNL) of the genetic underpinnings of over 2000 health conditions across 44 million genetic variants which resulted in the identification of 38 270 independent genetic variants associating with one or more health traits. Of these, over 2000 identified associations were unique to non-European ancestry. Cross-cutting methods have advanced state-of-the-art artificial intelligence (AI) including large language natural language processing and a system biology study focused on opioid addiction awarded the 2018 Gordon Bell Prize for outstanding achievement in high-performance computing. The collaboration has completed work in prostate cancer, suicide prevention, and cardiovascular disease, and cross-cutting data science. Predictive models developed in these projects are being tested for application in clinical management. Discussion: Eight new projects were launched in 2023, taking advantage of the momentum generated by the previous collaboration. A major challenge has been limitations in the scope of appropriated funds at DOE which cannot currently be used for health research. Conclusion: Extensive multidisciplinary interactions take time to establish and are essential to continued progress. New funding models for maintaining high-performance computing infrastructure at the ORNL and for supporting continued collaboration by joint VA-DOE research teams are needed. ¬© 2024 Published by Oxford University Press on behalf of the American Medical Informatics Association.\n",
      "This paper explores regional planning as an applied field of practice in Africa, where it is observed as a failed project or a flawed endeavour. In theory, regional planning (the practice) is based on regional science (the field of study) from which regional policy and regional development flow. The article advances the argument that Africa did not benefit from regional science in the past due to data scarcity. However, with the advent of big data and the data deluge caused by big data, regional science and regional planning stand to benefit. We argue that with the proliferation of big data, Africa can now tap into its opportunities and correct its ‚Äòugly‚Äô past regarding regional planning. With big data and artificial intelligence (AI)-tools, coupled with skilled and capable people, Africa can effectively mine data and catch up with global trends in urban and regional planning. The study concludes that there is a need to embrace big data and mine data for African regional science and regional planning to be successful. In addition, the African regional planning project was not a failure, but it suffered a stillbirth due to a lack of data and related infrastructure. ¬© 2024 The Authors\n",
      "Model Predictive Control (MPC) is an established control framework, based on the solution of an optimisation problem to determine the (optimal) control action at each discrete-time sample. Accordingly, major theoretical advances have been provided in the literature, such as closed-loop stability and recursive feasibility certificates, for the most diverse kinds of processes descriptions. Nevertheless, identifying good, trustworthy models for complex systems is a task heavily affected by uncertainties. As of this, developing MPC algorithms directly from data has recently received a considerable amount of attention over the last couple of years. In this work, we review the available data-based MPC formulations, which range from reinforcement learning schemes, adaptive controllers, and novel solutions based on behavioural theory and trajectory representations. In particular, we examine the recent research body on this topic, highlighting the main features and capabilities of available algorithms, while also discussing the fundamental connections among approaches and, comparatively, their advantages and limitations. ¬© 2024 Elsevier Ltd\n",
      "Purpose The relationship between the height of the V wave in the central venous pressure (CVP) waveform and the severity of tricuspid regurgitation (TR) is well known. Their diagnostic ability is unconfirmed. This study explored CVP waveform variations with TR. Methods All patients who underwent preoperative echocardiography and CVP waveform measurements before surgery at our institution were included. Indices were created to capture each feature of the CVP waveform. The median value for each case was obtained and statistically analyzed according to the severity of TR. A deep learning technique, Transformer, was used to handle the complex features of CVP waveforms. Results This study included 436 cases. The values for C wave ‚Äì Y descent, X descent ‚Äì Y descent, and V wave ‚Äì Y descent differed significantly in the Jonckheere‚ÄìTerpstra test (p = 0.0018, 0.027, and 0.077, respectively). The area under the receiver operating characteristic (ROC) curve (AUC) for X descent ‚Äì Y descent in two groups, none to moderate TR and severe TR, was 0.83 (95% confidence interval (CI) [0.68, 0.98]). For Transformer, the accuracy of the validation dataset was 0.97. Conclusions The shape of the CVP waveform varied with the severity of TR in a large dataset. ¬© The Author(s) 2024.\n",
      "In the context of rapid urbanization, accurately identifying the visual factors that influence environmental safety perception is crucial for improving urban transportation environments and enhancing pedestrian safety. With the increase in urban population density and traffic flow, optimizing urban environmental design to elevate residents‚Äô sense of safety has become a key issue in urban planning and management. However, the existing studies face numerous challenges in conducting large-scale quantitative analysis of environmental safety perception in complex scenarios, such as difficulties in data acquisition and limitations in analytical methods. This study addresses these challenges by applying image semantic segmentation and object detection techniques to extract key visual elements from street view images, combined with manual scoring and deep learning methods, to construct a road safety perception dataset. Using a LightGBM model and the SHAP interpretation framework, in this study, we identify the critical visual factors influencing environmental safety perception. An empirical study was conducted in Macau, a modern city where Eastern and Western cultures intersect, and tourism thrives. The findings reveal that: ‚ë† The overall environmental safety perception in the eight parishes and surrounding roads of Macau is relatively high, with significant regional differences in safety perception scores around Macau‚Äôs parish roads; ‚ë° The proportions of buildings, sidewalks, roads, and trees in images are the four primary factors influencing environmental safety perception; ‚ë¢ The proportions and quantities of visual elements interact with each other, and their reasonable distribution helps form clear spatial visibility and creates conducive activity spaces, thereby enhancing the perception of environmental safety. Through empirical analysis, this study uncovers the mechanisms by which visual elements in urban street scenes affect environmental safety perception, providing scientific evidence for urban planning and transportation environment improvement. The research holds theoretical significance and offers practical references for urban design and management, demonstrating broad application value. ¬© The Author(s) 2024.\n",
      "2D magnetotelluric (MT) imaging detects underground structures by measuring electromagnetic fields. This study tackles two issues in the field: traditional methods‚Äô limitations due to insufficient forward modeling data, and the challenge of multiple solutions in complex scenarios. We introduce an enhanced 2D MT imaging approach with a novel self-attention mechanism, involving: 1. Generating diverse geophysical models and responses to increase data variety and volume. 2. Creating a Swin‚ÄìUnet-based 2D MT Imaging network with self-attention for better modeling and relation capture, incorporating a MT sample generator using real data to lessen large-scale supervised training dependence, and refining the loss function for optimal validation. This method also includes eliminating MT background response to boost training efficiency and reduce training time. 3. Applying a transverse electric/transverse magnetic method for comprehensive 2D MT data response. Tests show that our method greatly improves 2D MT imaging‚Äôs accuracy and efficiency, with excellent generalization. ¬© The Author(s) 2024.\n",
      "Defined as the merging of social and environmental sustainability into corporate operations, sustainable entrepreneurship has embraced data science more and more to improve operational effectiveness and decision-making. Using statistics, machine learning, and computer science to uncover insights from challenging datasets, this interdisciplinary method blends the ideas of sustainability with sophisticated data analysis approaches. Our research supports the choice of this issue by stressing the urgent requirement of sophisticated analytical instruments to negotiate the complexity of sustainable business practices. We compare our proposed model against Logistic Regression, Feedforward Neural Networks, and Support Vector Machines (SVMs). This not only shows how better CNN models are for certain uses but also highlights the general possibilities of data science in promoting sustainability in business. Our results highlight the transforming ability of sophisticated machine learning methods in promoting informed, sustainable decision-making and supporting the more general conversation on sustainable business. ¬© 2024 Elsevier Inc.\n",
      "This study introduces a cutting-edge profit health assessment framework that merges signaling theory and agency theory within a data science context, leveraging both financial and nonfinancial indicators to provide a comprehensive, multidimensional evaluation of earnings quality in publicly traded companies. This model transcends conventional earnings management frameworks by focusing on the holistic earnings condition of firms and addressing challenges related to information asymmetry and managerial motives. The empirical analysis utilizes data from small and medium-sized private enterprises listed on China‚Äôs SME Board between 2018 and 2022, employing machine learning algorithms to rigorously validate the model‚Äôs effectiveness. The results reveal that 85.7% of the penalized firms by the China Securities Regulatory Commission from 2018 to 2023 exhibited low levels of profit health. This data science driven analysis deepens the understanding of corporate earnings health for regulators and investors. The proposed framework not only expands the theoretical underpinnings of earnings management but also serves as a novel evaluative tool for stakeholders, paving the way for broader application across diverse markets and sectors and rethinking earnings quality assessment methodologies through a data science lens. ¬© The Author(s) 2024.\n",
      "Data-informed decision making is a critical goal for many community-based public health research initiatives. However, community partners often encounter challenges when interacting with data. The Community-Engaged Data Science (CEDS) model offers a goal-oriented, iterative guide for communities to collaborate with research data scientists through data ambassadors. This study presents a case study of CEDS applied to research on the opioid epidemic in 18 counties in Ohio as part of the HEALing Communities Study (HCS). Data ambassadors provided a pivotal role in empowering community coalitions to translate data into action using key steps of CEDS which included: data landscapes identifying available data in the community; data action plans from logic models based on community data needs and gaps of data; data collection/sharing agreements; and data systems including portals and dashboards. Throughout the CEDS process, data ambassadors emphasized sustainable data workflows, supporting continued data engagement beyond the HCS. The implementation of CEDS in Ohio underscored the importance of relationship building, timing of implementation, understanding communities‚Äô data preferences, and flexibility when working with communities. Researchers should consider implementing CEDS and integrating a data ambassador in community-based research to enhance community data engagement and drive data-informed interventions to improve public health outcomes. ¬© The Author(s) 2024.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    abstract = fetch_abstract(df['link'][i])\n",
    "    print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  abstracts\u001b[38;5;66;03m#df\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Extract abstracts\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df_with_abstracts \u001b[38;5;241m=\u001b[39m extract_abstracts_from_df(\u001b[43mdf\u001b[49m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Display the updated DataFrame\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_with_abstracts)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# yoo = fetch_abstract(df['link'][0])\n",
    "# print(yoo)\n",
    "def extract_abstracts_from_df(df):\n",
    "    abstracts = []\n",
    "    for i in range(12):\n",
    "        abstract = fetch_abstract(df['link'][i])\n",
    "        abstracts.append(abstract)\n",
    "        print(abstract)\n",
    "    # df['abstract'] = abstracts\n",
    "    return  abstracts#df\n",
    "\n",
    "# Extract abstracts\n",
    "df_with_abstracts = extract_abstracts_from_df(df)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_with_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from playwright.sync_api import Page, expect\n",
    "\n",
    "def extract_abstact(page: Page):\n",
    "    page.goto(df['link'][0])\n",
    "    expect(page).to_have_title(df['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly pick articles\n",
    "def pick_random_articles(articles, total=1000):\n",
    "    if len(articles) < total:\n",
    "        print(f\"Warning: Only {len(articles)} articles available. Returning all.\")\n",
    "        return articles\n",
    "    return random.sample(articles, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 11)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kv/cjr920_13657jf274w2_22t00000gn/T/ipykernel_1235/340877856.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0marticle_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# If the article has a valid link, try to extract the abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'link'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Use the 'link' directly for the abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mabstract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_abstracts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass the link to fetch abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "article_names = []\n",
    "\n",
    "for article in articles:\n",
    "    # If the article has a valid link, try to extract the abstract\n",
    "    if df['link']:  # Use the 'link' directly for the abstract\n",
    "        abstract = extract_abstracts(df)  # Pass the link to fetch abstract\n",
    "        article_names.append(abstract)  # Collect abstracts\n",
    "    else:\n",
    "        article_names.append(\"No link available\")  # Add a placeholder if no link is present\n",
    "\n",
    "# Print the first 10 abstracts as a preview\n",
    "print(article_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85209789532&origin=inward'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.scopus.com/inward/record.uri?partn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from playwright.sync_api import Page, expect\n",
    "\n",
    "def extract_abstact(page: Page):\n",
    "    page.goto(df['link'][0])\n",
    "    expect(page).to_have_title(df['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Example text\n",
    "text = \"Data Science in Supporting Hotel Management: An Application of Deep Learning Techniques in 2024!\"\n",
    "\n",
    "# Define a regular expression pattern to extract words\n",
    "words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "# Remove stopwords by checking if each word is not in the stopword list\n",
    "filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Display the filtered words\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_title  \\\n",
      "0    [Data, Science, Supporting, Hotel, Management,...   \n",
      "1    [Data, science, Analysis, Using, Deep, Learnin...   \n",
      "2    [Data, science, based, reconstruction, D, memb...   \n",
      "3    [Data, science, sustainable, entrepreneurship,...   \n",
      "4    [Applying, data, science, methodologies, artif...   \n",
      "..                                                 ...   \n",
      "995  [Data, Science, literacy, future, security, in...   \n",
      "996  [Predictive, modeling, heat, formation, sulfur...   \n",
      "997  [Data, Science, Model, Predictive, Control, su...   \n",
      "998  [Machine, learning, data, science, techniques,...   \n",
      "999  [Harnessing, Data, Science, Produced, Water, E...   \n",
      "\n",
      "                               cleaned_publicationName  \n",
      "0           [Smart, Innovation, Systems, Technologies]  \n",
      "1    [International, Conference, Knowledge, Enginee...  \n",
      "2                         [Journal, Membrane, Science]  \n",
      "3         [Technological, Forecasting, Social, Change]  \n",
      "4               [American, Journal, Medical, Genetics]  \n",
      "..                                                 ...  \n",
      "995  [Journal, Policing, Intelligence, Counter, Ter...  \n",
      "996                [European, Physical, Journal, Plus]  \n",
      "997                        [Journal, Process, Control]  \n",
      "998  [Machine, Learning, Data, Science, Techniques,...  \n",
      "999  [Society, Petroleum, Engineers, SPE, Water, Li...  \n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Extract words (alphabetic characters only)\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "    return filtered_words # Join words back into a string\n",
    "\n",
    "# Apply the clean_text function to the 'title' and 'name' columns\n",
    "df['cleaned_title'] = df['title'].apply(lambda x: clean_text(str(x)))\n",
    "df['cleaned_publicationName'] = df['publicationName'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "print(df[['cleaned_title', 'cleaned_publicationName']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"scopus_data.csv\"\n",
    "df.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"scopus_data.csv\"\n",
    "df.to_csv(output_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

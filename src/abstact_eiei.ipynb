{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse JSON response\n",
    "def parse_scopus_response(json_response):\n",
    "    entries = []\n",
    "    for entry in json_response.get('search-results', {}).get('entry', []):\n",
    "        cover_date = entry.get('prism:coverDate')\n",
    "        # Ensure cover_date is present and not in the future\n",
    "        if cover_date and datetime.strptime(cover_date, \"%Y-%m-%d\").date() <= datetime.now().date():\n",
    "            affiliations = entry.get('affiliation', [])\n",
    "            affiliation_info = []\n",
    "            for aff in affiliations:\n",
    "                aff_name = aff.get('affilname', 'N/A')\n",
    "                aff_city = aff.get('affiliation-city', 'N/A')\n",
    "                aff_country = aff.get('affiliation-country', 'N/A')\n",
    "                affiliation_info.append({\n",
    "                    'name': aff_name,\n",
    "                    'city': aff_city,\n",
    "                    'country': aff_country\n",
    "                })\n",
    "\n",
    "            article = {\n",
    "                'title': entry.get('dc:title'),\n",
    "                'author': entry.get('dc:creator'),\n",
    "                'publicationName': entry.get('prism:publicationName'),\n",
    "                'cover_date': cover_date,\n",
    "                'scopus_id': entry.get('dc:identifier'),\n",
    "                'cited_by_count': entry.get('citedby-count'),\n",
    "                'open_access': entry.get('openaccessFlag'),\n",
    "                'eid' : entry.get('eid'),\n",
    "                'aggregationType' : entry.get('prism:aggregationType', 'N/A'),\n",
    "                'affiliations': affiliation_info,\n",
    "                'link': entry.get('link')[2]['@href'] if 'link' in entry and len(entry['link']) > 0 else None\n",
    "            }\n",
    "            entries.append(article)\n",
    "    return entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch data from Scopus API with pagination\n",
    "def fetch_scopus_data(api_key, query, max_records=1000, count=25):\n",
    "    base_url = \"https://api.elsevier.com/content/search/scopus\"\n",
    "    all_entries = []\n",
    "    start_index = 0\n",
    "\n",
    "    while len(all_entries) < max_records:\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'start': start_index,\n",
    "            'count': count,\n",
    "            'apiKey': api_key\n",
    "        }\n",
    "        headers = {'Accept': 'application/json'}\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            json_response = response.json()\n",
    "            articles = parse_scopus_response(json_response)\n",
    "            if not articles:\n",
    "                print(\"No more articles found or all articles have future dates.\")\n",
    "                break  # Stop if no articles are found in the response\n",
    "            all_entries.extend(articles)\n",
    "            start_index += count\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "\n",
    "        # Avoid exceeding the total max_records\n",
    "        if len(all_entries) >= max_records:\n",
    "            all_entries = all_entries[:max_records]\n",
    "            break\n",
    "\n",
    "    return all_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title        author  \\\n",
      "0  Analysing trends of computational urban scienc...      Kumar D.   \n",
      "1  A landmark federal interagency collaboration t...  Justice A.C.   \n",
      "2  Regional planning: A failed or flawed project ...    Chirisa I.   \n",
      "3  Data Science and Model Predictive Control:: A ...   Morato M.M.   \n",
      "4  Assessment of the relationship between central...    Akabane S.   \n",
      "\n",
      "                        publicationName  cover_date              scopus_id  \\\n",
      "0           Computational Urban Science  2024-12-01  SCOPUS_ID:85209789532   \n",
      "1                            JAMIA Open  2024-12-01  SCOPUS_ID:85208963031   \n",
      "2  Regional Science Policy and Practice  2024-12-01  SCOPUS_ID:85208099811   \n",
      "3            Journal of Process Control  2024-12-01  SCOPUS_ID:85207933325   \n",
      "4                    Scientific Reports  2024-12-01  SCOPUS_ID:85207210995   \n",
      "\n",
      "  cited_by_count  open_access                 eid aggregationType  \\\n",
      "0              0         True  2-s2.0-85209789532         Journal   \n",
      "1              0         True  2-s2.0-85208963031         Journal   \n",
      "2              0         True  2-s2.0-85208099811         Journal   \n",
      "3              0        False  2-s2.0-85207933325         Journal   \n",
      "4              0         True  2-s2.0-85207210995         Journal   \n",
      "\n",
      "                                        affiliations  \\\n",
      "0  [{'name': 'State University of New York Albany...   \n",
      "1  [{'name': 'VA Connecticut Healthcare System', ...   \n",
      "2  [{'name': 'University of the Free State', 'cit...   \n",
      "3  [{'name': 'Université Grenoble Alpes', 'city':...   \n",
      "4  [{'name': 'The University of Tokyo Hospital', ...   \n",
      "\n",
      "                                                link  \n",
      "0  https://www.scopus.com/inward/record.uri?partn...  \n",
      "1  https://www.scopus.com/inward/record.uri?partn...  \n",
      "2  https://www.scopus.com/inward/record.uri?partn...  \n",
      "3  https://www.scopus.com/inward/record.uri?partn...  \n",
      "4  https://www.scopus.com/inward/record.uri?partn...  \n"
     ]
    }
   ],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"c8690de363626d560cf56cc17f9369d6\"\n",
    "    QUERY = 'TITLE(\"data science\")'\n",
    "    TOTAL_ARTICLES = 1100\n",
    "\n",
    "    # Step 1: Fetch data from API\n",
    "    articles = fetch_scopus_data(API_KEY, QUERY, max_records=TOTAL_ARTICLES)\n",
    "\n",
    "    # Step 3: Save data to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    # df = pd.DataFrame(articles_with_abstracts)\n",
    "    print(df.head())  # Print the first few rows for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dear/Data Science/Project/Data-Sci-project/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "script_dir = os.getcwd()\n",
    "print(script_dir)\n",
    "path = os.path.join(script_dir,\"../results/fetched_scopus.csv\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch abstract from a link\n",
    "def fetch_abstract(link,delay):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    }\n",
    "    try:\n",
    "        # Request the page\n",
    "        response = requests.get(link, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find the section with id=\"abstractSection\"\n",
    "            abstract_section = soup.find('section', id='abstractSection')\n",
    "            if abstract_section:\n",
    "                # Find all <p> tags within this section\n",
    "                paragraphs = abstract_section.find_all('p')\n",
    "                # Combine all text from the <p> tags\n",
    "                return \" \".join(p.get_text(strip=True) for p in paragraphs)\n",
    "            else:\n",
    "                return \"Abstract section not found\"\n",
    "        elif response.status_code == 403:\n",
    "            return \"Access forbidden - Possible login required\"\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Rate limit hit. Retrying \")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2\n",
    "        else:\n",
    "            return f\"Failed to fetch page (status {response.status_code})\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request failed: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban computing with a data science approaches can play a pivotal role in understaning and analyzing the potential of these methods for strategic, short-term, and sustainable planning. The recent development in urban areas have progressed towards the data-driven smart sustainable approaches to resolve the complexities around urban areas. The urban system faces severe challenges and these are complicated to capture, predict, resolve and deliver. The current study advances an unconventional decision-support framework to integrate the complexities of science, urban sustainability theories, and data science, with a data-intensive science to incorporate grassroots initiatives for a top-down policies. This work will influence the urban data analytics to optimize the designs and solutions to enhance sustainability, efficiency, resilience, equity, and quality of life. This work emphasizes the significant trends of data-driven and model-driven decision support systems. This will help to address and create an optimal solution for multifaceted challenges of an urban setup within the analytical framework. The analytical investigations includes the research about land use prediction, environmental monitoring, transportation modelling, and social equity analysis. The fusion of urban computing, intelligence, and sustainability science is expected to resolve and contribute in shaping resilient, equitable, and future environmentally sensible eco-cities. It examines the emerging trends in the domain of computational urban science and data science approaches for sustainable development being utilized to address urban challenges including resource management, environmental impact, and social equity. The analysis of recent improvements and case studies highlights the potential of data-driven insights with computational models for promoting resilient sustainable urban environments, towards more effective and informed policy-making. Thus, this work explores the integration of computational urban science and data science methodologies to advance sustainable development. © The Author(s) 2024.\n",
      "Objectives: In 2016, the Department of Veterans Affairs (VA) and the Department of Energy (DOE) established an Interagency Agreement (IAA), the Million Veteran Program-Computational Health Analytics for Medical Precision to Improve Outcomes Now (MVP-CHAMPION) research collaboration. Materials and Methods: Oversight fell under the VA Office of Research Development (VA ORD) and DOE headquarters. An Executive Committee and 2 senior scientific liaisons work with VA and DOE leadership to optimize efforts in the service of shared scientific goals. The program supported centralized data management and genomic analysis including creation of a scalable approach to cataloging phenotypes. Cross-cutting methods including natural language processing, image processing, and reusable code were developed. Results: The 79.6 million dollar collaboration has supported centralized data management and genomic analysis including a scalable approach to cataloging phenotypes and launched over 10 collaborative scientific projects in health conditions highly prevalent in veterans. A ground-breaking analysis on the Summit and Andes supercomputers at the Oak Ridge National Laboratory (ORNL) of the genetic underpinnings of over 2000 health conditions across 44 million genetic variants which resulted in the identification of 38 270 independent genetic variants associating with one or more health traits. Of these, over 2000 identified associations were unique to non-European ancestry. Cross-cutting methods have advanced state-of-the-art artificial intelligence (AI) including large language natural language processing and a system biology study focused on opioid addiction awarded the 2018 Gordon Bell Prize for outstanding achievement in high-performance computing. The collaboration has completed work in prostate cancer, suicide prevention, and cardiovascular disease, and cross-cutting data science. Predictive models developed in these projects are being tested for application in clinical management. Discussion: Eight new projects were launched in 2023, taking advantage of the momentum generated by the previous collaboration. A major challenge has been limitations in the scope of appropriated funds at DOE which cannot currently be used for health research. Conclusion: Extensive multidisciplinary interactions take time to establish and are essential to continued progress. New funding models for maintaining high-performance computing infrastructure at the ORNL and for supporting continued collaboration by joint VA-DOE research teams are needed. © 2024 Published by Oxford University Press on behalf of the American Medical Informatics Association.\n",
      "This paper explores regional planning as an applied field of practice in Africa, where it is observed as a failed project or a flawed endeavour. In theory, regional planning (the practice) is based on regional science (the field of study) from which regional policy and regional development flow. The article advances the argument that Africa did not benefit from regional science in the past due to data scarcity. However, with the advent of big data and the data deluge caused by big data, regional science and regional planning stand to benefit. We argue that with the proliferation of big data, Africa can now tap into its opportunities and correct its ‘ugly’ past regarding regional planning. With big data and artificial intelligence (AI)-tools, coupled with skilled and capable people, Africa can effectively mine data and catch up with global trends in urban and regional planning. The study concludes that there is a need to embrace big data and mine data for African regional science and regional planning to be successful. In addition, the African regional planning project was not a failure, but it suffered a stillbirth due to a lack of data and related infrastructure. © 2024 The Authors\n",
      "Model Predictive Control (MPC) is an established control framework, based on the solution of an optimisation problem to determine the (optimal) control action at each discrete-time sample. Accordingly, major theoretical advances have been provided in the literature, such as closed-loop stability and recursive feasibility certificates, for the most diverse kinds of processes descriptions. Nevertheless, identifying good, trustworthy models for complex systems is a task heavily affected by uncertainties. As of this, developing MPC algorithms directly from data has recently received a considerable amount of attention over the last couple of years. In this work, we review the available data-based MPC formulations, which range from reinforcement learning schemes, adaptive controllers, and novel solutions based on behavioural theory and trajectory representations. In particular, we examine the recent research body on this topic, highlighting the main features and capabilities of available algorithms, while also discussing the fundamental connections among approaches and, comparatively, their advantages and limitations. © 2024 Elsevier Ltd\n",
      "Purpose The relationship between the height of the V wave in the central venous pressure (CVP) waveform and the severity of tricuspid regurgitation (TR) is well known. Their diagnostic ability is unconfirmed. This study explored CVP waveform variations with TR. Methods All patients who underwent preoperative echocardiography and CVP waveform measurements before surgery at our institution were included. Indices were created to capture each feature of the CVP waveform. The median value for each case was obtained and statistically analyzed according to the severity of TR. A deep learning technique, Transformer, was used to handle the complex features of CVP waveforms. Results This study included 436 cases. The values for C wave – Y descent, X descent – Y descent, and V wave – Y descent differed significantly in the Jonckheere–Terpstra test (p = 0.0018, 0.027, and 0.077, respectively). The area under the receiver operating characteristic (ROC) curve (AUC) for X descent – Y descent in two groups, none to moderate TR and severe TR, was 0.83 (95% confidence interval (CI) [0.68, 0.98]). For Transformer, the accuracy of the validation dataset was 0.97. Conclusions The shape of the CVP waveform varied with the severity of TR in a large dataset. © The Author(s) 2024.\n",
      "In the context of rapid urbanization, accurately identifying the visual factors that influence environmental safety perception is crucial for improving urban transportation environments and enhancing pedestrian safety. With the increase in urban population density and traffic flow, optimizing urban environmental design to elevate residents’ sense of safety has become a key issue in urban planning and management. However, the existing studies face numerous challenges in conducting large-scale quantitative analysis of environmental safety perception in complex scenarios, such as difficulties in data acquisition and limitations in analytical methods. This study addresses these challenges by applying image semantic segmentation and object detection techniques to extract key visual elements from street view images, combined with manual scoring and deep learning methods, to construct a road safety perception dataset. Using a LightGBM model and the SHAP interpretation framework, in this study, we identify the critical visual factors influencing environmental safety perception. An empirical study was conducted in Macau, a modern city where Eastern and Western cultures intersect, and tourism thrives. The findings reveal that: ① The overall environmental safety perception in the eight parishes and surrounding roads of Macau is relatively high, with significant regional differences in safety perception scores around Macau’s parish roads; ② The proportions of buildings, sidewalks, roads, and trees in images are the four primary factors influencing environmental safety perception; ③ The proportions and quantities of visual elements interact with each other, and their reasonable distribution helps form clear spatial visibility and creates conducive activity spaces, thereby enhancing the perception of environmental safety. Through empirical analysis, this study uncovers the mechanisms by which visual elements in urban street scenes affect environmental safety perception, providing scientific evidence for urban planning and transportation environment improvement. The research holds theoretical significance and offers practical references for urban design and management, demonstrating broad application value. © The Author(s) 2024.\n",
      "2D magnetotelluric (MT) imaging detects underground structures by measuring electromagnetic fields. This study tackles two issues in the field: traditional methods’ limitations due to insufficient forward modeling data, and the challenge of multiple solutions in complex scenarios. We introduce an enhanced 2D MT imaging approach with a novel self-attention mechanism, involving: 1. Generating diverse geophysical models and responses to increase data variety and volume. 2. Creating a Swin–Unet-based 2D MT Imaging network with self-attention for better modeling and relation capture, incorporating a MT sample generator using real data to lessen large-scale supervised training dependence, and refining the loss function for optimal validation. This method also includes eliminating MT background response to boost training efficiency and reduce training time. 3. Applying a transverse electric/transverse magnetic method for comprehensive 2D MT data response. Tests show that our method greatly improves 2D MT imaging’s accuracy and efficiency, with excellent generalization. © The Author(s) 2024.\n",
      "Defined as the merging of social and environmental sustainability into corporate operations, sustainable entrepreneurship has embraced data science more and more to improve operational effectiveness and decision-making. Using statistics, machine learning, and computer science to uncover insights from challenging datasets, this interdisciplinary method blends the ideas of sustainability with sophisticated data analysis approaches. Our research supports the choice of this issue by stressing the urgent requirement of sophisticated analytical instruments to negotiate the complexity of sustainable business practices. We compare our proposed model against Logistic Regression, Feedforward Neural Networks, and Support Vector Machines (SVMs). This not only shows how better CNN models are for certain uses but also highlights the general possibilities of data science in promoting sustainability in business. Our results highlight the transforming ability of sophisticated machine learning methods in promoting informed, sustainable decision-making and supporting the more general conversation on sustainable business. © 2024 Elsevier Inc.\n",
      "This study introduces a cutting-edge profit health assessment framework that merges signaling theory and agency theory within a data science context, leveraging both financial and nonfinancial indicators to provide a comprehensive, multidimensional evaluation of earnings quality in publicly traded companies. This model transcends conventional earnings management frameworks by focusing on the holistic earnings condition of firms and addressing challenges related to information asymmetry and managerial motives. The empirical analysis utilizes data from small and medium-sized private enterprises listed on China’s SME Board between 2018 and 2022, employing machine learning algorithms to rigorously validate the model’s effectiveness. The results reveal that 85.7% of the penalized firms by the China Securities Regulatory Commission from 2018 to 2023 exhibited low levels of profit health. This data science driven analysis deepens the understanding of corporate earnings health for regulators and investors. The proposed framework not only expands the theoretical underpinnings of earnings management but also serves as a novel evaluative tool for stakeholders, paving the way for broader application across diverse markets and sectors and rethinking earnings quality assessment methodologies through a data science lens. © The Author(s) 2024.\n",
      "Data-informed decision making is a critical goal for many community-based public health research initiatives. However, community partners often encounter challenges when interacting with data. The Community-Engaged Data Science (CEDS) model offers a goal-oriented, iterative guide for communities to collaborate with research data scientists through data ambassadors. This study presents a case study of CEDS applied to research on the opioid epidemic in 18 counties in Ohio as part of the HEALing Communities Study (HCS). Data ambassadors provided a pivotal role in empowering community coalitions to translate data into action using key steps of CEDS which included: data landscapes identifying available data in the community; data action plans from logic models based on community data needs and gaps of data; data collection/sharing agreements; and data systems including portals and dashboards. Throughout the CEDS process, data ambassadors emphasized sustainable data workflows, supporting continued data engagement beyond the HCS. The implementation of CEDS in Ohio underscored the importance of relationship building, timing of implementation, understanding communities’ data preferences, and flexibility when working with communities. Researchers should consider implementing CEDS and integrating a data ambassador in community-based research to enhance community data engagement and drive data-informed interventions to improve public health outcomes. © The Author(s) 2024.\n",
      "The Structural Genomics Consortium is an international open science research organization with a focus on accelerating early-stage drug discovery, namely hit discovery and optimization. We, as many others, believe that artificial intelligence (AI) is poised to be a main accelerator in the field. The question is then how to best benefit from recent advances in AI and how to generate, format and disseminate data to enable future breakthroughs in AI-guided drug discovery. We present here the recommendations of a working group composed of experts from both the public and private sectors. Robust data management requires precise ontologies and standardized vocabulary while a centralized database architecture across laboratories facilitates data integration into high-value datasets. Lab automation and opening electronic lab notebooks to data mining push the boundaries of data sharing and data modeling. Important considerations for building robust machine-learning models include transparent and reproducible data processing, choosing the most relevant data representation, defining the right training and test sets, and estimating prediction uncertainty. Beyond data-sharing, cloud-based computing can be harnessed to build and disseminate machine-learning models. Important vectors of acceleration for hit and chemical probe discovery will be (1) the real-time integration of experimental data generation and modeling workflows within design-make-test-analyze (DMTA) cycles openly, and at scale and (2) the adoption of a mindset where data scientists and experimentalists work as a unified team, and where data science is incorporated into the experimental design. © The Author(s) 2024.\n",
      "Background: Health Data Science (HDS) is a novel interdisciplinary field that integrates biological, clinical, and computational sciences with the aim of analysing clinical and biological data through the utilisation of computational methods. Training healthcare specialists who are knowledgeable in both health and data sciences is highly required, important, and challenging. Therefore, it is essential to analyse students’ learning experiences through artificial intelligence techniques in order to provide both teachers and learners with insights about effective learning strategies and to improve existing HDS course designs. Methods: We applied artificial intelligence methods to uncover learning tactics and strategies employed by students in an HDS massive open online course with over 3,000 students enrolled. We also used statistical tests to explore students’ engagement with different resources (such as reading materials and lecture videos) and their level of engagement with various HDS topics. Results: We found that students in HDS employed four learning tactics, such as actively connecting new information to their prior knowledge, taking assessments and practising programming to evaluate their understanding, collaborating with their classmates, and repeating information to memorise. Based on the employed tactics, we also found three types of learning strategies, including low engagement (Surface learners), moderate engagement (Strategic learners), and high engagement (Deep learners), which are in line with well-known educational theories. The results indicate that successful students allocate more time to practical topics, such as projects and discussions, make connections among concepts, and employ peer learning. Conclusions: We applied artificial intelligence techniques to provide new insights into HDS education. Based on the findings, we provide pedagogical suggestions not only for course designers but also for teachers and learners that have the potential to improve the learning experience of HDS students. © The Author(s) 2024.\n",
      "Background: Models for structuring big-data and data-analytics projects typically start with a definition of the project’s goals and the business value they are expected to create. The literature identifies proper project definition as crucial for a project’s success, and also recognizes that the translation of business objectives into data-analytic problems is a difficult task. Unfortunately, common project structures, such as CRISP-DM, provide little guidance for this crucial stage when compared to subsequent project stages such as data preparation and modeling. Contribution: This paper contributes structure to the project-definition stage of data-analytic projects by proposing the Data-Analytic Problem Structure (DAPS). The diagrammatic technique facilitates the collaborative development of a consistent and precise definition of a data-analytic problem, and the articulation of how it contributes to the organization’s goals. In addition, the technique helps to identify important assumptions, and to break down large ambitions in manageable subprojects. Methods: The semi-formal specification technique took other models for problem structuring — common in fields such as operations research and business analytics — as a point of departure. The proposed technique was applied in 47 real data-analytic projects and refined based on the results, following a design-science approach. © The Author(s) 2024.\n",
      "Instagram is one of the most popular and widely used social network platforms. It is used as a digital tool to connect with other users and also to share information and influence them for marketing and advertising purposes. The influence of popular users is broadly determined by post’s engagement rate in terms of likes, comments, and shares, and the number of followers as well. An objective and comprehensive measure of popularity is necessary to understand the factors that will help make an influencer marketing campaign more successful and beneficial for business activities. This research work attempts to take various features of an influencer account and Instagram posts dataset and develop a novel model that accurately quantifies and determines the influence of a user on Instagram. The research is based on datasets of top regional Instagram influencers and their posts based on categories signified through hashtags and captions. Our research attempts to develop a model using principal component analysis to quantify influence and using it to rank influencers. In our experiment, the proposed model after experimentation, gave the Instagram username “iqbaal.e” influence score as 874,712.9526, username “1nctdream” as 753,830.5847 and username “weareone. exo” as 668,054.4360. The proposed model ranks were compared with other ranks for Instagram users based on other measures such as follower rank etc. User names “huyitian”, “bintangemon” and “bimopd” are top social media influencers based on the proposed model for better business advertising and digital marketing outcomes with collected data and experiment context. This proposed approach gives an exploration for the stakeholders to quantify the impact of influencer in social media and demonstrate an innovative approach. © The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature 2024.\n",
      "[No abstract available]\n",
      "Background: Economic incentives can improve clinical outcomes among in-care people living with HIV (PLHIV), but evidence is limited for their effectiveness among out-of-care PLHIV or those at risk of disengagement. We propose a type 1 hybrid effectiveness-implementation study to advance global knowledge about the use of economic incentives to strengthen the continuity of HIV care and accelerate global goals for HIV epidemic control. Methods: The Rudi Kundini, Pamoja Kundini study will evaluate two implementation models of an economic incentive strategy for supporting two groups of PLHIV in Tanzania. Phase 1 of the study consists of a two-arm, cluster randomized trial across 32 health facilities to assess the effectiveness of a home visit plus one-time economic incentive on the proportion of out-of-care PLHIV with viral load suppression (< 1000 copies/ml) 6 months after enrollment (n = 640). Phase 2 is an individual 1:1 randomized controlled trial designed to determine the effectiveness of a short-term counseling and economic incentive program offered to in-care PLHIV who are predicted through machine learning to be at risk of disengaging from care on the outcome of viral load suppression at 12 months (n = 692). The program includes up to three incentives conditional upon visit attendance coupled with adapted counselling sessions for this population of PLHIV. Consistent with a hybrid effectiveness-implementation study design, phase 3 is a mixed methods evaluation to explore barriers and facilitators to strategy implementation in phases 1 and 2. Results will be used to guide optimization and scale-up of the incentive strategies, if effective, to the larger population of Tanzanian PLHIV who struggle with continuity of HIV care. Discussion: Innovative strategies that recognize the dynamic process of lifelong retention in HIV care are urgently needed. Strategies such as conditional economic incentives are a simple and effective method for improving many health outcomes, including those on the HIV continuum. If coupled with other supportive services such as home visits (phase 1) or with tailored counselling (phase 2), economic incentives have the potential to strengthen engagement among the subpopulation of PLHIV who struggle with retention in care and could help to close the gap towards reaching global “95–95-95” goals for ending the AIDS epidemic. Trial registration: Phase 1: ClinicalTrials.gov, NCT05248100, registered 2/21/2022. Phase 2: ClinicalTrials.gov, NCT05373095, registered 5/13/2022. © The Author(s) 2024.\n",
      "Failed to fetch page (status 404)\n",
      "This article examines how claims to predictable borders via data science techniques are crafted in bureaucratic institutions. Through a case study of testing algorithmic systems at a transnational agency, we examine how humanitarian organizations reconcile the risks of predictive technologies with the benefits they claim to receive. Drawing on a content analysis of policy documents and interviews with humanitarian technologists, we identify three organizational strategies to justify working toward predictability: constantly seeking novel variables and data, maintaining ambiguity, and shifting models to adapt to changing circumstances. These strategies, we argue, sustain the claim that a predictable border is possible even when the technical reality of machine learning models does not live up to bureaucratic imaginaries. The so-called success of a predictable border does not solely derive from its technical capacity to estimate human mobility accurately but from creating a semblance of a predictable border inside an organization. © The Author(s) 2023.\n",
      "Purpose: Data science industry is a multidisciplinary field that deals with a large amount of data and derives useful information for taking routine and strategic business decisions. The purpose of this article is to examine the relationship between pricing models, engagement models, and firm performance (FP). This study also aims at uncovering the most effective pricing model and engagement model for improving FP. Design/methodology/approach: Indian data scientists were the respondents of the study. A total of 213 responses were carefully chosen. The data were analyzed using structural equations on Statistical Package for Social Sciences-Analysis of Moment Structures (SPSS-AMOS) version 25 software. Findings: The findings of the study suggested the positive and significant impact of pricing models and engagement models on FP. Value-based pricing strategies have the maximum impact on FP. On the other hand, managed services have a higher influence on FP. Originality/value: By developing a multi-faceted framework, this study is a novel contribution to the field of business strategy, especially for the data science industry. © 2023, Emerald Publishing Limited.\n",
      "As chemistry expands to more complex and interdisciplinary areas, a new generation of diverse researchers must engage with science and learn effective cross-disciplinary collaboration and communication. To these ends, we designed and implemented In the Mix, a graduate student-led, two-day workshop for undergraduate students promoting collaborative science in the context of energy storage innovations. The interactive workshop was designed for future and emerging researchers to gain hands-on experience with data science, computational chemistry, and electrochemistry techniques that are critical for developing materials for battery technologies. Participants also visited commercial renewable energy facilities to help them connect discovery-based research with industry and broader societal considerations. The workshop content and structure ensured that participants experienced the interrelatedness of the fields and understood the importance of collaborative research to yield scientific advances with real-world applications. An external team evaluated the workshop and participants’ perceptions of their experiences. While our research context was energy storage, the workshop goals and outcomes are applicable to other contexts. Interdisciplinary, experiential workshops are a key avenue to broadening participation in science and research, and the ideas presented here can be readily modified for other scientific contexts and/or incorporated as broader impact activities. © 2024 American Chemical Society and Division of Chemical Education, Inc.\n"
     ]
    }
   ],
   "source": [
    "delay = 1\n",
    "for i in range(20):\n",
    "    abstract = fetch_abstract(df['link'][i],delay)\n",
    "    print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_extract = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dear_need_help = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Access forbidden - Possible login required'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_abstract(df['link'][0],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstracts_from_df(df):\n",
    "    delay = 50\n",
    "    for i in range(60,80):\n",
    "        abstract = fetch_abstract(df['link'][i],delay)\n",
    "        dear_need_help.append(abstract)\n",
    "        print(abstract)\n",
    "    return  dear_need_help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access forbidden - Possible login required\n",
      "Access forbidden - Possible login required\n",
      "Access forbidden - Possible login required\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mextract_abstracts_from_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[88], line 4\u001b[0m, in \u001b[0;36mextract_abstracts_from_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m delay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m60\u001b[39m,\u001b[38;5;241m80\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     abstract \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_abstract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlink\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     dear_need_help\u001b[38;5;241m.\u001b[39mappend(abstract)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(abstract)\n",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m, in \u001b[0;36mfetch_abstract\u001b[0;34m(link, delay)\u001b[0m\n\u001b[1;32m      3\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Request the page\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     10\u001b[0m         soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:1349\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "extract_abstracts_from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869\n",
      "[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 193, 194, 195, 196, 197, 198, 199, 200, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083]\n"
     ]
    }
   ],
   "source": [
    "#len(batch_extract)\n",
    "#print(batch_extract[60])\n",
    "ot =0\n",
    "index =[]\n",
    "for i in range(len(batch_extract)):\n",
    "    if(batch_extract[i] != None): #and batch_extract[i]!= '[No abstract available]'): \n",
    "        ot += 1\n",
    "    else:index.append(i)\n",
    "print(ot)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Urban computing with a data science approaches can play a pivotal role in understaning and analyzing the potential of these methods for strategic, short-term, and sustainable planning. The recent development in urban areas have progressed towards the data-driven smart sustainable approaches to resolve the complexities around urban areas. The urban system faces severe challenges and these are complicated to capture, predict, resolve and deliver. The current study advances an unconventional decision-support framework to integrate the complexities of science, urban sustainability theories, and data science, with a data-intensive science to incorporate grassroots initiatives for a top-down policies. This work will\\xa0influence\\xa0the urban data analytics to optimize the designs and solutions to enhance sustainability, efficiency, resilience, equity, and quality of life. This work emphasizes the significant trends of data-driven and model-driven decision support systems. This will help to address and create an optimal solution\\xa0for\\xa0multifaceted challenges of an urban setup\\xa0within the analytical framework. The analytical\\xa0investigations includes\\xa0the research about land use prediction, environmental monitoring, transportation modelling, and social equity analysis. The fusion of urban computing, intelligence, and sustainability science is expected to resolve and contribute in shaping resilient, equitable, and future environmentally sensible eco-cities. It examines the emerging trends in the domain of computational urban science and data science approaches for sustainable development being utilized to address urban challenges including resource management, environmental impact, and social equity. The analysis of recent improvements and case studies highlights the potential of data-driven insights with computational models for promoting resilient sustainable urban environments, towards more effective and informed policy-making. Thus, this work explores the integration of computational urban science and data science methodologies to advance sustainable development. © The Author(s) 2024.',\n",
       " 'Objectives: In 2016, the Department of Veterans Affairs (VA) and the Department of Energy (DOE) established an Interagency Agreement (IAA), the Million Veteran Program-Computational Health Analytics for Medical Precision to Improve Outcomes Now (MVP-CHAMPION) research collaboration. Materials and Methods: Oversight fell under the VA Office of Research Development (VA ORD) and DOE headquarters. An Executive Committee and 2 senior scientific liaisons work with VA and DOE leadership to optimize efforts in the service of shared scientific goals. The program supported centralized data management and genomic analysis including creation of a scalable approach to cataloging phenotypes. Cross-cutting methods including natural language processing, image processing, and reusable code were developed. Results: The 79.6 million dollar collaboration has supported centralized data management and genomic analysis including a scalable approach to cataloging phenotypes and launched over 10 collaborative scientific projects in health conditions highly prevalent in veterans. A ground-breaking analysis on the Summit and Andes supercomputers at the Oak Ridge National Laboratory (ORNL) of the genetic underpinnings of over 2000 health conditions across 44 million genetic variants which resulted in the identification of 38 270 independent genetic variants associating with one or more health traits. Of these, over 2000 identified associations were unique to non-European ancestry. Cross-cutting methods have advanced state-of-the-art artificial intelligence (AI) including large language natural language processing and a system biology study focused on opioid addiction awarded the 2018 Gordon Bell Prize for outstanding achievement in high-performance computing. The collaboration has completed work in prostate cancer, suicide prevention, and cardiovascular disease, and cross-cutting data science. Predictive models developed in these projects are being tested for application in clinical management. Discussion: Eight new projects were launched in 2023, taking advantage of the momentum generated by the previous collaboration. A major challenge has been limitations in the scope of appropriated funds at DOE which cannot currently be used for health research. Conclusion: Extensive multidisciplinary interactions take time to establish and are essential to continued progress. New funding models for maintaining high-performance computing infrastructure at the ORNL and for supporting continued collaboration by joint VA-DOE research teams are needed. © 2024 Published by Oxford University Press on behalf of the American Medical Informatics Association.',\n",
       " 'This paper explores regional planning as an applied field of practice in Africa, where it is observed as a failed project or a flawed endeavour. In theory, regional planning (the practice) is based on regional science (the field of study) from which regional policy and regional development flow. The article advances the argument that Africa did not benefit from regional science in the past due to data scarcity. However, with the advent of big data and the data deluge caused by big data, regional science and regional planning stand to benefit. We argue that with the proliferation of big data, Africa can now tap into its opportunities and correct its ‘ugly’ past regarding regional planning. With big data and artificial intelligence (AI)-tools, coupled with skilled and capable people, Africa can effectively mine data and catch up with global trends in urban and regional planning. The study concludes that there is a need to embrace big data and mine data for African regional science and regional planning to be successful. In addition, the African regional planning project was not a failure, but it suffered a stillbirth due to a lack of data and related infrastructure. © 2024 The Authors',\n",
       " 'Model Predictive Control (MPC) is an established control framework, based on the solution of an optimisation problem to determine the (optimal) control action at each discrete-time sample. Accordingly, major theoretical advances have been provided in the literature, such as closed-loop stability and recursive feasibility certificates, for the most diverse kinds of processes descriptions. Nevertheless, identifying good, trustworthy models for complex systems is a task heavily affected by uncertainties. As of this, developing MPC algorithms directly from data has recently received a considerable amount of attention over the last couple of years. In this work, we review the available data-based MPC formulations, which range from reinforcement learning schemes, adaptive controllers, and novel solutions based on behavioural theory and trajectory representations. In particular, we examine the recent research body on this topic, highlighting the main features and capabilities of available algorithms, while also discussing the fundamental connections among approaches and, comparatively, their advantages and limitations. © 2024 Elsevier Ltd',\n",
       " 'Purpose The relationship between the height of the V wave in the central venous pressure (CVP) waveform and the severity of tricuspid regurgitation (TR) is well known. Their diagnostic ability is unconfirmed. This study explored CVP waveform variations with TR. Methods All patients who underwent preoperative echocardiography and CVP waveform measurements before surgery at our institution were included. Indices were created to capture each feature of the CVP waveform. The median value for each case was obtained and statistically analyzed according to the severity of TR. A deep learning technique, Transformer, was used to handle the complex features of CVP waveforms. Results This study included 436 cases. The values for C wave – Y descent, X descent – Y descent, and V wave – Y descent differed significantly in the Jonckheere–Terpstra test (p = 0.0018, 0.027, and 0.077, respectively). The area under the receiver operating characteristic (ROC) curve (AUC) for X descent – Y descent in two groups, none to moderate TR and severe TR, was 0.83 (95% confidence interval (CI) [0.68, 0.98]). For Transformer, the accuracy of the validation dataset was 0.97. Conclusions The shape of the CVP waveform varied with the severity of TR in a large dataset. © The Author(s) 2024.',\n",
       " 'In the context of rapid urbanization, accurately identifying the visual factors that influence environmental safety perception is crucial for improving urban transportation environments and enhancing pedestrian safety. With the increase in urban population density and traffic flow, optimizing urban environmental design to elevate residents’ sense of safety has become a key issue in urban planning and management. However, the existing studies face numerous challenges in conducting large-scale quantitative analysis of environmental safety perception in complex scenarios, such as difficulties in data acquisition and limitations in analytical methods. This study addresses these challenges by applying image semantic segmentation and object detection techniques to extract key visual elements from street view images, combined with manual scoring and deep learning methods, to construct a road safety perception dataset. Using a LightGBM model and the SHAP interpretation framework, in this study, we identify the critical visual factors influencing environmental safety perception. An empirical study was conducted in Macau, a modern city where Eastern and Western cultures intersect, and tourism thrives. The findings reveal that: ① The overall environmental safety perception in the eight parishes and surrounding roads of Macau is relatively high, with significant regional differences in safety perception scores around Macau’s parish roads; ② The proportions of buildings, sidewalks, roads, and trees in images are the four primary factors influencing environmental safety perception; ③ The proportions and quantities of visual elements interact with each other, and their reasonable distribution helps form clear spatial visibility and creates conducive activity spaces, thereby enhancing the perception of environmental safety. Through empirical analysis, this study uncovers the mechanisms by which visual elements in urban street scenes affect environmental safety perception, providing scientific evidence for urban planning and transportation environment improvement. The research holds theoretical significance and offers practical references for urban design and management, demonstrating broad application value. © The Author(s) 2024.',\n",
       " '2D magnetotelluric (MT) imaging detects underground structures by measuring electromagnetic fields. This study tackles two issues in the field: traditional methods’ limitations due to insufficient forward modeling data, and the challenge of multiple solutions in complex scenarios. We introduce an enhanced 2D MT imaging approach with a novel self-attention mechanism, involving: 1. Generating diverse geophysical models and responses to increase data variety and volume. 2. Creating a Swin–Unet-based 2D MT Imaging network with self-attention for better modeling and relation capture, incorporating a MT sample generator using real data to lessen large-scale supervised training dependence, and refining the loss function for optimal validation. This method also includes eliminating MT background response to boost training efficiency and reduce training time. 3. Applying a transverse electric/transverse magnetic method for comprehensive 2D MT data response. Tests show that our method greatly improves 2D MT imaging’s accuracy and efficiency, with excellent generalization. © The Author(s) 2024.',\n",
       " 'Defined as the merging of social and environmental sustainability into corporate operations, sustainable entrepreneurship has embraced data science more and more to improve operational effectiveness and decision-making. Using statistics, machine learning, and computer science to uncover insights from challenging datasets, this interdisciplinary method blends the ideas of sustainability with sophisticated data analysis approaches. Our research supports the choice of this issue by stressing the urgent requirement of sophisticated analytical instruments to negotiate the complexity of sustainable business practices. We compare our proposed model against Logistic Regression, Feedforward Neural Networks, and Support Vector Machines (SVMs). This not only shows how better CNN models are for certain uses but also highlights the general possibilities of data science in promoting sustainability in business. Our results highlight the transforming ability of sophisticated machine learning methods in promoting informed, sustainable decision-making and supporting the more general conversation on sustainable business. © 2024 Elsevier Inc.',\n",
       " 'This study introduces a cutting-edge profit health assessment framework that merges signaling theory and agency theory within a data science context, leveraging both financial and nonfinancial indicators to provide a comprehensive, multidimensional evaluation of earnings quality in publicly traded companies. This model transcends conventional earnings management frameworks by focusing on the holistic earnings condition of firms and addressing challenges related to information asymmetry and managerial motives. The empirical analysis utilizes data from small and medium-sized private enterprises listed on China’s SME Board between 2018 and 2022, employing machine learning algorithms to rigorously validate the model’s effectiveness. The results reveal that 85.7% of the penalized firms by the China Securities Regulatory Commission from 2018 to 2023 exhibited low levels of profit health. This data science driven analysis deepens the understanding of corporate earnings health for regulators and investors. The proposed framework not only expands the theoretical underpinnings of earnings management but also serves as a novel evaluative tool for stakeholders, paving the way for broader application across diverse markets and sectors and rethinking earnings quality assessment methodologies through a data science lens. © The Author(s) 2024.',\n",
       " 'Data-informed decision making is a critical goal for many community-based public health research initiatives. However, community partners often encounter challenges when interacting with data. The Community-Engaged Data Science (CEDS) model offers a goal-oriented, iterative guide for communities to collaborate with research data scientists through data ambassadors. This study presents a case study of CEDS applied to research on the opioid epidemic in 18 counties in Ohio as part of the HEALing Communities Study (HCS). Data ambassadors provided a pivotal role in empowering community coalitions to translate data into action using key steps of CEDS which included: data landscapes identifying available data in the community; data action plans from logic models based on community data needs and gaps of data; data collection/sharing agreements; and data systems including portals and dashboards. Throughout the CEDS process, data ambassadors emphasized sustainable data workflows, supporting continued data engagement beyond the HCS. The implementation of CEDS in Ohio underscored the importance of relationship building, timing of implementation, understanding communities’ data preferences, and flexibility when working with communities. Researchers should consider implementing CEDS and integrating a data ambassador in community-based research to enhance community data engagement and drive data-informed interventions to improve public health outcomes. © The Author(s) 2024.',\n",
       " 'The Structural Genomics Consortium is an international open science research organization with a focus on accelerating early-stage drug discovery, namely hit discovery and optimization. We, as many others, believe that artificial intelligence (AI) is poised to be a main accelerator in the field. The question is then how to best benefit from recent advances in AI and how to generate, format and disseminate data to enable future breakthroughs in AI-guided drug discovery. We present here the recommendations of a working group composed of experts from both the public and private sectors. Robust data management requires precise ontologies and standardized vocabulary while a centralized database architecture across laboratories facilitates data integration into high-value datasets. Lab automation and opening electronic lab notebooks to data mining push the boundaries of data sharing and data modeling. Important considerations for building robust machine-learning models include transparent and reproducible data processing, choosing the most relevant data representation, defining the right training and test sets, and estimating prediction uncertainty. Beyond data-sharing, cloud-based computing can be harnessed to build and disseminate machine-learning models. Important vectors of acceleration for hit and chemical probe discovery will be (1) the real-time integration of experimental data generation and modeling workflows within design-make-test-analyze (DMTA) cycles openly, and at scale and (2) the adoption of a mindset where data scientists and experimentalists work as a unified team, and where data science is incorporated into the experimental design. © The Author(s) 2024.',\n",
       " 'Background: Health Data Science (HDS) is a novel interdisciplinary field that integrates biological, clinical, and computational sciences with the aim of analysing clinical and biological data through the utilisation of computational methods. Training healthcare specialists who are knowledgeable in both health and data sciences is highly required, important, and challenging. Therefore, it is essential to analyse students’ learning experiences through artificial intelligence techniques in order to provide both teachers and learners with insights about effective learning strategies and to improve existing HDS course designs. Methods: We applied artificial intelligence methods to uncover learning tactics and strategies employed by students in an HDS massive open online course with over 3,000 students enrolled. We also used statistical tests to explore students’ engagement with different resources (such as reading materials and lecture videos) and their level of engagement with various HDS topics. Results: We found that students in HDS employed four learning tactics, such as actively connecting new information to their prior knowledge, taking assessments and practising programming to evaluate their understanding, collaborating with their classmates, and repeating information to memorise. Based on the employed tactics, we also found three types of learning strategies, including low engagement (Surface learners), moderate engagement (Strategic learners), and high engagement (Deep learners), which are in line with well-known educational theories. The results indicate that successful students allocate more time to practical topics, such as projects and discussions, make connections among concepts, and employ peer learning. Conclusions: We applied artificial intelligence techniques to provide new insights into HDS education. Based on the findings, we provide pedagogical suggestions not only for course designers but also for teachers and learners that have the potential to improve the learning experience of HDS students. © The Author(s) 2024.',\n",
       " 'Background: Models for structuring big-data and data-analytics projects typically start with a definition of the project’s goals and the business value they are expected to create. The literature identifies proper project definition as crucial for a project’s success, and also recognizes that the translation of business objectives into data-analytic problems is a difficult task. Unfortunately, common project structures, such as CRISP-DM, provide little guidance for this crucial stage when compared to subsequent project stages such as data preparation and modeling. Contribution: This paper contributes structure to the project-definition stage of data-analytic projects by proposing the Data-Analytic Problem Structure (DAPS). The diagrammatic technique facilitates the collaborative development of a consistent and precise definition of a data-analytic problem, and the articulation of how it contributes to the organization’s goals. In addition, the technique helps to identify important assumptions, and to break down large ambitions in manageable subprojects. Methods: The semi-formal specification technique took other models for problem structuring — common in fields such as operations research and business analytics — as a point of departure. The proposed technique was applied in 47 real data-analytic projects and refined based on the results, following a design-science approach. © The Author(s) 2024.',\n",
       " 'Instagram is one of the most popular and widely used social network platforms. It is used as a digital tool to connect with other users and also to share information and influence them for marketing and advertising purposes. The influence of popular users is broadly determined by post’s engagement rate in terms of likes, comments, and shares, and the number of followers as well. An objective and comprehensive measure of popularity is necessary to understand the factors that will help make an influencer marketing campaign more successful and beneficial for business activities. This research work attempts to take various features of an influencer account and Instagram posts dataset and develop a novel model that accurately quantifies and determines the influence of a user on Instagram. The research is based on datasets of top regional Instagram influencers and their posts based on categories signified through hashtags and captions. Our research attempts to develop a model using principal component analysis to quantify influence and using it to rank influencers. In our experiment, the proposed model after experimentation, gave the Instagram username “iqbaal.e” influence score as 874,712.9526, username “1nctdream” as 753,830.5847 and username “weareone. exo” as 668,054.4360. The proposed model ranks were compared with other ranks for Instagram users based on other measures such as follower rank etc. User names “huyitian”, “bintangemon” and “bimopd” are top social media influencers based on the proposed model for better business advertising and digital marketing outcomes with collected data and experiment context. This proposed approach gives an exploration for the stakeholders to quantify the impact of influencer in social media and demonstrate an innovative approach. © The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature 2024.',\n",
       " '[No abstract available]',\n",
       " 'Background: Economic incentives can improve clinical outcomes among in-care people living with HIV (PLHIV), but evidence is limited for their effectiveness among out-of-care PLHIV or those at risk of disengagement. We propose a type 1 hybrid effectiveness-implementation study to advance global knowledge about the use of economic incentives to strengthen the continuity of HIV care and accelerate global goals for HIV epidemic control. Methods: The Rudi Kundini, Pamoja Kundini study will evaluate two implementation models of an economic incentive strategy for supporting two groups of PLHIV in Tanzania. Phase 1 of the study consists of a two-arm, cluster randomized trial across 32 health facilities to assess the effectiveness of a home visit plus one-time economic incentive on the proportion of out-of-care PLHIV with viral load suppression (< 1000 copies/ml) 6\\xa0months after enrollment (n = 640). Phase 2 is an individual 1:1 randomized controlled trial designed to determine the effectiveness of a short-term counseling and economic incentive program offered to in-care PLHIV who are predicted through machine learning to be at risk of disengaging from care on the outcome of viral load suppression at 12\\xa0months (n = 692). The program includes up to three incentives conditional upon visit attendance coupled with adapted counselling sessions for this population of PLHIV. Consistent with a hybrid effectiveness-implementation study design, phase 3 is a mixed methods evaluation to explore barriers and facilitators to strategy implementation in phases 1 and 2. Results will be used to guide optimization and scale-up of the incentive strategies, if effective, to the larger population of Tanzanian PLHIV who struggle with continuity of HIV care. Discussion: Innovative strategies that recognize the dynamic process of lifelong retention in HIV care are urgently needed. Strategies such as conditional economic incentives are a simple and effective method for improving many health outcomes, including those on the HIV continuum. If coupled with other supportive services such as home visits (phase 1) or with tailored counselling (phase 2), economic incentives have the potential to strengthen engagement among the subpopulation of PLHIV who struggle with retention in care and could help to close the gap towards reaching global “95–95-95” goals for ending the AIDS epidemic. Trial registration: Phase 1: ClinicalTrials.gov, NCT05248100, registered 2/21/2022. Phase 2: ClinicalTrials.gov, NCT05373095, registered 5/13/2022. © The Author(s) 2024.',\n",
       " 'Biomedical deployments of data science capitalize on vast, heterogeneous data sources. This promotes a diversified understanding of what counts as evidence for health-related interventions, beyond the strictures associated with evidence-based medicine. Focusing on COVID-19 transmission and prevention research, I consider the epistemic implications of this diversification of evidence in relation to (1) experimental design, especially the revival of natural experiments as sources of reliable epidemiological knowledge; and (2) modeling practices, particularly the recognition of transdisciplinary expertise as crucial to developing and interpreting data models. Acknowledging such shifts in evidential, experimental, and modeling practices helps avoid harmful applications of data-intensive methods. © The Author(s), 2023.',\n",
       " 'This article examines how claims to predictable borders via data science techniques are crafted in bureaucratic institutions. Through a case study of testing algorithmic systems at a transnational agency, we examine how humanitarian organizations reconcile the risks of predictive technologies with the benefits they claim to receive. Drawing on a content analysis of policy documents and interviews with humanitarian technologists, we identify three organizational strategies to justify working toward predictability: constantly seeking novel variables and data, maintaining ambiguity, and shifting models to adapt to changing circumstances. These strategies, we argue, sustain the claim that a predictable border is possible even when the technical reality of machine learning models does not live up to bureaucratic imaginaries. The so-called success of a predictable border does not solely derive from its technical capacity to estimate human mobility accurately but from creating a semblance of a predictable border inside an organization. © The Author(s) 2023.',\n",
       " 'Purpose: Data science industry is a multidisciplinary field that deals with a large amount of data and derives useful information for taking routine and strategic business decisions. The purpose of this article is to examine the relationship between pricing models, engagement models, and firm performance (FP). This study also aims at uncovering the most effective pricing model and engagement model for improving FP. Design/methodology/approach: Indian data scientists were the respondents of the study. A total of 213 responses were carefully chosen. The data were analyzed using structural equations on Statistical Package for Social Sciences-Analysis of Moment Structures (SPSS-AMOS) version 25 software. Findings: The findings of the study suggested the positive and significant impact of pricing models and engagement models on FP. Value-based pricing strategies have the maximum impact on FP. On the other hand, managed services have a higher influence on FP. Originality/value: By developing a multi-faceted framework, this study is a novel contribution to the field of business strategy, especially for the data science industry. © 2023, Emerald Publishing Limited.',\n",
       " 'As chemistry expands to more complex and interdisciplinary areas, a new generation of diverse researchers must engage with science and learn effective cross-disciplinary collaboration and communication. To these ends, we designed and implemented In the Mix, a graduate student-led, two-day workshop for undergraduate students promoting collaborative science in the context of energy storage innovations. The interactive workshop was designed for future and emerging researchers to gain hands-on experience with data science, computational chemistry, and electrochemistry techniques that are critical for developing materials for battery technologies. Participants also visited commercial renewable energy facilities to help them connect discovery-based research with industry and broader societal considerations. The workshop content and structure ensured that participants experienced the interrelatedness of the fields and understood the importance of collaborative research to yield scientific advances with real-world applications. An external team evaluated the workshop and participants’ perceptions of their experiences. While our research context was energy storage, the workshop goals and outcomes are applicable to other contexts. Interdisciplinary, experiential workshops are a key avenue to broadening participation in science and research, and the ideas presented here can be readily modified for other scientific contexts and/or incorporated as broader impact activities. © 2024 American Chemical Society and Division of Chemical Education, Inc.',\n",
       " 'Laser-Induced Breakdown Spectroscopy (LIBS) is a versatile and powerful analytical technique widely used for rapid, in situ elemental analysis across various fields, from industrial quality control to planetary exploration. This review addresses the critical aspects and emerging trends in LIBS, focusing on calibration challenges, integrating complementary techniques (data fusion), and applying data science for spectral analysis. Calibration is a fundamental challenge in LIBS due to matrix effects, signal drift, and variations in experimental conditions. Recent advancements aim to develop matrix-independent calibration models and employ machine learning algorithms to improve calibration accuracy and robustness. LIBS has also proven invaluable in space exploration, particularly on Mars. Instruments like ChemCam and SuperCam have successfully utilized LIBS to perform real-time chemical analysis of the Martian surface, providing critical insights into its composition and history. The review further explores the advancements in multivariate calibration techniques for handling complex and multi-component systems. Techniques such as Partial Least Squares (PLS) regression and Principal Component Analysis (PCA) are increasingly employed to address the high dimensionality of LIBS data, enhancing the precision and reliability of the analysis. In addition, combining LIBS with other instrumental analytical techniques expands its analytical capabilities. Data fusion strategies integrating LIBS with techniques like Raman spectroscopy, X-ray fluorescence (XRF), and hyperspectral imaging provide a more comprehensive understanding of material composition. These integrated systems, supported by sophisticated data fusion algorithms, offer unprecedented insights and accuracy. Finally, applying data science in LIBS transforms spectral inspection and analysis. Machine learning and deep learning methods are being adopted to automate and enhance the processing and interpretation of LIBS spectra, uncovering complex patterns and improving analysis accuracy. The future lies in leveraging big data analytics and real-time processing to address more complex analytical challenges. In conclusion, LIBS is evolving rapidly, driven by advancements in calibration methods, techniques integration, and data science. This review highlights the potential of LIBS to continue pushing the boundaries of material analysis and its significant contributions to diverse scientific and industrial fields. © 2024 The Royal Society of Chemistry.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'Laser ablation is a vital material removal technique, but current methods lack a data-driven approach to assess quality. This study proposes a novel method, employing information entropy, a concept from data science, to evaluate laser ablation quality. By analyzing the randomness associated with the ablation process through the distribution of a probability value (reb), we quantify the uncertainty (entropy) of the ablation. Our research reveals that higher energy levels lead to lower entropy, signifying a more controlled and predictable ablation process. Furthermore, using an interval time closer to the baseline value improves the ablation consistency. Additionally, the analysis suggests that the energy level has a stronger correlation with entropy than the baseline interval time (bit). The entropy decreased by 6.32 from 12.94 at 0.258 mJ to 6.62 at 0.378 mJ, while the change due to the bit was only 2.12 (from 10.84 at bit/2 to 8.72 at bit). This indicates that energy is a more dominant factor for predicting ablation quality. Overall, this work demonstrates the feasibility of information entropy analysis for evaluating laser ablation, paving the way for optimizing laser parameters and achieving a more precise material removal process. © 2024 by the authors.',\n",
       " 'Background: The exposome (e.g., totality of environmental exposures) and its role in Alzheimer’s Disease and Alzheimer’s Disease and Related Dementias (AD/ADRD) are increasingly critical areas of study. However, little is known about how interventions on the exposome, including personal behavioral modification or policy-level interventions, may impact AD/ADRD disease burden at the population level in real-world settings and the cost-effectiveness of interventions. Methods: We performed a critical review to discuss the challenges in modeling exposome interventions on population-level AD/ADRD burden and the potential of using agent-based modeling (ABM) and other advanced data science methods for causal inference to achieve this. Results: We describe how ABM can be used for empirical causal inference modeling and provide a virtual laboratory for simulating the impacts of personal and policy-level interventions. These hypothetical experiments can provide insight into the optimal timing, targeting, and duration of interventions, identifying optimal combinations of interventions, and can be augmented with economic analyses to evaluate the cost-effectiveness of interventions. We also discuss other data science methods, including structural equation modeling and Mendelian randomization. Lastly, we discuss challenges in modeling the complex exposome, including high dimensional and sparse data, the need to account for dynamic changes over time and over the life course, and the role of exposome burden scores developed using item response theory models and artificial intelligence to address these challenges. Conclusions: This critical review highlights opportunities and challenges in modeling exposome interventions on population-level AD/ADRD disease burden while considering the cost-effectiveness of different interventions, which can be used to aid data-driven policy decisions. © 2024 by the authors.',\n",
       " 'Teachers’ professional learning often includes online components. This study examined how a case of 37 teachers utilized a specific online asynchronous professional learning platform designed to support teachers’ growth in learning to teach statistics and data science in secondary schools in the United States. The platform’s features and learning materials were designed based on effective online learning designs, supports for self-guided learning, and research on the teaching and learning of statistics and data science. We paid particular attention to the features we designed into the platform to support self-regulation and personalizing the experiences to meet their preferred learning goals such as allowing for free choice of learning materials, flexibility of when and how long to engage, providing personal recommendations based on user input, internal systems to track progress, and generating certificates of completion. In this study, we used a case study with both quantitative and qualitative data to examine whether teachers had gains in meeting learning goals related to their development in teaching statistics and data science, had sustained engagement, and found the features for personalization supportive for their learning. Results showed, overall, positive growth towards meeting learning goals and making small changes towards improved classroom practice. Most teachers were generally engaged in sustained ways across the study period, though we found six different patterns of completion that highlight ways in which teachers’ goal-directed and self-regulated learning occurred within the busy schedules of educators. Several personalized features, especially the recommendations and tracking system, were highly utilized and perceived as supportive of teachers’ learning. © 2024 by the authors.',\n",
       " '[No abstract available]',\n",
       " 'Digital twins offer a new and exciting framework that has recently attracted significant interest in fields such as oncology, immunology, and cardiology. The basic idea of a digital twin is to combine simulation and learning to create a virtual model of a physical object. In this paper, we explore how the concept of digital twins can be generalized into a broader, overarching field. From a theoretical standpoint, this generalization is achieved by recognizing that the duality of a digital twin fundamentally connects complexity science with data science, leading to the emergence of complexity data science as a synthesis of the two. We examine the broader implications of this field, including its historical roots, challenges, and opportunities. © The Author(s) 2024.',\n",
       " 'Computer vision education is increasingly important in modern technology curricula; yet, it often lacks a systematic approach integrating both theoretical concepts and practical applications. This study proposes a staged framework for computer vision education designed to progressively build learners’ competencies across four levels. This study proposes a four-staged framework for computer vision education, progressively introducing concepts from basic image recognition to advanced video analysis. Validity assessments were conducted twice with 25 experts in the field of AI education and curricula. The results indicated high validity of the staged framework. Additionally, a pilot program, applying computer vision to acid–base titration activities, was implemented with 40 upper secondary school students to evaluate the effectiveness of the staged framework. The pilot program showed significant improvements in students’ understanding and interest in both computer vision and scientific inquiry. This research contributes to the AI educational field by offering a structured, adaptable approach to computer vision education, integrating AI, data science, and computational thinking. It provides educators with a structured guide for implementing progressive, hands-on learning experiences in computer vision, while also highlighting areas for future research and improvement in educational methodologies. © 2024 by the authors.',\n",
       " 'Recent calls to take up data science either revolve around the superior predictive performance associated with machine learning or the potential of data science techniques for exploratory data analysis. Many believe that these strengths come at the cost of explanatory insights, which form the basis for theorization. In this paper, we show that this trade-off is false. When used as a part of a full research process, including inductive, deductive and abductive steps, machine learning can offer explanatory insights and provide a solid basis for theorization. We present a systematic five-step theory-building and theory-testing cycle that consists of: 1. Element identification (reduction); 2. Exploratory analysis (induction); 3. Hypothesis development (retroduction); 4. Hypothesis testing (deduction); and 5. Theorization (abduction). We demonstrate the usefulness of this approach, which we refer to as co-duction, in a vignette where we study firm growth with real-world observational data. This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication.',\n",
       " 'The massive accumulation of available multi-modal mineral exploration data for most metallogenic belts worldwide provides abundant information for the discovery of mineral resources. However, managing and analyzing these ever-growing and multidisciplinary mineral exploration data has become increasingly time-consuming and labor-intensive. Artificial intelligence (AI) has demonstrated powerful prediction and knowledge integration capabilities, enabling geologists to efficiently leverage mineral exploration data. This paper reviews publications on state-of-the-art AI applications for ten mineral exploration tasks ranging from data mining to grade and tonnage estimation. These studies are based on expert systems, fuzzy logic, and various machine learning algorithms designed to optimize and improve the workflow of mineral exploration. We recognize that most AI for mineral exploration is data-driven research for now. However, AI models that couple geological knowledge and mineral exploration data will be increasingly favored in this field in the future. This paper also discusses the challenges of AI in mineral exploration research and the implications of future developments associated with novel technologies and practical deployments. Although AI has not yet been extensively tested for practical deployment in mineral exploration, its study execution exhibits the potential to trigger a fundamental research paradigm shift. © 2024 Elsevier B.V.',\n",
       " 'Purpose of Review: The rising burden of cardiovascular disease (CVD) in Africa is of great concern. Health data sciences is a rapidly developing field which has the potential to improve health outcomes, especially in low-middle income countries with burdened healthcare systems. We aim to explore the current CVD landscape in Africa, highlighting the importance of health data sciences in the region and identifying potential opportunities for application and growth by leveraging health data sciences to improve CVD outcomes. Recent findings: While there have been a number of initiatives aimed at developing health data sciences in Africa over the recent decades, the progress and growth are still in their early stages. Its maximum potential can be leveraged through adequate funding, advanced training programs, focused resource allocation, encouraging bidirectional international partnerships, instituting best ethical practices, and prioritizing data science health research in the region. Summary: The findings of this review explore the current landscape of CVD and highlight the potential benefits and utility of health data sciences to address CVD challenges in Africa. By understanding and overcoming the barriers associated with health data sciences training, research, and application in the region, focused initiatives can be developed to promote research and development. These efforts will allow policymakers to form informed, evidence-based frameworks for the prevention and management of CVDs, and ultimately result in improved CVD outcomes in the region. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.',\n",
       " \"Request failed: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\",\n",
       " '[No abstract available]',\n",
       " 'As intelligent vehicles (IVs) continue to advance in fully connected environments, the collection of data from various sources in intelligent transportation systems (ITSs) has reached unprecedented levels. This paper aims to provide an integrative review of the processing and utilization of this vast data for optimizing smart mobility (SM) and extracting actionable insights to enhance planning and decision-making. While the data science (DS) frameworks have proven its effectiveness in sectors such as healthcare, tourism, social media, and the internet industries, there remains a lack of systematic research on DS in the context of SM (referred to as DS2M) within the ITS field. In this paper, we examine the potential applications of DS in IV systems by exploring relevant literature in DS domains, including discussions on data uncertainty, deep learning-based interpretability, reinforcement learning, and the relationships within IV data. These applications include IV control systems, data analytics visualisation, parallel-driving IV systems, and other DS2M applications. Furthermore, the analysis of seminal and recent literature emphasizes the absence of widely recognized benchmarks, which poses challenges to the validation and demonstration of new studies in this evolving domain. © 2024 Elsevier Ltd',\n",
       " \"Background: The language that children are exposed to in their early years is enhanced by children's picture books. It is important to better characterise this input, and recent research has begun to explore corpora of narrative picture books. However, previous research has been restricted by methodological limitations that make it difficult to develop large datasets. Further, information texts become increasingly important as children progress through school, but little is known about the language of their earliest form, namely, informational picture books. The current study investigates how informational and narrative picture book exposure might change the language environment of children in a way that supports reading development. Methods: The study applies data science methods to build a larger language model than previously possible and investigates the lexical profile of over 2000 narrative and information picture books. Picture book vocabulary is innovatively derived from digital sources of books read-aloud online, which pushes the field forward by providing researchers access to larger pools of data than previously possible. Detailed comparisons of informational and narrative picture books are reported regarding their lexical diversity, density, morphology, academic vocabulary and semantic clusters. Models are developed to estimate the additional word-type exposure a child may encounter in their language environment from narrative and informational picture books. Results: The study demonstrates that information and narrative picture books expose children to substantially different semantic environments. It is demonstrated that information picture books provide extensive exposure to academic vocabulary, providing important input aligned with later reading needs. Further, computational models indicate that book reading once every day or second day over a year might boost unique-word exposure by approximately 10% for some language environments. Conclusions: Combining informational and narrative picture books enhance the language environment of children more than narratives alone, providing more lexical diversity, density and complex morphology. © 2024 The Author(s). Journal of Research in Reading published by John Wiley & Sons Ltd on behalf of United Kingdom Literacy Association.\",\n",
       " 'Cybersecurity is the practice of protecting systems, networks, and data from digital attacks, theft, and damage. It involves implementing measures and technologies to safeguard information confidentiality, integrity, and availability. Machine learning (ML) is revolutionizing the field of cybersecurity by providing advanced tools and techniques to detect, analyze, and respond to cyber threats more effectively and efficiently. This paper proposed Ethereum Hashing Hyperbolic Cryptography (EHHC) for cyber security in the data science model. The proposed EHHC model comprises the Hyperbolic Curve Cryptography (HCC) model for data science security. With the integration of the HCC model in the Ethereum blockchain hashing is performed for data science data security. The proposed EHHC model is deployed in the Ethereum blockchain for data security for cyber security. The cyber threats are estimated and classified with the machine learning model for the classification of attacks using the CICIDS, UNSW-NB15 and KDD datasets. Through the incorporation of the EHHC model cyber threats are classified and detected for the different simulation environments. The results demonstrated that the proposed EHHC model achieves a higher classification accuracy of 96.1% with a minimal computation time of ~12% than the conventional cryptographic techniques. The results expressed a higher classification for cyber threat detection and classification in the data science environment. © Little Lion Scientific.',\n",
       " 'Catalysis of multicomponent transformations requires controlled assembly of reactants within the active site. Supramolecular scaffolds possess synthetic microenvironments that enable precise modulation over noncovalent interactions (NCIs) engaged by reactive, encapsulated species. While molecular properties that describe the behavior of single guests in host cavities have been studied extensively, multicomponent transformations remain challenging to design and deploy. Here, simple univariate regression and threshold analyses are employed to model reactivity in a cascade reduction of azaarenes catalyzed by water-soluble metal organic cages. Yield and stereoselectivity models help deduce unknown mechanisms of reactivity by the multicomponent, host-guest complexes. Furthermore, a comprehensive model is established for NCIs driving stereoselectivity in the reported host-guest adducts. © 2024 The Authors. Published by American Chemical Society.',\n",
       " 'Paralympic wheelchair athletes solely depend on the power of their upper-body for their on- court wheeled mobility as well as for performing sport-specific actions in ball sports, like a basketball shot or a tennis serve. The objective of WheelPower is to improve the power output of athletes in their sport-specific wheelchair to perform better in competition. To achieve this objective the current project systematically combines the three Dutch measurement innovations (WMPM, Esseda wheelchair ergometer, PitchPerfect system) to monitor a large population of athletes from different wheelchair sports resulting in optimal power production by wheelchair athletes during competition. The data will be directly implemented in feedback tools accessible to athletes, trainers and coaches which gives them the unique opportunity to adapt their training and wheelchair settings for optimal performance. Hence, the current consortium facilitates mass and focus by uniting scientists and all major Paralympic wheelchair sports to monitor the power output of many wheelchair athletes under field and lab conditions, which will be assisted by the best data science approach to this challenge. © Riemer J. K. Vegter, Rowie J. F. Janssen, Marit P. van Dijk, Marco J. M. Hoozemans, Dirkjan H. E. J. Veeger, Han J. H. P. Houdijk, Luc H. V. van der Woude, Monique A. M. Berger, Rienk M. A. van der Slikke, and Sonja de Groot.',\n",
       " 'Purpose: There is a need for precollege learning designs that empower youth to be epistemic agents in contexts that intersect burgeoning areas of computing, big data and social media. The purpose of this study is to explore how “sandbox” or open-inquiry data science with social media supports learning. Design/methodology/approach: This paper offers vignettes from an illustrative youth study case that highlights the pedagogical prospects and obstacles tied to designing for open-ended inquiry with computational data science to access or “scrape” Twitter/X. The youth case showcases how social media can be taken up productively and in ways that facilitate epistemological agency, an approach where individuals actively shape understanding and knowledge-creation processes, highlighting the potentially transformative impact this approach might have in empowering learners to engage productively. Findings: The authors identify three key affordances for learning that emerged from the illustrative case: (1) flexible opportunities for content-specific domain mastery, (2) situated inquiry that embodies next-generation science practices and (3) embedded computational skill development. The authors discuss these findings in relation to contemporary education needs to broaden participation in data science and computing. Originality/value: To address challenges in current data science education associated with supporting sustained and productive engagement in computing-based data science, the authors leverage a “sandbox” approach – an original pedagogical framework to support open inquiry with precollege groups. The authors demonstrate how “big data” drawn from social media with high school-aged youth supports learning designs and outcomes by emphasizing learner interests and authentic practice. © 2024, Emerald Publishing Limited.',\n",
       " 'Series manufacturers in the field of hydroforming do not always have the necessary database for predictive maintenance, especially for the production of complex hydro-formed components. Small and medium-sized companies in particular often lack the resources to acquire, process and profitably utilize these data sets under production conditions. The IHU processes that occur in practice are usually highly complex, both in terms of geometry and against the background of additional process steps (punching, plunging). This results in large amounts of data and complex data analyses, which are reflected in the data processing costs. A data analysis is to be carried out on the basis of a complex hydroforming process and its additional benefit for quality management and predictive maintenance is to be explained. © 2024 Walter de Gruyter GmbH, Berlin/Boston, Germany.',\n",
       " 'Balinese saka calendar has a system of good and bad days called Ala Ayuning Dewasa. This system included a variety of activities. One of them is cayenne pepper farming. The farmers in Bali have used the Ala Ayuning Dewasa as a guide in planting because they are believed to coincide with favorable natural conditions. However, there has been no empirical evidence regarding the suitability of the system. This study aims to begin empirical evidence by showing a correlation between this system and optimal air temperature and humidity in growing cayenne pepper. This study uses the formula y = sin(π/105° x + 90°) as a mathematical model of the Ala Ayuning Dewasa, where the maximum cycle value (y=1) represents a good day and the minimum value (y = -1) indicates a bad day. Correlation was measured against the initial assessment of the accuracy of a day as the beginning of cayenne pepper planting based on temperature and air humidity for three months after (time needed from planting to harvesting). The assessment was carried out using temperature and humidity data from October 1, 2014 to January 1, 2017. The comparison shows that there is no correlation between the two with correlation value of -0.161. However, the results of observations show that there is a similar pattern even with the beginning of a different phase. By revising the beginning of the Ala Ayuning Dewasa calendar by 60 days, the correlation value becomes 0.799. The suggestion from this initial research is to shift the beginning of Ala Ayuning Dewasa planting of cayenne pepper by 60 days to increase the correlation with optimal temperature and humidity conditions for cayenne pepper growth. © 2024 American Institute of Physics Inc.. All rights reserved.',\n",
       " \"The landscape of professional football is undergoing a profound transformation driven by advancements in information systems and data science. In this era of technological evolution, the integration of data analytics and specialized metrics has revolutionized decision-making processes within football management. Leveraging advanced wearable devices and state-of-the-art information systems, teams have access now to vast amounts of data, enabling more precise and informed decisions across various facets of the game, from match tactics to talent scouting and injury prevention. Drawing inspiration from the renowned Moneyball revolution in baseball, this paper examines the application and evolution of its principles within football, focusing on the case study of Brentford FC. Through a comprehensive analysis of primary and secondary data, including interviews, questionnaires, and statistics from official platforms, the development model of Brentford FC is examined. Results showcase the team's remarkable ascent, propelled by the implementation of innovative, advanced statistical analyses and composite performance indicators, including metrics like expected goals (xG). Brentford FC's success story exemplifies how a data-driven approach can empower even smaller clubs with limited financial resources to compete effectively against stronger opponents. By harnessing the power of data science and information systems, Brentford FC not only did it achieve promotion to the prestigious Premier League, but it also enhanced its financial value substantially. Its performance on the pitch, guided by sophisticated performance indicators, underscores the transformative potential of data-driven strategies in modern football management. Thus, this paper contributes to the understanding of how information systems and data science are reshaping the football landscape, offering valuable insights into the strategic adoption of analytics for sustainable success in professional football. © 2024 American Institute of Physics Inc.. All rights reserved.\",\n",
       " 'Rapidly advancing computer technology has demonstrated great potential in recent years to assist in the generation and discovery of promising molecular structures. Herein, we present a data science-centric “Design–Discovery–Evaluation” scheme for exploring novel polyimides (PIs) with desired dielectric constants (3). A virtual library of over 100 000 synthetically accessible PIs is created by extending existing PIs. Within the framework of quantitative structure–property relationship (QSPR), a model sufficient to predict 3 at multiple frequencies is developed with an R2of 0.9768, allowing further high-throughput screening of the prior structures with desired 3. Furthermore, the structural feature representation method of atomic adjacent group (AAG) is introduced, using which the reliability of high-throughput screening results is evaluated. This workflow identifies 9 novel PIs (3 >5 at 103Hz and glass transition temperatures between 250 °C and 350 °C) with potential applications in high-temperature capacitive energy storage, and confirms these promising findings by high-fidelity molecular dynamics (MD) simulations. © 2024 The Author(s)',\n",
       " '[No abstract available]',\n",
       " 'Leveraging vast neuroimaging and electrophysiological datasets, AI algorithms are uncovering patterns that offer unprecedented insights into brain structure and function. Neuroinformatics, the fusion of neuroscience and AI, is advancing technologies like brain-computer interfaces, AI-driven cognitive enhancement, and personalized neuromodulation for treating neurological disorders. These developments hold potential to improve cognitive functions, restore motor abilities, and create human–machine collaborative systems. Looking ahead, the convergence of neuroscience and AI is set to transform cognitive modeling, decision-making, and mental health interventions. This fusion mirrors the quest for nuclear fusion energy, both driven by the need to unlock profound sources of understanding. As STEM disciplines continue to drive core developments of foundational models of the brain, neuroinformatics promises to lead innovations in augmented intelligence, personalized healthcare, and effective decision-making systems. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'Background: Research on healthcare disparities in pediatric radiology is limited, leading to the persistence of missed care opportunities (MCO). We hypothesize that the COVID-19 pandemic exacerbated existing health disparities in access to pediatric radiology services. Objective: Evaluate the social determinants of health and sociodemographic factors related to pediatric radiology MCO before, during, and after the COVID-19 pandemic. Materials and methods: The study examined all outpatient pediatric radiology exams at a pediatric medical center and its affiliate centers from 03/08/19 to 06/07/21 to identify missed care opportunities. Logistic regression with the least absolute shrinkage and selection operator (LASSO) method and classification and regression tree (CART) analysis were used to explore factors and visualize relationships between social determinants and missed care opportunities. Results: A total of 62,009 orders were analyzed: 30,567 pre-pandemic, 3,205 pandemic, and 28,237 initial recovery phase. Median age was 11.34\\xa0years (IQR 5.24–15.02), with 50.8% females (31,513/62,009). MCO increased during the pandemic (1,075/3,205; 33.5%) compared to pre-pandemic (5,235/30,567; 17.1%) and initial recovery phase (4,664/28,237; 16.5%). The CART analysis identified changing predictors of missed care opportunities across different periods. Pre-pandemic, these were driven by exam-specific factors and patient age. During the pandemic, social determinants like income, distance, and ethnicity became key. In the initial recovery phase, the focus returned to exam-specific factors and age, but ethnicity continued to influence missed care, particularly in neurological exams for Hispanic patients. Logistic regression revealed similar results: during the pandemic, increased distance from the examination site (OR 1.1), residing outside the state (OR 1.57), Hispanic (OR 1.45), lower household income ($25,000–50,000 (OR 3.660) and $50,000–75,000 (OR 1.866)), orders for infants (OR 1.43), and fluoroscopy (OR 2.3) had higher odds. In the initial recovery phase, factors such as living outside the state (OR 1.19), orders for children (OR 0.79), and being Hispanic (OR 1.15) correlate with higher odds of MCO. Conclusion: The application of basic data science techniques is a valuable tool in uncovering complex relationships between sociodemographic factors and disparities in pediatric radiology, offering crucial insights into addressing inequalities in care. Graphical Abstract: (Figure presented.) © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.',\n",
       " '[No abstract available]',\n",
       " 'The session “Data Science for Integrated Dynamic Structural Biology” was a notable success at the joint congress of the 21st IUPAB and the 62nd BSJ (Biophysics Society of Japan). This session included four invited talks and one contributed talk, which together delved into recent advancements in computational methods integrating physics, experimental data, and bioinformatics to study the structure and dynamic properties of molecular assemblies and their interactions. © International Union for Pure and Applied Biophysics (IUPAB) and Springer-Verlag GmbH Germany, part of Springer Nature 2024.',\n",
       " '[No abstract available]',\n",
       " 'Radiotherapy aims to achieve a high tumor control probability while minimizing damage to normal tissues. Personalizing radiotherapy treatments for individual patients, therefore, depends on integrating physical treatment planning with predictive models of tumor control and normal tissue complications. Predictive models could be improved using a wide range of rich data sources, including tumor and normal tissue genomics, radiomics, and dosiomics. Deep learning will drive improvements in classifying normal tissue tolerance, predicting intra-treatment tumor changes, tracking accumulated dose distributions, and quantifying the tumor response to radiotherapy based on imaging. Mechanistic patient-specific computer simulations (‘digital twins’) could also be used to guide adaptive radiotherapy. Overall, we are entering an era where improved modeling methods will allow the use of newly available data sources to better guide radiotherapy treatments. © 2024',\n",
       " 'This work provides a review of data science methods that can be used to address a wide variety of business problems in the banking sector. The paper examines three modelling paradigms: the response, incremental response and the rate sensitivity to response approaches, emphasising the role they play to address these problems. These paradigms and the methods they involve are presented in combination with real cases to illustrate their potential in extracting valuable business insights from data. It is enhanced their usefulness to help business experts like risk managers, commercial managers, financial directors and chief executive officers to plan their strategies and guide decision making on the basis of the insights given by their outcomes. The scope of the work is twofold: it presents a unified view of the methods and how the fit the aforementioned paradigms while, at the same time, it examines some business cases for their application. Both issues will be of interest for technical and managerial teams involved in running data science projects in banking. © 2024 The Author(s). Expert Systems published by John Wiley & Sons Ltd.',\n",
       " 'The job market is evolving continuously due to changes in economic landscapes, technological improvements, and skill requirements. In the era of digitalization, a wealth of data is becoming available, opening up new opportunities for labor market analysis. Many stakeholders can make informed decisions if they benefit from accurate and timely insights about the job market. However, traditional data sources and methods used for labor market analysis often fall short of capturing the diversity and trends of the evolving job market. Recently, researchers started exploring various data sources by leveraging data science techniques, which makes information extraction achievable. This survey reviews recent research published between 2015 and 2022 on labor market analytics through data science techniques and discusses future research directions. 101 primary studies were classified and evaluated to identify the data sources utilized for job market analysis; the skill extraction methods and their type; the occupation and sector identification methods; and the application of the study conducted. Finally, we explore potential avenues for future research in this area. © 2024 Elsevier Ltd',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'Collaborative neuroscience requires systematic data management and analysis. How this is best done in practice remains unclear. Based on a survey across collaborative neuroscience projects, we document the current state of the art focusing on data integration, sharing, and researcher training. We propose best practices and list actions and policies to attain these goals. © 2024 Elsevier Inc.',\n",
       " 'Purpose. The purpose of our article is to research and forecast prices for agricultural products using the example of potato prices based on the most effective models using data science techniques. Methodology / approach. Various forecasting models are explored, starting from baseline models like decomposition and exponential smoothing models to more advanced techniques such as ARIMA, SARIMA, as well as deep learning models including neural network. The data is split into training and testing sets, and models are validated using cross-validation techniques and optimised through hyperparameter tuning. Model performance is evaluated using metrics such as MAE, MSE, RMSE, and MAPE. The selected model is then used to generate future price forecasts, with uncertainty quantified through confidence intervals. Results. The study successfully applied advanced data science techniques to forecast potato prices, leveraging a range of effective models. By analysing historical price data and using various forecasting methods, the research identified the most accurate models for predicting future price trends. The results demonstrate that the selected models can provide reliable forecasts. In particular, the results showed that the SARIMA (1,0,0)(0,1,1)[12] model could achieve good forecast results when applied to real problems and, thus, can be effectively used for forecasting tasks especially considering seasonality. In addition, it should be noted that the ETS (M, Ad, A) model has a higher prediction accuracy at the time intervals closest to the original data. The obtained results support using both models simultaneously for forecasting, which can compensate for the shortcomings of each of them. The models can be used separately, to more accurately predict the values for the required period, or a combination of them is also possible. Originality / scientific novelty. The study’s originality lies in development of methods for effectively accounting for seasonality in agricultural price data, such as using seasonal decomposition techniques or more advanced techniques that combine statistical and data science approaches. The novelty implies the implementation of real-time data processing and forecasting system allows for the timely prediction of price changes, enabling stakeholders to make more informed decisions. Practical value / implications. Forecasting potato prices holds significant practical value for various stakeholders. For farmers, accurate forecasts enable informed decisions on the optimal times to plant, harvest, and sell their crops, thereby optimising their profits. In the supply chain, distributors and retailers can use these forecasts to manage inventory more effectively and plan contracts, reducing waste and avoiding shortages. Policymakers benefit from forecasts by anticipating market fluctuations and stabilising prices, which supports both consumers and producers. For consumers, stable pricing ensures better budgeting and helps avoid sudden price spikes, making essential foods more affordable. Overall, accurate price forecasting enhances market efficiency by reducing uncertainty and aiding investors in managing risk. © 2024, Institute of Eastern European Research and Consulting. All rights reserved.',\n",
       " 'Educators face many practical challenges when introducing data science to students.School networks are strictly controlled by outside resources which creates a barrier to installing and managing both software and hardware.Our tools aim to alleviate these practical issues to support educators to teach data science to children in K-12 classes.Our micro:bit accessory the clip:bit allows students to collect their own data outdoors.All the data is gathered in a physical, transparent Classroom Cloudlet for analysis and visualisations.The tools are mobile, transparent and accessible to children and educators making teaching data science achievable. © 2024 Copyright held by the owner/author(s).',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'Abstract section not found',\n",
       " 'Correction to: International Journal of Data Science and Analytics https://doi.org/10.1007/s41060-024-00527-8. In the original article, the value in 7th row under infeasible (%) column of Table 3 is published incorrectly as 1.97, the correct value is 4.93. The value in 7th row under optimal (%) column is incorrectly published as 98.03, the correct value is 95.07. (Table presented.) Frequency of optimization statuses (Sect.\\xa04.2) by feature-selection method and search method for alternatives Feature selection Search Optimization status Infeasible (%) Feasible (%) Optimal (%) FCBF seq. 74.51 0.00 25.49 FCBF sim. (min) 73.07 1.73 25.20 FCBF sim. (sum) 73.07 2.19 24.75 MI seq. 4.93 0.00 95.07 MI sim. (min) 4.67 9.60 85.73 MI sim. (sum) 4.67 3.17 92.16 Model Gain seq. 4.93 0.00 95.07 Model Gain sim. (min) 4.67 5.55 89.79 Model Gain sim. (sum) 4.67 1.92 93.41 mRMR seq. 4.88 9.63 85.49 mRMR sim. (min) 4.67 49.04 46.29 mRMR sim. (sum) 4.67 67.39 27.95 Results with k=5, a∈{1,2,3,4,5}, and excluding Greedy Wrapper, which uses the solver for satisfiability checking rather than optimizing. Each row adds up to 100% The original article has been corrected. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.',\n",
       " 'As a wider range of organizations explore using data science systems, data science research has given growing attention to the role of domain experts. Most of this research still views data science systems as centered on the development of statistical models or algorithms by technical data scientists, with domain experts limited to the role of informers. Our paper turns attention to how domain experts mediate whether data science models or algorithms lead to action through their situated data practices. Drawing on ethnographic fieldwork and a pilot machine learning project at a craft brewery, we identify situations where the brewers’ data practices led to unreliable, incomplete data, and unpack how such data limited the effectiveness of data science activities. Extending research in CSCW and STS on domain experts’ data practices to the data science context, we aim to inform the design of data science systems that are more actionable for their end-users. © The Author(s), under exclusive licence to Springer Nature B.V. 2023.',\n",
       " 'Urban data science (UDS) is developing rapidly and starting to be widely adopted in urban planning research and curricula. However, economic development planners have been relatively slow in introducing UDS into their toolkits. This reality is a disservice to the subfield and students. This article discusses the motivation and current practices of UDS in economic development, identifies successes and challenges, and suggests actions moving forward. Professional training, curriculum innovation, and support from departments, institutions, and the broader academic and professional communities are called for. © The Author(s) 2022.',\n",
       " 'The proceedings contain 40 papers. The topics discussed include: NFDI4Health workflow and service for synthetic data generation, assessment and risk management; a pipeline for the usage of the core data set of the medical informatics initiative for process mining – a technical case report; semi-automatic export of electrophysiological metadata to NFDI4Health local data hubs: use case of microneurography odML-tables – a technical case report; early multimodal data integration for data-driven medical research – a scoping review; the concept of a versatile computing tool chain for utilizing the core data set of the medical informatics initiative in the INTERPOLAR Project; unlocking real-world health data from non-university locations through effective provision of tools and knowledge; and integration of trusted third party software into an EDC system for data protection – compliant identity management, consent management and pseudonymization in medical research studies.',\n",
       " 'Abstract section not found',\n",
       " 'In Data Science, there is a continual demand for statistical comparison to identify the most advantageous algorithms. Finding a software tool that facilitates the execution of multiple tests on different Data Science experiments without relying on additional libraries poses a challenge. This paper introduces StaTDS, an open-source library and web application implemented entirely in pure Python, designed to analyze, test, and compare Data Science algorithms. StaTDS implements all statistical tests without external dependencies. It ensures its durability and avoids future uncontrolled deprecated dependencies. With support for a wide variety of statistical tests (24 in total), StaTDS surpasses existing libraries dedicated to statistical testing. Moreover, the library incorporates tests to guide users in determining whether to employ parametric or non-parametric tests, such as the assessment of normality and homoscedasticity. This platform-independent library is available on GitHub under the GNU General Public License. © 2024 The Author(s)',\n",
       " \"The rapid advancement of Data Science, Artificial Intelligence, and Machine Learning has created a dynamic job market. In line with other professions, salaries are provided as a means of compensating professionals for their work. However, it is evident from previous research that salary levels vary across different job fields, as each field contributes uniquely to its respective domain. The magnitude of this contribution directly influences the salary structure within a field. To shed light on this phenomenon, this data analysis project aims to examine the salaries dataset. The project's objective is to identify the factors that influence salary levels in these fields through comprehensive analysis. By exploring these trends, we can gain insights into the continued value of these fields in the coming years. © 2024 Author(s).\",\n",
       " \"KDD AIDSH 2024 aims to foster discussions and developments that push the boundaries of Artificial Intelligence (AI) and Data Science (DS) in healthcare, enhance diagnostic accuracy and promote human-centric approaches to healthcare, thus stimulating future interdisciplinary collaborations. This year's symposium will focus on expanding the application of AI/DS in healthcare/medicine and bridging existing gaps. The workshop invites submissions of full papers as well as work-in-progress on the application of AI/DS in healthcare. The workshop will feature three invited talks from eminent speakers, spanning academia, industry, and clinical researchers. In addition, selected papers will be invited to publish in Health Data Science, a Science Partner Journal. This summary provides a brief description of the half-day workshop to be held on August 26th, 2024. The webpage for the workshop can be found at https://aimel.ai/kdd2024aidsh. © 2024 Copyright held by the owner/author(s).\",\n",
       " 'The European Data Science Day offers a full day focused exclusively on innovative KDD-relevant research and development projects from national and regional funding programs, as well as corporate, start-up, and nonprofit channels. The idea is to bring together a diverse community of researchers in Data Science, Machine Learning, Language Technologies, and Knowledge Discovery, as well as partnerships in the social and physical sciences/arts, to showcase the state-of-the-art in research and applications. © 2024 Copyright held by the owner/author(s).',\n",
       " 'Abstract section not found',\n",
       " 'In the healthcare landscape, data science (DS) methods have emerged as indispensable tools to harness real-world data (RWD) from various data sources such as electronic health records, claim and registry data, and data gathered from digital health technologies. Real-world evidence (RWE) generated from RWD empowers researchers, clinicians, and policymakers with a more comprehensive understanding of real-world patient outcomes. Nevertheless, persistent challenges in RWD (e.g., messiness, voluminousness, heterogeneity, multimodality) and a growing awareness of the need for trustworthy and reliable RWE demand innovative, robust, and valid DS methods for analyzing RWD. In this article, I review some common current DS methods for extracting RWE and valuable insights from complex and diverse RWD. This article encompasses the entire RWE-generation pipeline, from study design with RWD to data preprocessing, exploratory analysis, methods for analyzing RWD, and trustworthiness and reliability guarantees, along with data ethics considerations and open-source tools. This review, tailored for an audience that may not be experts in DS, aspires to offer a systematic review of DS methods and assists readers in selecting suitable DS methods and enhancing the process of RWE generation for addressing their specific challenges. Copyright © 2024 by the author(s).',\n",
       " 'Advances in biomedical data science and artificial intelligence (AI) are profoundly changing the landscape of healthcare. This article reviews the ethical issues that arise with the development of AI technologies, including threats to privacy, data security, consent, and justice, as they relate to donors of tissue and data. It also considers broader societal obligations, including the importance of assessing the unintended consequences of AI research in biomedicine. In addition, this article highlights the challenge of rapid AI development against the backdrop of disparate regulatory frameworks, calling for a global approach to address concerns around data misuse, unintended surveillance, and the equitable distribution of AI’s benefits and burdens. Finally, a number of potential solutions to these ethical quandaries are offered. Namely, the merits of advocating for a collaborative, informed, and flexible regulatory approach that balances innovation with individual rights and public welfare, fostering a trustworthy AI-driven healthcare ecosystem, are discussed. Copyright © 2024 by the author(s).',\n",
       " 'The undergraduate degree program in medical data science aims to train future data scientists with a medical lens to tackle healthcare challenges using a data-driven approach. The program is a collaborative effort within the Berlin University Alliance, addressing the lack of healthcare-focused data science education in Berlin and Germany. The curriculum covers mathematics, informatics, medical informatics, and medicine, featuring diverse didactic formats. Graduates will be equipped to lead data science and digital transformation projects in healthcare. © 2024 The Authors.',\n",
       " \"Data Science emerged as a new cross-disciplinary discipline at the intersection of statistics, computer science, and expertise in a specific domain, such as health and biology. The data science field, alongside other data-related professions, is continuously evolving. We conducted a study examining tasks assigned to first-year internship students pursuing a Master's degree in Health Data Science, exploring the missions, technologies employed and skills required, and internship alignment with students' training through semi-structured interviews with 32 participants. Three quarters of the students were placed in teams within the public sector. Among these entities, there were 11 hospitals and 12 universities. Although the majority of students did their internship as part of a methodological team, they often had a healthcare professional on their team. Nearly half of the missions involved descriptive analysis, followed by 9 missions focused on etiology or prediction and 8 missions on implementing a data warehouse. The majority of students had to perform data management and produce graphs, while only half conducted statistical analysis. The findings highlighted that data management remains a major challenge, and it should be taken into consideration when designing training programs. In future, it remains to determine whether this trend will continue with second-year students or if, with experience, they are more often assigned statistical analyses. © 2024 The Authors.\",\n",
       " 'Aspirations for artificial intelligence (AI) as a catalyst for scientific discovery are growing. High-profile successes deploying AI in domains such as protein folding have highlighted AI’s potential to unlock new frontiers of scientific knowledge. However, the pathway from AI innovation to deployment in research is not linear. Those seeking to drive a new wave of scientific progress through the application of AI require a diffusion engine that can enhance AI adoption across disciplines. Lessons from previous waves of technology change, experiences of deploying AI in real-world contexts and an emerging research agenda from the AI for science community suggest a framework for accelerating AI adoption. This framework requires action to build supply chains of ideas between disciplines; rapidly transfer technological capabilities through open research; create AI tools that empower researchers; and embed effective data stewardship. Together, these interventions can cultivate an environment of open data science that deliver the benefits of AI across the sciences. © 2024 The Author(s). Wolters Kluwer Health, Inc.',\n",
       " 'Abstract section not found',\n",
       " '[No abstract available]',\n",
       " 'With the development of digital pedagogical resources, courses, and the recent COVID-19 pandemic, there has been a rise in the use of video-based learning (VBL) and teaching as one of the primary methods of instruction. Additionally, in recent years, bioinformatics has surfaced as an integral discipline in life sciences, where scientists are able to manipulate and analyze large sets of data. As a result, the need for digitally enhanced undergraduate and graduate teaching of basic bioinformatics skill sets of an applied nature has become increasingly high. Here, we designed and implemented a set of video-based bioinformatics tutorials as an open educational resource to be taught in an online synchronous, asynchronous, as well as HyFlex setting. These tutorials were designed to identify a ligand against unknown amino acid and nucleotide sequences to unveil their context in diverse species. This was achieved by navigating online bioinformatic databases, performing multiple sequence alignment, phylogenetic analyses, protein structure prediction/comparison, and docking. In the end, students also completed a survey questionnaire outlining their experience with the VBL. By the end of the term, VBL enabled the students to learn and apply bioinformatic concepts and tools to predict the protein structure from an unknown sequence and dock it with the ligands. Students rated VBL as one of the most powerful learning mediums out of many used as part of the module. Bioinformatic videos, besides capturing and distributing the bioinformatic information, also provided an invigorating environment where students better learned, understood, and retained the content. American Chemical Society. Published 2024 by American Chemical Society and Division of Chemical Education, Inc.',\n",
       " 'Data science is playing a crucial role in enhancing food and supplement safety, ensuring that products meet regulatory standards and are safe for consumption. This chapter explores the application of data science techniques in monitoring and ensuring the safety and quality of food and dietary supplements. The authors examine the methodologies used for data collection, analysis, and predictive modeling to detect contaminants, adulteration, and compliance with safety regulations. The chapter also covers the integration of big data sources, such as laboratory results, consumer feedback, and supply chain data, to provide comprehensive safety assessments. Case studies and real-world applications illustrate how data science can preemptively identify potential safety issues and improve regulatory compliance. This chapter aims to provide a detailed understanding of how leveraging data science can enhance food and supplement safety, thereby protecting public health. © 2024, IGI Global. All rights reserved.',\n",
       " \"This article examines how data science and AI change the metaverse and its effects on society and corporations. It shows how AI-powered tech creates complex virtual worlds and clever agents, improving human engagement in these online places. The paper examines data science and AI ethics, including security, privacy, and algorithmic bias, to emphasize the need for ethical norms and open practices. The essay discusses AI's merits and cons in the metaverse, including job creation, creativity, data exploitation, and displacement. This research shows how data science and artificial intelligence will impact industry, societal norms, and global problem-solving in the metaverse. It also warns against unregulated technological advancement. © 2024, IGI Global. All rights reserved.\",\n",
       " 'The field of nutrition is experiencing a remarkable shift towards personalization. The traditional \"one- size- fits- all\" approach to dietary recommendations is increasingly being challenged by the recognition that individuals have unique genetic, metabolic, and environmental factors that influence their nutritional needs. The objective of the chapter is to delve into the utilization of data science for personalized nutrition. It aims to explore the latest advancements in research concerning the integration of machine learning models to personalize every step of the nutrition care process. This review underscores the prospective role of AI in the realm of clinical nutrition and how such applications could advance care quality while streamlining healthcare provision. By shedding light on this area, the goal is to stimulate discussion, potentially allay concerns, and foster collaborative efforts to optimize the application of AI in clinical nutrition and beyond. It is crucial to acknowledge the extensive and intricate ethical and legal considerations surrounding AI in healthcare. © 2024, IGI Global. All rights reserved.',\n",
       " 'The advent of data science has revolutionized various sectors, with personalized nutrition emerging as a significant beneficiary. This chapter explores the integration of data science techniques in developing tailored dietary recommendations. By leveraging big data, including genomic, phenotypic, and lifestyle information, personalized nutrition aims to optimize health and prevent diseases. The authors delve into the methodologies for data collection, analysis, and interpretation, highlighting the role of machine learning algorithms in predicting individual nutritional needs. Case studies and real-world applications are discussed to illustrate the practical benefits and challenges of implementing personalized nutrition strategies. This chapter aims to provide a comprehensive understanding of how data science can transform nutrition into a highly individualized and effective approach to health and well-being. © 2024, IGI Global. All rights reserved.',\n",
       " 'Abstract section not found',\n",
       " 'Potential bottlenecks in high-throughput determination of enantiomeric excess in chiral primary amines is being addressed, by Howard et al. in this issue of Chem, by developing a data-driven methodology that replaces extensive calibration measurements and the corresponding calibration curves through a combination of machine learning methods and computations that produces a theoretical model and predicted calibration curves. © 2024 Elsevier Inc.',\n",
       " 'Introductory statistics textbook with a focus on data science topics such as prediction, correlation, and data exploration Statistics for Data Science and Analytics is a comprehensive guide to statistical analysis using Python, presenting important topics useful for data science such as prediction, correlation, and data exploration. The authors provide an introduction to statistical science and big data, as well as an overview of Python data structures and operations. A range of statistical techniques are presented with their implementation in Python, including hypothesis testing, probability, exploratory data analysis, categorical variables, surveys and sampling, A/B testing, and correlation. The text introduces binary classification, a foundational element of machine learning, validation of statistical models by applying them to holdout data, and probability and inference via the easy-to-understand method of resampling and the bootstrap instead of using a myriad of \"kitchen sink\" formulas. Regression is taught both as a tool for explanation and for prediction. This book is informed by the authors\\' experience designing and teaching both introductory statistics and machine learning at Statistics.com. Each chapter includes practical examples, explanations of the underlying concepts, and Python code snippets to help readers apply the techniques themselves. Statistics for Data Science and Analytics includes information on sample topics such as: • Int, float, and string data types, numerical operations, manipulating strings, converting data types, and advanced data structures like lists, dictionaries, and sets • Experiment design via randomizing, blinding, and before-after pairing, as well as proportions and percents when handling binary data • Specialized Python packages like numpy, scipy, pandas, scikit-learn and statsmodels-the workhorses of data science-and how to get the most value from them • Statistical versus practical significance, random number generators, functions for code reuse, and binomial and normal probability distributions Written by and for data science instructors, Statistics for Data Science and Analytics is an excellent learning resource for data science instructors prescribing a required intro stats course for their programs, as well as other students and professionals seeking to transition to the data science field. © 2025 by John Wiley & Sons, Inc. All rights reserved.',\n",
       " 'Abstract section not found',\n",
       " 'A Jyotirlinga or Jyotirlingam is a devotional representation of Hindu Deity. The word is made up of Sanskrit \\'jyotis\\' which means \\'radiance\\' and linga, also spelled lingam, which means \\'sign\\' or \"distinguishing symbol\". Hinduism defines Jyotirlingam as the radiant sign of the Almighty. In our data scientific view, nonetheless, Jyotirlinga represents the embodiment of time series. It exhibits stationarity and a multi-model pattern of naturally occurring time series, a classical data science pattern that has a causative relationship with historical events, world macroeconomics, agriculture, and other worldly events. Volume 2 has delved into the significance of six prominent Jyotirlingas - Rameshwaram, Nageshvara, Kashi Vishwanath, Trimbakeshwar, Kedarnath, and Grishneshwar - and explored their spiritual and scientific aspects. Through an interdisciplinary analysis that incorporates a range of fields, from statistics to environmental engineering, from historical perspectives to data science, we have gained invaluable insights into the patterns and trends that shape these sacred symbols. By embracing the fusion of modern science and traditional spiritual practices, we have the potential to unlock new levels of understanding and enlightenment, bridging the gap between ancient wisdom and contemporary knowledge. © 2024 by Nova Science Publishers, Inc. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'There is a paradigm shift towards data-centric AI, where model efficacy relies on quality, unified data. The common research analytics and data lifecycle environment (CRADLE™) is an infrastructure and framework that supports a data-centric paradigm and materials data science at scale through heterogeneous data management, elastic scaling, and accessible interfaces. We demonstrate CRADLE’s capabilities through five materials science studies: phase identification in X-ray diffraction, defect segmentation in X-ray computed tomography, polymer crystallization analysis in atomic force microscopy, feature extraction from additive manufacturing, and geospatial data fusion. CRADLE catalyzes scalable, reproducible insights to transform how data is captured, stored, and analyzed. Graphical abstract: (Figure presented.) © The Author(s) 2024.',\n",
       " '[No abstract available]',\n",
       " 'Purpose of Review: Big Data Science can be used to pragmatically guide the allocation of resources within the context of national HIV programs and inform priorities for intervention. In this review, we discuss the importance of grounding Big Data Science in the principles of equity and social justice to optimize the efficiency and effectiveness of the global HIV response. Recent Findings: Social, ethical, and legal considerations of Big Data Science have been identified in the context of HIV research. However, efforts to mitigate these challenges have been limited. Consequences include disciplinary silos within the field of HIV, a lack of meaningful engagement and ownership with and by communities, and potential misinterpretation or misappropriation of analyses that could further exacerbate health inequities. Summary: Big Data Science can support the HIV response by helping to identify gaps in previously undiscovered or understudied pathways to HIV acquisition and onward transmission, including the consequences for health outcomes and associated comorbidities. However, in the absence of a guiding framework for equity, alongside meaningful collaboration with communities through balanced partnerships, a reliance on big data could continue to reinforce inequities within and across marginalized populations. © The Author(s) 2024.',\n",
       " 'Introduction: Artificial intelligence tools such as the large language models (LLMs) Bard and ChatGPT have generated significant research interest. Utilization of these LLMs to study the epidemiology of a target population could benefit urologists. We investigated whether Bard and ChatGPT can perform a large-scale calculation of the incidence and prevalence of kidney stone disease. Materials and Methods: We obtained reference values from two published studies, which used the National Health and Nutrition Examination Survey (NHANES) database to calculate the prevalence and incidence of kidney stone disease. We then tested the capability of Bard and ChatGPT to perform similar calculations using two different methods. First, we instructed the LLMs to access the data sets and independently perform the calculation. Second, we instructed the interfaces to generate a customized computer code, which could perform the calculation on downloaded data sets. Results: While ChatGPT denied the ability to access and perform calculations on the NHANES database, Bard intermittently claimed the ability to do so. Bard provided either accurate results or inaccurate and inconsistent results. For example, Bard’s ‘‘calculations’’ for the incidence of kidney stones from 2015 to 2018 were 2.1% (95% CI 1.5–2.7), 1.75% (95% CI 1.6–1.9), and 0.8% (95% CI 0.7–0.9), while the published number was 2.1% (95% CI 1.5–2.7). Bard provided discrete mathematical details of its calculations, however, when prompted further, admitted to having obtained the numbers from online sources, including our chosen reference articles, rather than from a de novo calculation. Both LLMs were able to produce a code (Python) to use on the downloaded NHANES data sets, however, these would not readily execute. Conclusions: ChatGPT and Bard are currently incapable of performing epidemiologic calculations and lack transparency and accountability. Caution should be used, particularly with Bard, as claims of its capabilities were convincingly misleading, and results were inconsistent. © Mary Ann Liebert, Inc.',\n",
       " \"Purpose: To characterize the incidence of kidney failure associated with intravitreal anti-VEGF exposure; and compare the risk of kidney failure in patients treated with ranibizumab, aflibercept, or bevacizumab. Design: Retrospective cohort study across 12 databases in the Observational Health Data Sciences and Informatics (OHDSI) network. Subjects: Subjects aged ≥ 18 years with ≥ 3 monthly intravitreal anti-VEGF medications for a blinding disease (diabetic retinopathy, diabetic macular edema, exudative age-related macular degeneration, or retinal vein occlusion). Methods: The standardized incidence proportions and rates of kidney failure while on treatment with anti-VEGF were calculated. For each comparison (e.g., aflibercept versus ranibizumab), patients from each group were matched 1:1 using propensity scores. Cox proportional hazards models were used to estimate the risk of kidney failure while on treatment. A random effects meta-analysis was performed to combine each database's hazard ratio (HR) estimate into a single network-wide estimate. Main Outcome Measures: Incidence of kidney failure while on anti-VEGF treatment, and time from cohort entry to kidney failure. Results: Of the 6.1 million patients with blinding diseases, 37 189 who received ranibizumab, 39 447 aflibercept, and 163 611 bevacizumab were included; the total treatment exposure time was 161 724 person-years. The average standardized incidence proportion of kidney failure was 678 per 100 000 persons (range, 0–2389), and incidence rate 742 per 100 000 person-years (range, 0–2661). The meta-analysis HR of kidney failure comparing aflibercept with ranibizumab was 1.01 (95% confidence interval [CI], 0.70–1.47; P = 0.45), ranibizumab with bevacizumab 0.95 (95% CI, 0.68–1.32; P = 0.62), and aflibercept with bevacizumab 0.95 (95% CI, 0.65–1.39; P = 0.60). Conclusions: There was no substantially different relative risk of kidney failure between those who received ranibizumab, bevacizumab, or aflibercept. Practicing ophthalmologists and nephrologists should be aware of the risk of kidney failure among patients receiving intravitreal anti-VEGF medications and that there is little empirical evidence to preferentially choose among the specific intravitreal anti-VEGF agents. Financial Disclosures: Proprietary or commercial disclosure may be found in the Footnotes and Disclosures at the end of this article. © 2024 American Academy of Ophthalmology\",\n",
       " 'Cardiovascular diseases are causing more deaths across the globe. With innovations in Artificial Intelligence (AI) predicting such diseases early is very important research area. With learning based approaches that exploit knowledge from given samples, it is possible to improve disease prediction process. There are many aspects to proper healthcare such as preventing diseases with suitable diet and lifestyle, early detection of diseases if any and efficient treatment. Data is being accumulated in every domain. However, the healthcare industry is on top of the list as it provides large volumes of data pertaining to human health, diet and drug aspects. The existing literature has not shown adequate research in this direction. The Healthcare industry has an unprecedented impact on the well-being of people across the globe. In the recent observations by World Health Organization (WHO), data science approach towards disease prediction greatly complements existing Clinical Decision Support Systems (CDSSs).This research paper presents a comprehensive study on the application of data science techniques for disease prediction and drug recommendation in healthcare, focusing on a case study involving cardiovascular diseases. The primary objective of this study is to develop a robust predictive model that identifies the likelihood of cardiovascular diseases in patients, and subsequently recommends drug interventions for optimal treatment outcomes. Here we propose Disease Prediction and Drug Recommendation Framework (DPDRF). The framework is realized by defining an algorithm known as Cardio Disease Prediction and Drug Recommendation (CDP-DR). The Disease Prediction and Drug Recommendation algorithm in turn uses different supervised machine learning (ML) algorithms such as Random Forest (RF), Logistic Regression (LR), Support Vector Machine (SVM), Decision Tree (DT), Stochastic Gradient Descent (SGD), Gradient Boosting, and Extreme Gradient Boosting (XGB). Another algorithm known as Entropy and Gain based Hybrid Feature Selection (EG-HFS) is defined to leverage quality of training leading to performance enhancement of prediction models. The experimental results with cardio disease prediction as a case study revealed that the proposed framework is useful in disease prediction and drug recommendations by using different prediction models. Highest accuracy achieved by the proposed system is 96.23%. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.',\n",
       " 'The significance of ethics in data science research has attracted considerable attention in recent years. While there is widespread agreement on the importance of teaching ethics within computing contexts, there is no clear method for its implementation and assessment. Studies focusing on methods for integrating ethics into data science courses reveal that students tend to neglect ethical concerns in their data analysis. Based on the data we collected from questionnaires distributed to undergraduate science and engineering students, this paper expands the discussion beyond human concerns and ethics in data science education. As we will show, students tend to neglect the context when attempting to solve data science questions. We argue that gaps in understanding the context relating to the data result in gaps in the analysis as well as in the interpretation of the data. Thus, we propose anthropological thinking as a pedagogy to overcome the context neglect. Placing the spotlight on the context promotes a holistic understanding of the phenomenon being analyzed, as it includes important considerations that do not necessarily fit the more commonly used term human concerns. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.',\n",
       " 'Handling outliers is an important step in data analysis, and it can be approached through three different ways, namely; accommodation, omission, or winsorization. This article investigates the impact of four winsorization statistics (mean, median, mode, and quantiles) on parameter estimation through an extensive simulation study. Three probability distributions (normal, negative binomial, and exponential) are considered, each with varying degrees of contamination. The simulation results suggest that winsoriza-tion is effective for small contamination levels and large sample sizes. Furthermore, it is recommended to winsorize outliers in symmetric distributions using any of the location parameters. However, for asymmetric distributions, the median should be employed. To illustrate these findings, a real dataset on internet usage session durations for 4,500 users, comprising over 2 million records, are fitted to the exponential distribution. The identified outliers were winsorized using the aforementioned statistics. © 2024, ASA Associazione per la Statistica Applicata. All rights reserved.',\n",
       " 'Abstract section not found',\n",
       " 'Data science, a prominent subset of artificial intelligence (AI), specializes in analyzing vast datasets using diverse tools and methods to derive insights and predictions. Its applications span various domains, notably cybersecurity, crucial for safeguarding systems and data from internal and external threats amid escalating challenges. The integration of data science in sectors like energy and environment underscores its transformative potential. The digitization of energy systems enables real-time data acquisition, facilitating timely analysis of human impacts on the environment and society. Furthermore, data science aids in monitoring policy effectiveness towards sustainable development (SD) goals. However, the rapid evolution of technology presents formidable challenges, necessitating organizational adaptability and utilization of cutting-edge advancements, especially in online communication. Amidst these shifts, data science emerges as a pivotal tool for informed decision-making and innovation, poised to make significant contributions across diverse sectors of the economy. © 2024, IGI Global. All rights reserved.',\n",
       " 'Air pollution causes premature death and disease and disproportionately harms non-white and lower-income groups in the United States. Government policies are responsible for the racial disparity in air pollution exposure and related health outcomes. Investigating complex relationships between policies, air pollution, and health requires (i) harmonized data connecting policies, environmental exposures, socioeconomic characteristics, and health at the individual and area level; (ii) interpretable estimands accounting for the complex interplay between policies and disparities in exposures and health outcomes; and (iii) data science approaches that can elucidate direct and indirect policy effects on disparities to identify effective interventions. We review statistical considerations and new data science approaches needed to scrutinize the policy impacts on disparities in air pollution exposure and health outcomes.',\n",
       " \"The disruptive effect and lasting implications of generative artificial intelligence ('Gen AI') tools for higher education are yet unknown. ChatGPT 3.5 was released in November 2022 and soon had over 100 million users. This public software release triggered a global Gen AI race among technology industry giants. Educators in higher education were unprepared for this seismic shift. For the Fall 2023 term, we revised two graduate courses in nursing informatics and data science using generative AI to anticipate broad student awareness of these tools. The aim of this report is to present two different examples of course component evaluation using publicly available Gen AI tools to inform course component revisions. This effort was undertaken to 'proof' assignments against inappropriate use of Gen AI, revise assignments to incorporate use of Gen AI tools, and inform course policies about appropriate use of Gen AI for course work. © 2024 The Authors.\",\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'In a rapidly changing context, due to the growth of administrative complexity, the computational hegemony of private powers and the growing European and national legislation on data and artificial intelligence systems, it is necessary for administrations to start using algorithmic technologies for the exercise of administrative functions in a conscious manner. Starting from these premises, the author reflects on what knowledge must be present in public administrations in order to govern the use of AI systems for the care of collective interests, highlighting limits and criticalities of the current situation. © 2024, Consiglio Nazionale delle Ricerche. All rights reserved.',\n",
       " 'Insufficient physical activity (PA) is highly prevalent in society, despite its negative impact on personal health. Effective interventions are clearly needed. However, improvements to behavioral interventions face many challenges, from noisy and missing measurements to inadequate understanding of the dynamics of behavior change in context. In this paper, we present a comprehensive, data-driven, system identification approach aimed at overcoming challenges to better understand the dynamics of PA behavior change, which in turn can improve the efficacy of these interventions. The proposed approach consists of an innovative input signal design (aimed at providing informative data sets to study the concept of “just-in-time” dynamics), Singular Spectrum Analysis (SSA; for noise reduction and exploring the separability of the measured output signal), and Model-on-Demand (MoD) estimation, a hybrid data-driven modeling approach, which allows identifying dynamics under changing operating conditions. The proposed approach is evaluated on data for a representative participant from the JustWalk JITAI study. The results demonstrate significant potential of the methodology in enhancing the understanding of the dynamics of behavior change in context. © 2024 The Authors.',\n",
       " 'As AI technology continues to advance, the importance of human-AI collaboration becomes increasingly evident, with numerous studies exploring its potential in various felds. One vital feld is data science, including feature engineering (FE), where both human ingenuity and AI capabilities play pivotal roles. Despite the existence of AI-generated recommendations for FE, there remains a limited understanding of how to efectively integrate and utilize humans’ and AI’s knowledge. To address this gap, we design a readily-usable prototype, human&AI-assisted FE in Jupyter notebooks. It harnesses the strengths of humans and AI to provide feature suggestions to users, seamlessly integrating these recommendations into practical workfows. Using the prototype as a research probe, we conducted an exploratory study to gain valuable insights into data science practitioners’ perceptions, usage patterns, and their potential needs when presented with feature suggestions from both humans and AI. Through qualitative analysis, we discovered that the “Creator” of the feature (i.e., AI or human) signifcantly infuences users’ feature selection, and the semantic clarity of the suggested feature greatly impacts its adoption rate. Furthermore, our fndings indicate that users perceive both diferences and complementarity between features generated by humans and those generated by AI. Lastly, based on our study results, we derived a set of design recommendations for future human&AI FE design. Our fndings show the collaborative potential between humans and AI in the feld of FE. © 2024 Copyright held by the owner/author(s).',\n",
       " 'The rising adoption of electric vehicles (EVs), driven by carbon neutrality goals, has prompted the need for accurate forecasting of EVs’ charging behavior. However, this task presents several challenges due to the dynamic nature of EVs’ usage patterns, including fluctuating demand and unpredictable charging durations. In response to these challenges and different from previous works, this paper presents a novel and holistic methodology for day-ahead forecasting of EVs’ plugged-in status and power consumption in charging stations (CSs). The proposed framework encompasses data analysis, pre-processing, feature engineering, feature selection, the use and comparison of diverse machine learning forecasting algorithms, and validation. A real-world dataset from a CS in Boulder City is employed to evaluate the framework’s effectiveness, and the results demonstrate its proficiency in predicting the EVs’ plugged-in status, with XGBoost’s classifier achieving remarkable accuracy with an F1-score of 0.97. Furthermore, an in-depth evaluation of six regression methods highlighted the supremacy of gradient boosting algorithms in forecasting the EVs’ power consumption, with LightGBM emerging as the most effective method due to its optimal balance between prediction accuracy with a 4.22% normalized root-mean-squared error (NRMSE) and computational efficiency with 5 s of execution time. The proposed framework equips power system operators with strategic tools to anticipate and adapt to the evolving EV landscape. © 2024 by the authors.',\n",
       " 'The evolving landscape of data science education poses challenges for instructors in general education classes. With the expansion of higher education dedicated to cultivating data scientists, integrating data science education into university curricula has become imperative. However, addressing diverse student backgrounds underscores the need for a systematic review of course content and design. This study systematically reviews 60 data science courses syllabi in general education across all universities in Taiwan. Utilizing content analysis, bibliometric, and text-mining methodologies, this study quantifies key metrics found within syllabi, including instructional materials, assessment techniques, learning objectives, and covered topics. The study highlights infrequent textbook sharing, with particular focus on Python programming. Assessment methods primarily involve participation, assignments, and projects. Analysis of Bloom’s Taxonomy suggests a focus on moderate complexity learning objectives. The topics covered prioritize big data competency, analytical techniques, programming competency, and teaching strategies in descending order. This study makes a valuable contribution to the current knowledge by tackling the challenge of delineating the specific content of data science. It also provides valuable references for potentially streamlining the integration of multiple disciplines within introductory courses while ensuring flexibility for students with varying programming and statistical proficiencies in the realm of data science education. © 2024 by the author.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " \"In the expansive domain of data science, clustering algorithms play a pivotal role in segmenting datasets into meaningful groups without prior knowledge of their underlying patterns. This research provides an in-depth evaluation of the time and space complexities of three widely-used clustering algorithms: K-Means, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), and Hierarchical Clustering. The study delves into each algorithm's inherent strengths and limitations, factoring in real-world data application scenarios. Our results indicate varying performance metrics, with K-Means showcasing scalability for larger datasets, DBSCAN aptly handling datasets with arbitrary shapes and noise, and Hierarchical Clustering offering insights into intricate hierarchical structures. By offering a comprehensive comparison, this article aims to guide data scientists in selecting the most appropriate clustering technique based on specific problem requirements and dataset characteristics. © 2024 Author(s).\",\n",
       " \"Background: The world is becoming increasingly urbanised. As cities around the world continue to grow, it is important for urban planners and policy makers to understand how different urban configuration patterns affect the environment and human health. However, previous studies have provided mixed findings. We aimed to identify European urban configuration types, on the basis of the local climate zones categories and street design variables from Open Street Map, and evaluate their association with motorised traffic flows, surface urban heat island (SUHI) intensities, tropospheric NO2, CO2per person emissions, and age-standardised mortality. Methods: We considered 946 European cities from 31 countries for the analysis defined in the 2018 Urban Audit database, of which 919 European cities were analysed. Data were collected at a 250 m × 250 m grid cell resolution. We divided all cities into five concentric rings based on the Burgess concentric urban planning model and calculated the mean values of all variables for each ring. First, to identify distinct urban configuration types, we applied the Uniform Manifold Approximation and Projection for Dimension Reduction method, followed by the k-means clustering algorithm. Next, statistical differences in exposures (including SUHI) and mortality between the resulting urban configuration types were evaluated using a Kruskal–Wallis test followed by a post-hoc Dunn's test. Findings: We identified four distinct urban configuration types characterising European cities: compact high density (n=246), open low-rise medium density (n=245), open low-rise low density (n=261), and green low density (n=167). Compact high density cities were a small size, had high population densities, and a low availability of natural areas. In contrast, green low density cities were a large size, had low population densities, and a high availability of natural areas and cycleways. The open low-rise medium and low density cities were a small to medium size with medium to low population densities and low to moderate availability of green areas. Motorised traffic flows and NO2exposure were significantly higher in compact high density and open low-rise medium density cities when compared with green low density and open low-rise low density cities. Additionally, green low density cities had a significantly lower SUHI effect compared with all other urban configuration types. Per person CO2emissions were significantly lower in compact high density cities compared with green low density cities. Lastly, green low density cities had significantly lower mortality rates when compared with all other urban configuration types. Interpretation: Our findings indicate that, although the compact city model is more sustainable, European compact cities still face challenges related to poor environmental quality and health. Our results have notable implications for urban and transport planning policies in Europe and contribute to the ongoing discussion on which city models can bring the greatest benefits for the environment, climate, and health. Funding: Spanish Ministry of Science and Innovation, State Research Agency, Generalitat de Catalunya, Centro de Investigación Biomédica en red Epidemiología y Salud Pública, and Urban Burden of Disease Estimation for Policy Making as a Horizon Europe project. © 2024 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license\",\n",
       " 'Mobility data captures the locations of moving objects such as humans, animals, and cars. With the availability of Global Positioning System (GPS)-equipped mobile devices and other inexpensive location-Tracking technologies, mobility data is collected ubiquitously. In recent years, the use of mobility data has demonstrated a significant impact in various domains, including traffic management, urban planning, and health sciences. In this article, we present the domain of mobility data science. Towards a unified approach to mobility data science, we present a pipeline having the following components: mobility data collection, cleaning, analysis, management, and privacy. For each of these components, we explain how mobility data science differs from general data science, we survey the current state-of-The-Art, and describe open challenges for the research community in the coming years. © 2024 Copyright held by the owner/author(s).',\n",
       " '[No abstract available]',\n",
       " 'This article presents a novel approach to introducing principal component analysis (PCA), using summary tables and descriptive statistics. Given its applicability across a variety of academic disciplines, this topic offers abundant opportunity for class discussion and activities. However, teaching PCA in an introductory class can be challenging due to the potential abstraction of multivariate datasets, and especially when students have a minimal background in statistics or data science. This method aims to help teachers bridge the gap between basic descriptive statistics and the more advanced concepts of PCA; this is done by disregarding mathematical optimization, while emphasizing the use of summary tables and the programming language R. The focus is on implementing this method in an introductory tertiary data science course; however, it may potentially be used in higher level courses, and across a variety of disciplines. © 2024 Teaching Statistics Trust.',\n",
       " '[No abstract available]',\n",
       " 'The concepts of a consequence relation and operation, though very abstract and theoretical, may be related to specific categories of information systems (i.e. mathematical frontends of data tables); as it has been demonstrated by D. Vakarelov, there exist correspondence between Pawlak information systems and Scott as well as Tarski consequence operations. This line of research goes (via representation) from abstract concepts to data. In this paper we would like to take the opposite direction: from data (via construction) to consequence relations. The main emphasis is laid here not on general categories of consequence relations (e.g. Scott or Tarski ones) but on concrete operators that can be retrieved from information systems (e.g. different examples of Scott consequence). To this end, we employ Galois connections and adjunctions (en masse called Galois mappings) and study the consequence relations that can be built via these maps. The main novelty of our research comes from the investigation of consequence relations induced by adjunctions rather than monotone Galois connections, which have been the main subject of studies so far. Surprisingly, the operations obtained from adjunctions possess a number of counter-intuitive properties, which (in turn) request some intelligible interpretations. And this is our next objective: to make sense of these consequence relations in the context of information processing. © 2024',\n",
       " 'Purpose: Integrating data science techniques in healthcare has emerged as a transformative force and holds immense potential for improving patient outcomes, enhancing operational efficiency, and advancing medical research by utilizing medical information. This paper aims to explore various data science techniques employed in the field of healthcare, highlight corresponding challenges, and identify opportunities for innovation. Methods: A comprehensive literature review was conducted to gather insight into the current use to data science applications in healthcare domain. We explored various databases like Google Scholar, PubMed, IEEE Xplore using relevant keywords such as “data science”, “data science in healthcare”, “data science techniques”, “data science applications in healthcare”, “Machine Learning in healthcare”, and “Predictive Analytics”. We focused on papers published between 2015 and 2023. Results: Data Science techniques including Artificial Intelligence, Machine Learning, Deep Learning, Natural Language Processing, etc. are being used for tasks such as disease prediction, personalized treatment recommendation, medical imaging analysis, and healthcare resource optimization. Nonetheless, challenges such as data privacy, quality, data access and ethics in using healthcare data were identified as barriers in their extensive implementation. Conclusions: Important insights that could improve patient outcomes and lower healthcare expenditures can be obtained by employing data science techniques on data generated through wearable technology, medical imaging, Electronic Health Records (EHR), etc. To fully utilize data science in healthcare, however, data quality, privacy, and security issues must be resolved, and deliberative frameworks must be created to enable the use of data science in healthcare in an ethical and responsible manner. © The Author(s) under exclusive licence to International Union for Physical and Engineering Sciences in Medicine (IUPESM) 2024.',\n",
       " 'When designing data science (DS) pipelines, end-users can get overwhelmed by the large and growing set of available data preprocessing and modeling techniques. Intelligent discovery assistants (IDAs) and automated machine learning (AutoML) solutions aim to facilitate end-users by (semi-)automating the process. However, they are expensive to compute and yield limited applicability for a wide range of real-world use cases and application domains. This is due to (a) their need to execute thousands of pipelines to get the optimal one, (b) their limited support of DS tasks, e.g., supervised classification or regression only, and a small, static set of available data preprocessing and ML algorithms; and (c) their restriction to quantifiable evaluation processes and metrics, e.g., tenfold cross-validation using the ROC AUC score for classification. To overcome these limitations, we propose a human-in-the-loop approach for the assisteddesignofdatasciencepipelines using previously executed pipelines. Based on a user query, i.e.,\\xa0data and a DS task, our framework outputs a ranked list of pipeline candidates from which the user can choose to execute or modify in real time. To recommend pipelines, it first identifies relevant datasets and pipelines utilizing efficient similarity search. It then ranks the candidate pipelines using multi-objective sorting and takes user interactions into account to improve suggestions over time. In our experimental evaluation, the proposed framework significantly outperforms the state-of-the-art IDA tool and achieves similar predictive performance with state-of-the-art long-running AutoML solutions while being real-time, generic to any evaluation processes and DS tasks, and extensible to new operators. © The Author(s) 2024.',\n",
       " 'The article deals with the processes of multi-factor forecasting of statistical trends for Data Science problems. Most of the classic approaches to data processing consist of studying the consequences of phenomena rather than the factors of their appearance. At the same time, the factors affecting the behavior of the investigated process are assumed to be random and are not investigated. The article discusses the approach to forecasting the parameters of the trend of statistical time se-ries, which consists of the study of factors that lead to changes in the dynamics of the studied process. This approach potentially has better indicators of adequacy, ac-curacy, and efficiency in obtaining final solutions than classical approaches. The implementation of this approach is shown using an example of the analysis of exchange rate changes. The obtained results show the practicality of considering multi-factoriality in forecasting tasks. © O. Pysarchuk, T. Andreieva, O. Grinenko, D. Baran, 2024.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'Introduction African cities, particularly Abidjan and Johannesburg, face challenges of rapid urban growth, informality and strained health services, compounded by increasing temperatures due to climate change. This study aims to understand the complexities of heat-related health impacts in these cities. The objectives are: (1) mapping intraurban heat risk and exposure using health, socioeconomic, climate and satellite imagery data; (2) creating a stratified heat–health forecast model to predict adverse health outcomes; and (3) establishing an early warning system for timely heatwave alerts. The ultimate goal is to foster climate-resilient African cities, protecting disproportionately affected populations from heat hazards. Methods and analysis The research will acquire health-related datasets from eligible adult clinical trials or cohort studies conducted in Johannesburg and Abidjan between 2000 and 2022. Additional data will be collected, including socioeconomic, climate datasets and satellite imagery. These resources will aid in mapping heat hazards and quantifying heat–health exposure, the extent of elevated risk and morbidity. Outcomes will be determined using advanced data analysis methods, including statistical evaluation, machine learning and deep learning techniques. Ethics and dissemination The study has been approved by the Wits Human Research Ethics Committee (reference no: 220606). Data management will follow approved procedures. The results will be disseminated through workshops, community forums, conferences and publications. Data deposition and curation plans will be established in line with ethical and safety considerations. © Author(s) (or their employer(s)) 2024.',\n",
       " \"This paper presents a pilot study of an interest-driven data science curriculum for high school students. The curriculum uses authentic and meaningful data exploration activities to situate data science in students' lived experiences. The curriculum aims to lay the computational foundation of data science and equip students with the necessary skills and practices to become informed and active citizens in our data-driven world. The pilot study, conducted in two sections of a computer science class, demonstrates the curriculum's inquiry-based approach, which allows students to formulate questions based on their interests and answer them by manipulating publicly available datasets. The study illustrates how a block-based learning environment and API data retrieval can be harnessed to support data science learning activities that situate the topics in learners' lived experiences and create an engaging learning experience. The study advances our understanding of ways to use novel technologies to introduce learners to data science, emphasizing the practical implications of using authentic data and the inquiry-based approach in curriculum design. © 2024 Owner/Author.\",\n",
       " 'An ER diagram is a fundamental visual abstraction to design a database. Modern ER notation has evolved with UML symbols to represent both entities (logical level) and relational tables (physical level). On the other hand, flow diagrams (flowcharts, process flow) remain an important mechanism to visualize the main steps of a data processing pipeline. However, in modern data science projects there is a significant fraction of data that does not come from databases or data that is exported outside the database system, being processed by Python code, without any data model whatsoever. In this paper, we present a novel diagram which is built from source code and its associated browser-based GUI for collaborating on data integration and data preprocessing, mixing diverse data sources and diverse programming languages (mainly Python and SQL). Specifically, our targets are data integration, data cleaning and data transformation, which are needed to derive data sets that can be used as input for a machine learning model. We present a couple of target applications and a preliminary GUI, which partially automates diagram creation. We show our diagram has promise understanding, extending and reusing both data preparation source code and data sets. © 2024 Copyright is held by the owner/author(s).',\n",
       " 'Data preprocessing and engineering are essential parts of any AI system, as indicated by the current trend of data-centric AI. However, until now, explainability efforts have almost exclusively focused on models. We propose explanations for preprocessing pipelines that express the impact of each step on the resulting model behavior based on existing feature attribution methods. In the process, we introduce two related but distinct measures of impact for preprocessing steps: Leave-out Impact (What do we lose/gain by leaving out this step?) and Immediate Impact (What do we lose/gain by adding this step at this time?). Both are obtained by constructing variations of the original pipeline and comparing the resulting model behavior represented as feature importance vectors. These measures reflect the intuition of impact but also express the effects of a step and its interactions with the rest of the pipeline on the internal workings of the trained model. © 2024 Copyright is held by the owner/author(s).',\n",
       " '[No abstract available]',\n",
       " 'Future Communication Systems Using Artificial Intelligence, Internet of Things and Data Science mainly focuses on the techniques of artificial intelligence (AI), Internet of Things (IoT) and data science for future communications systems. The goal of AI, IoT and data science for future communications systems is to create a venue for industry and academics to collaborate on the development of network and system solutions based on data science, AI and IoT. Recent breakthroughs in IoT, mobile and fixed communications and computation have paved the way for a data-centric society of the future. New applications are increasingly reliant on machine-to-machine connections, resulting in unusual workloads and the need for more efficient and dependable infrastructures. Such a wide range of traffic workloads and applications will necessitate dynamic and highly adaptive network environments capable of self-optimization for the task at hand while ensuring high dependability and ultra-low latency. Networking devices, sensors, agents, meters and smart vehicles/systems generate massive amounts of data, necessitating new levels of security, performance and dependability. Such complications necessitate the development of new tools and approaches for providing successful services, management and operation. Predictive network analytics will play a critical role in insight generation, process automation required for adapting and scaling to new demands, resolving issues before they impact operational performance (e.g., preventing network failures and anticipating capacity requirements) and overall network decision-making. To increase user experience and service quality, data mining and analytic techniques for inferring quality of experience (QoE) signals are required. AI, IoT, machine learning, reinforcement learning and network data analytics innovations open new possibilities in areas such as channel modeling and estimation, cognitive communications, interference alignment, mobility management, resource allocation, network control and management, network tomography, multi-agent systems and network ultra-broadband deployment prioritization. These new analytic platforms will aid in the transformation of our networks and user experience. Future networks will enable unparalleled automation and optimization by intelligently gathering, analyzing, learning and controlling huge volumes of information. © 2024 selection and editorial matter, Dr Inam Ullah, Dr Inam Ullah Khan, Dr Mariya Ouaissa, Dr Mariyam Ouaissa, Dr Salma El Hajjami; individual chapters, the contributors. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " 'The Solid-State Materials Chemistry Data Science Hackathon (SSMCDAT), held at Lehigh University from January 19-21, 2023, demonstrated the power of interdisciplinary collaboration in tackling challenges in solid-state materials chemistry. This article highlights key outcomes, participant feedback, and future research and collaboration pathways. Published 2024 by American Chemical Society.',\n",
       " 'Herein, we advocate for the integration of the pipelines for data science (e.g., extraction, cleaning, and exploration) and machine learning (e.g., training data collection, feature selection, model selection, and parameter tuning), toward responsible and trustworthy artificial intelligence. We argue that the metadata generated by the machine-learning pipeline, which includes model outputs and model accuracy scores, is best managed and analyzed using data-science tools, thereby obtaining actionable insights into model performance, interpretability, and bias. We illustrate via two examples from our recent work as proof of concept: data summarization for model performance diagnostics; and input and output exploration to understand retrieval-augmented language models. © 2024 ACM.',\n",
       " \"As the Lakehouse architecture becomes more widespread, ensuring the reproducibility of data workloads over data lakes emerges as a crucial concern for data practitioners. However, achieving reproducibility remains challenging. The size of data pipelines contributes to slow testing and iterations, while the intertwining of business logic and data management complicates debugging and increases error susceptibility. In this paper, we highlight recent advancements made at Bauplan in addressing this challenge. We introduce a system designed to decouple compute from data management, by leveraging a cloud runtime alongside Nessie, an open-source catalog with Git semantics. Demonstrating the system's capabilities, we showcase its ability to offer time-travel and branching semantics on top of object storage, and offer full pipeline reproducibility with a few CLI commands. © 2024 ACM.\",\n",
       " 'For any corporate and financial organization, data science is the most reliable tool for assisting their leaders in making choices based on current events and facts. It was created using a variety of technologies and academic fields. The most significant field that contributes to the deep and basic workings of data science is statistics. In order to conduct essential mechanisms, functions, processes, and procedures to analyze and quantify uncertainty, statistics is assisting the most common tools and approaches in data science. The importance of statistics in delivering essential capabilities such as data gathering and refinement, data processing, data analysis, modeling, model selection, and model validation, as well as visualization for decision-making, is presented in this work. This research article elucidated the essential features of data science using various statistical tools and techniques. © 2024 Author(s).',\n",
       " \"Purpose: This study is part of a participatory design research project and aims to develop and study pedagogical frameworks and tools for integrating computational thinking (CT) concepts and data science practices into elementary school classrooms. Design/methodology/approach: This paper describes a pedagogical approach that uses a data science framework the research team developed to assist teachers in providing data science instruction to elementary-aged students. Using phenomenological case study methodology, the authors use classroom observations, student focus groups, video recordings and artifacts to detail ways learners engage in data science practices and understand how they perceive their engagement during activities and learning. Findings: Findings suggest student engagement in data science is enhanced when data problems are contextualized and connected to students’ lived experiences; data analysis and data-based decision-making is practiced in multiple ways; and students are given choices to communicate patterns, interpret graphs and tell data stories. The authors note challenges students experienced with data practices including conflict between inconsistencies in data patterns and lived experiences and focusing on data visualization appearances versus relationships between variables. Originality/value: Data science instruction in elementary schools is an understudied, emerging and important area of data science education. Most elementary schools offer limited data science instruction; few elementary schools offer data science curriculum with embedded CT practices integrated across disciplines. This research assists elementary educators in fostering children's data science engagement and agency while developing their ability to reason, visualize and make decisions with data. © 2023, Emerald Publishing Limited.\",\n",
       " 'With a rising number of law enforcement agencies facing budgetary cuts, many turn to data science in an attempt to maintain service quality with fewer resources. A number of thus adopted solutions-including facial recognition, predictive policing, and risk assessments-have been contested by researchers and journalists alike. Yet comparatively little research is done at the strategy level, which determines where data science will be deployed in the first place. In this study, we interview 40 practitioners from Police Scotland, investigating what they believe to be crucial to successfully incorporate data science in their ways of working. Bucking the external trend, the participants distanced themselves from tools like facial recognition and risk assessment. Instead of focusing on individual use-cases, their primary concerns for the future were around (i) systemic issues around data is collection and use, (ii) goal misalignment between leadership and operational levels, (iii) the fear that datafication may undervalue important aspects of policing, and (iv) appropriate ways of interaction between data science teams and operational officers. Alongside the insights particular to Police Scotland, our work reaffirms how participatory approaches can go beyond the technical, and uncover structural and political barriers to success. © 2024 Owner/Author.',\n",
       " '[No abstract available]',\n",
       " 'Improving Equity in Data Science offers a comprehensive look at the ways in which data science can be conceptualized and engaged more equitably within the K-16 classroom setting, moving beyond merely broadening participation in educational opportunities. This book makes the case for field wide definitions, literacies and practices for data science teaching and learning that can be commonly discussed and used, and provides examples from research of these practices and literacies in action. Authors share stories and examples of research wherein data science advances equity and empowerment through the critical examination of social, educational, and political topics. In the first half of the book, readers will learn how data science can deliberately be embedded within K-12 spaces to empower students to use it to identify and address inequity. The latter half will focus on equity of access to data science learning opportunities in higher education, with a final synthesis of lessons learned and presentation of a 360-degree framework that links access, curriculum, and pedagogy as multiple facets collectively essential to comprehensive data science equity work. Practitioners and teacher educators will be able to answer the question, \"how can data science serve to move equity efforts in computing beyond basic inclusion to empowerment?\" whether the goal is to simply improve definitions and approaches to research on data science or support teachers of data science in creating more equitable and inclusive environments within their classrooms. © 2024 selection and editorial matter, Colby Tofel-Grehl and Emmanuel Schanzer; individual chapters, the contributors. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'Large language models (LLMs) have emerged as a powerful tool for biomedical researchers, demonstrating remarkable capabilities in understanding and generating human-like text. ChatGPT with its Code Interpreter functionality, an LLM connected with the ability to write and execute code, streamlines data analysis workflows by enabling natural language interactions. Using materials from a previously published tutorial, similar analyses can be performed through conversational interactions with the chatbot, covering data loading and exploration, model development and comparison, permutation feature importance, partial dependence plots, and additional analyses and recommendations. The findings highlight the significant potential of LLMs in assisting researchers with data analysis tasks, allowing them to focus on higher-level aspects of their work. However, there are limitations and potential concerns associated with the use of LLMs, such as the importance of critical thinking, privacy, security, and equitable access to these tools. As LLMs continue to improve and integrate with available tools, data science may experience a transformation similar to the shift from manual to automatic transmission in driving. The advancements in LLMs call for considering the future directions of data science and its education, ensuring that the benefits of these powerful tools are utilized with proper human supervision and responsibility. © 2024 Translational and Clinical Pharmacology.',\n",
       " 'Understanding the health effects of disasters is critical for effective preparedness, response, recovery, and mitigation. However, research is negatively impacted by both the limited availability of disaster data and the difficulty of identifying and utilizing disaster-specific and health data sources relevant to disaster research and management. In response to numerous requests from disaster researchers, emergency managers, and operational response organizations, 73 distinct data sources at the intersection of disasters and health were compiled and categorized. These data sources generally cover the entire United States, address both disasters and health, and are available to researchers at little or no cost. Data sources are described and characterized to support improved research and guide evidence-based decision making. Current gaps and potential solutions are presented to improve disaster data collection, utilization, and dissemination. © 2024 The Author(s).',\n",
       " 'In today’s investment landscape, the integration of environmental, social, and governance (ESG) factors with data-driven strategies is pivotal. This study delves into this fusion, employing sophisticated statistical techniques and Python programming to unveil insights often overlooked by traditional approaches. By analyzing extensive datasets, including S&P500 financial indicators from 2012 to 2021 and 2021 ESG metrics, investors can enhance portfolio performance. Emphasizing ESG integration for sustainable investing, the study underscores the potential for alpha generation. Time series analysis further elucidates market dynamics, empowering investors to align with both financial objectives and ethical values. Notably, the research uncovers a positive correlation between ESG risk and total risk, suggesting that companies with lower ESG risk tend to outperform those with higher ESG risk. Moreover, employing a long–short ESG risk strategy yields abnormal returns of approximately 4.37%. This integration of ESG factors not only mitigates risks associated with environmental, social, and governance issues but also capitalizes on opportunities for sustainable growth, fostering responsible investing practices and ensuring long-term financial returns, resilience, and value creation. © 2024 by the authors.',\n",
       " 'Modern production relies on data-based analytics for the prediction and optimization of production processes. Specialized data scientists perform tasks at companies and research institutions, dealing with real data from actual production environments. The roles of data preprocessing and data quality are crucial in data science, and an active research field deals with methodologies and technologies for this. While anecdotes and generalized surveys indicate preprocessing is the major operational task for data scientists, a detailed view of the subtasks and the domain of production data is missing. In this paper, we present a multi-stage survey on data science tasks in practice in the field of production. Using expert knowledge and insights, we found data preprocessing to be the major part of the tasks of data scientists. In detail, we found that tackling missing values, finding data point meanings, and synchronization of multiple time-series were often the most time-consuming preprocessing tasks. © 2024 by the authors.',\n",
       " 'Fishing tourism, a niche yet burgeoning sector within the broader tourism industry, presents a unique opportunity to explore the dynamics of the Experience Economy. This study delves into the intricacies of fishing tourism through the lens of data science, leveraging web crawling techniques to gather extensive data from reviewer profiles on TripAdvisor. By employing natural language processing (NLP) techniques, the study investigates the correlation between Experience Economy dimensions and user profiling aspects within this domain. The findings of this study shed light on the predominant dimensions of the Experience Economy within fishing tourism. “Entertainment” emerges as the primary dimension, closely followed by “Aesthetic”, “Educational”, and “Escapist” elements. Notably, the study reveals frequent co-occurrences of certain dimension pairs, such as “Entertainment”–“Aesthetic” and “Educational”–“Entertainment”, underscoring the multifaceted nature of the fishing tourism experience. The implications of this research extend beyond academic discourse to practical considerations for stakeholders in the fishing tourism sector. By highlighting the socioeconomic benefits inherent in fishing tourism for local communities and fishers, the study emphasizes the importance of governmental support. This support, in the form of infrastructure development, effective leadership, legislative measures, and financial backing, is deemed essential for fostering sustainable growth and development in this sector. This study stands as a pioneering endeavor within the realm of fishing tourism research, particularly in its focus on Experience Economy dimensions and user profiling. By drawing data from both business pages and user profiles on TripAdvisor, it provides a comprehensive understanding of the intricate interplay between tourist experiences and the broader economic and social landscape of fishing tourism destinations. © 2024 by the authors.',\n",
       " 'Literate programming—the bringing together of program code and natural language narratives—has become a ubiquitous approach in the realm of data science. This methodology is appealing as well for the domain of Density Functional Theory (DFT) calculations, particularly for interactively developing new methodologies and workflows. However, effective use of literate programming is hampered by old programming paradigms and the difficulties associated with using high performance computing (HPC) resources. Here we present two Python libraries that aim to remove these hurdles. First, we describe the PyBigDFT library, which can be used to setup materials or molecular systems and provides high-level access to the wavelet based BigDFT code. We then present the related remotemanager library, which is able to serialize and execute arbitrary Python functions on remote supercomputers. We show how together these libraries enable transparent access to HPC based DFT calculations and can serve as building blocks for rapid prototyping and data exploration. © 2024 IOP Publishing Ltd.',\n",
       " '[No abstract available]',\n",
       " 'Measuring success plays a central role in justifying and advocating for a statistical or data science consulting or collaboration program (SDSP) within an academic institution. We present several specific metrics to report to targeted audiences to tell the story for success of a robust and sustainable program. While gathering such metrics includes challenges, we discuss potential data sources and possible practices for SDSPs to inform their own approaches. Emphasizing essential metrics for reporting, we also share the metric gathering and reporting practices of two programs in greater detail. New or existing SDSPs should evaluate their local environments and tailor their practice to gathering, analysing and reporting success metrics accordingly. This approach provides a strong foundation to use success metrics to tell compelling stories about the SDSP and enhance program sustainability. The area of success metrics provides ample opportunity for future research projects that leverage qualitative methods and consider mechanisms for adapting to the changing landscape of data science. © 2024 The Authors. Stat published by John Wiley & Sons Ltd.',\n",
       " \"The primarily peer-to-peer, graduate student-staffed Data Science Consulting Service at NC State University Libraries, within the Data & Visualization Services (DVS) department and collaborating closely with the Data Science Academy (DSA), has established a sustainable service and staffing model focused on providing broad data science analytic support to researchers across the university community. The service addresses the needs of university researchers who possess domain knowledge in their fields of study but a skills gap in the data science competencies required for research. The literature shows that it has been difficult for libraries to cover these needs with existing staffing models. Few universities follow the model practiced at NC State University, so a scan of the current landscape of data science consulting at universities across the country was performed to establish context. The support model and its advantages are described, including partnership with the DSA, student success, model sustainability and future directions for the service. Through a summary of the DVS assessment and needs evaluation process, the service's advantages in staying ahead of patron needs are illustrated. This scalable, sustainable, student-focused model could be implemented by similar research institutions to expand the capacity of their technical research services. © 2024 The Author(s). Stat published by John Wiley & Sons Ltd.\",\n",
       " '[No abstract available]',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'In this work, the response of an ammonium perchlorate (AP)-hydroxyl-terminated polybutadiene (HTPB) composite material under impact loading is presented, utilizing computational cohesive finite element method (CFEM) simulations that are validated with drop hammer experiments. This study examined the impact behaviour of AP crystal sizes between 200 and 400 μm by varying impact velocities between 3 and 10\\xa0m/s. Based on the outcome of CFEM simulations, analysis of variance (ANOVA) tests and a response surface method (RSM) were utilized to construct a mathematical model approximating the relationships between simulation inputs and outcomes. Both computational and experimental results show that the local strain rate has a considerable positive correlation with crystal size, and the rate of temperature change has positive correlations with both crystal size and impact velocity. Further, it was observed that stiffness and compression energy are the primary factors to variances in local strain rate and rate of change of temperature. RSM has been found to be an effective tool for modelling impact responses of materials under varying experimental conditions. © 2023 The Authors. Strain published by John Wiley & Sons Ltd.',\n",
       " 'The Editor-in-Chief and the publisher have retracted this article. The article was submitted to be part of a guest-edited issue. An investigation by the publisher found a number of articles, including this one, with a number of concerns, including but not limited to compromised editorial handling and peer review process, inappropriate or irrelevant references or not being in scope of the journal or guest-edited issue. Based on the investigation’s findings the Editor-in-Chief therefore no longer has confidence in the results and conclusions of this article. The authors disagree with this retraction. The online version of this article contains the full text of the retracted article as Supplementary Information. © 2023 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.',\n",
       " 'Social media data are rapidly evolving and accessible, which presents opportunities for research. Data science techniques, such as sentiment or emotion analysis which analyse textual emotion, provide an opportunity to gather insight from social media. This paper describes a systematic scoping review of interdisciplinary evidence to explore how sentiment or emotion analysis methods alongside other data science methods have been used to examine nutrition, food and cooking social media content. A PRISMA search strategy was used to search nine electronic databases in November 2020 and January 2022. Of 7325 studies identified, thirty-six studies were selected from seventeen countries, and content was analysed thematically and summarised in an evidence table. Studies were published between 2014 and 2022 and used data from seven different social media platforms (Twitter, YouTube, Instagram, Reddit, Pinterest, Sina Weibo and mixed platforms). Five themes of research were identified: dietary patterns, cooking and recipes, diet and health, public health and nutrition and food in general. Papers developed a sentiment or emotion analysis tool or used available open-source tools. Accuracy to predict sentiment ranged from 33·33% (open-source engine) to 98·53% (engine developed for the study). The average proportion of sentiment was 38·8% positive, 46·6% neutral and 28·0% negative. Additional data science techniques used included topic modelling and network analysis. Future research requires optimising data extraction processes from social media platforms, the use of interdisciplinary teams to develop suitable and accurate methods for the subject and the use of complementary methods to gather deeper insights into these complex data. © The Author(s), 2023. Published by Cambridge University Press on behalf of The Nutrition Society.',\n",
       " \"People are increasingly turning to social media and online forums like Reddit to cope with work-related concerns. Previous research suggests that how others respond can be an important determinant of the sharer's affective and well-being outcomes. However, less is known about whether and how cues embedded in the content of what is shared can shape the type of responses that one receives from others,\\xa0obscuring the joint and interactive role that both the sharer and listener may play in influencing the sharer's outcomes. In this study, we develop theory to advance our understanding of online coping with an explicitly social focus using computational grounded theorizing and machine learning (ML) techniques applied to a large corpus of work-related conversations on Reddit. Specifically, our theoretical model sheds light on the dynamics of the online social coping process related to the domain of work. We show that how sharers and listeners interact and react to one another depends on the content of stressors shared, the social coping behaviors used when sharing, and whether the sharer and listener belong to the same occupational context. We contribute to the social coping literature in three ways. First, we clarify how social actors respond to cues embedded in the social coping attempt. Second, we examine the moderating role that such responses play in shaping sharer outcomes. Finally, we extend theory on social coping with work-related stressors to the online domain. Taken together, this research highlights the importance of the dynamic interplay between sharer and listener in the context of online social coping. © 2022 Wiley Periodicals, Inc.\",\n",
       " 'With the emerging era of E-commerce and online shopping, people are also in a habit to receive default product recommendations on the web pages that they access. Google is already providing such suggestions. Till now recommendations were made only based on previous sentiments or feedback or ratings, but this research has improved the product recommendation method by including one more parameter for the same. This article represents two parameters for making predictions of product allocation to a new customer. These parameters are ratings given by the existing users for that particular product and the region to which the new customer belongs. Following these parameters, a prediction model and an algorithm, Improved_Collab_Similarity, have been implemented. The dataset has been developed where India as a country along with all its States has been considered for products which are popular for their creation based on regional and ancient skills of the people belonging to that area. Results for the mentioned prediction model have been discussed in this article where generally precision increases with the increase in a number of products but at some points, it does not increase when a smaller number of that product was purchased by the customers. © 2022 John Wiley & Sons Ltd.',\n",
       " 'Automation technologies and data science techniques have been successfully applied to optimisation and discovery activities in the chemical sciences for decades. As the sophistication of these techniques and technologies have evolved, so too has the ambition to expand their scope of application to problems of significant synthetic difficulty. Of these applications, some of the most challenging involve investigation of chemical mechanism in organometallic processes (with particular emphasis on air- and moisture-sensitive processes), particularly with the reagent and/or catalyst used. We discuss herein the development of enabling methodologies to allow the study of these challenging systems and highlight some important applications of these technologies in problems of considerable interest to applied synthetic chemists. © 2024 RSC.',\n",
       " 'The proceedings contain 19 papers. The topics discussed include: discovery of the characteristics of the cubic Othello chessboard and its implementation of visualization expert system; the application of elite genetic algorithm in sustainable agricultural transportation; comparison of interaction profiling bipartite graph mining and graph neural network for malware-control domain detection; object detection transfer learning paradigm on cross domain street vehicle detection; an AI-based approach for mystery shopping audit in customer service; real-time anomaly detection in grinding wheels using a multimodal deep learning framework; business-centric modelling and visualization for retail promotion; research on the design of third-person 3D action role-playing games; and enhancing elderly care through a 3D interactive simulation system in a day care center.',\n",
       " \"Introduction Leveraging data science could significantly advance the understanding of the health impacts of climate change and air pollution to meet health systems' needs and improve public health in Africa. This scoping review will aim to identify and synthesise evidence on the use of data science as an intervention to address climate change and air pollution-related health challenges in Africa. Methods and analysis The search strategy will be developed, and the search will be conducted in the Web of Science, Scopus, CAB Abstracts, MEDLINE and EMBASE electronic databases. We will also search the reference lists of eligible articles for additional records. We will screen titles, technical reports, abstracts and full texts and select studies reporting the use of data science in relation to the health effects and interventions associated with climate change and air pollution in Africa. Ethics and dissemination There are no formal ethics requirements as we are not collecting primary data. Results, once published, will be disseminated via conferences and shared with policy-makers and public health, air pollution and climate change key stakeholders in Africa. © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.\",\n",
       " 'A long-standing question for CHI and CSCW community is to understand data science work and everyday practices. Based on six months of in-person and virtual ethnography with a private organization located in United States and India, this study explores how data science practitioners articulate and manage risks of data science project failure. Aligning with the plurality of risk articulations and power laden risk protocols, the research uses sociotechnical lens to explore the affordances between social actors and technology for risk management. The multiple affordances identified will inform strategies to help practitioners in everyday risk identification and management. The goal of this research is to offer empirical insights and actionable frameworks that will assist practitioners in navigating complex sociotechnical risk management landscapes. © 2024 Association for Computing Machinery. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'Marketing has always been about understanding consumer behavior and using that understanding, to make data-driven decisions. With the advent of data sciences, marketers now have access to more data than ever before, which can be analyzed using a range of cutting-edge technologies. This article explores the role of data sciences in marketing management, including the challenges, applications, and opportunities that arise from the use of data sciences in this field. We also discuss how companies can leverage these technologies to drive innovation and growth, and the importance of developing a data-driven marketing culture. The paper highlights how data sciences can be used for marketing and the pros and cons associated with the use of DATA SCIENCES POWERED tools and software in marketing. The paper finally concludes with demonstrating how data sciences become inevitable for a marketer who wants to have a competitive position in the market. The paper also gives conclusion from the writer and gives his personal opinion about how data sciences and marketing are inter-related. [1] © 2024 American Institute of Physics Inc.. All rights reserved.',\n",
       " 'The health system of a nation influences the well-being of its citizens. Maternal health is about the contentment of women throughout pregnancy, childbirth, and the postpartum period. In a country with millions of people like India, there are still goals in the area of maternal healthcare that need to be met despite widespread concern by the authorities. Spatial quantification of maternal health is necessary to identify the regions of immediate concern. In light of the methods and the variables used-the result of the quantification techniques produces a range of possible outcomes. The paper builds Composite Indicators based on some parameters of maternal healthcare, using different weighting methods, namely-TOPSIS, Iyengar-Sudarshan, Principal Component Analysis, Data Envelopment Analysis, and Ordered Weighted Average. Eventually, the most robust weighting technique is identified. The study finds Lakshadweep, Kerala, and Goa have better maternal healthcare, while Bihar, Arunachal Pradesh, and Nagaland are poorly positioned. © 2024, Society of Statistics, Computer and Applications. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " 'This bibliometric analysis explores the landscape of research in Data Science and Big Data Analytics over the period from 2010 to March 2024. Leveraging advanced bibliometric techniques, including data collection from Scopus, data screening, preprocessing, and analysis using VOSviewer, Bibliometric of R package, and Microsoft Excel, this study aims to identify key trends, patterns, and dynamics within the field. The analysis encompasses document types, publication and citation trends, contributing countries, influential authors and sources, keyword co-occurrence networks, and influential affiliations. The findings provide valuable insights into the scholarly discourse, collaboration networks, and emerging research directions in Data Science and Big Data Analytics, facilitating evidence-based decision-making and fostering innovation in the field. © 2024 Seventh Sense Research Group®.',\n",
       " 'Vehicle-to-grid (V2G) technology has emerged as a promising solution for enhancing the integration of electric vehicles (EVs) into the electric grid, offering benefits, such as distributed energy resource (DER) integration, grid stability support, and peak demand management, among others, as well as environmental advantages. This study provides a comprehensive review of V2G systems, with a specific focus on the role of the communication, as they have been identified as key enablers, as well as the challenges that V2G must face. It begins by introducing the fundamentals of V2G systems, including their architecture, operation, and a description of the benefits for different sectors. It then delves into the communication technologies and protocols in V2G systems, highlighting the key requirements in achieving reliable and efficient communication between EVs and the different agents involved. A comprehensive review of communication standards is described, as well as the main communication technologies, which are evaluated in terms of their suitability for V2G applications. Furthermore, the study discusses the challenges and environmental implications of V2G technology, emphasizing the importance of addressing strong and reliable communications to maximize its potential benefits. Finally, future research directions and potential solutions for overcoming challenges in V2G systems are outlined, offering useful insights for researchers, policymakers, and administrations as well as related industry stakeholders. © 2024 by the authors.',\n",
       " '[No abstract available]',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'The State of the Union address is a traditional annual message delivered by the President of the United States to Congress. As of February 2023, there have been 244 addresses, some delivered as a letter, some as a speech, and some as both. We present a collection of these addresses as an example dataset for data science courses to motivate the introduction of concepts from linear algebra and statistics. Exercises suitable for introductory and advanced data science courses are developed. © 2024 ACM.',\n",
       " 'The objective of this study is to examine variables that affect students’ satisfaction in postgraduate programmes using a case study of students at the African Centre of Excellence in Data Science (ACE-DS) at the University of Rwanda. It is important to study student satisfaction to understand how students feel about the quality of education, academic experiences, access to facilities, campus life, and extracurricular activities. A questionnaire was designed for the study, and data was collected from 81 students enrolled in master’s and PhD programs at the African Centre of Excellence in Data Science. In this study, a chi-square test was conducted to select factors associated with students’ satisfaction to be included in the model, and a multivariable logistic regression model was used for the analysis. The findings indicated that gender, level of study, programme of study, and the country of origin were not associated with student satisfaction. In contrast, curriculum relevance, accessing course materials, learning facilities, and the support from the ACE were statistically associated with students’ satisfaction. Therefore, the logistic regression model solely took into account variables which are associated with students’ satisfaction. This study uncovered the difficulties and opportunities affecting students’ satisfaction, and the findings showed that accessing course materials and satisfaction with centre support were statistically significant, while curriculum relevance and satisfaction with learning facilities were not. The findings from this study inform higher learning institutions and decision makers about how to improve the caliber and efficiency of higher education. © 2024 Conscientia Beam. All Rights Reserved.',\n",
       " 'This chapter examined the profound influence of data science and volunteered geographic information (VGI) on the delivery of public services. Volunteered geographic information (VGI), being material created by users, has had a substantial impact on making geographic information accessible to everybody, enabling people to actively engage in the creation and management of data. The incorporation of volunteered geographic information (VGI) into government operations has introduced novel prospects for enhancing service provision in diverse sectors such as education, health, transportation, and waste management. In addition, data science has enhanced VGI by using sophisticated methodologies like artificial intelligence (AI), internet of things (IoT), big data, and blockchain, thereby transforming the whole framework of government service provision. Nevertheless, in order to effectively use VGI in public sector services, it is essential to tackle significant obstacles such as data accuracy, safeguarding, inclusiveness, technical framework, and specialized expertise. The quality of VGI data may be improved by collaborative endeavors including governments, volunteers, and academics. © 2024, IGI Global. All rights reserved.',\n",
       " 'The circular economy (CE) aims to decouple the growth of the economy from the consumption of finite resources through strategies, such as eliminating waste, circulating materials in use, and regenerating natural systems. Due to the rapid development of data science (DS), promising progress has been made in the transition toward CE in the past decade. DS offers various methods to achieve accurate predictions, accelerate product sustainable design, prolong asset life, optimize the infrastructure needed to circulate materials, and provide evidence-based insights. Despite the exciting scientific advances in this field, there still lacks a comprehensive review on this topic to summarize past achievements, synthesize knowledge gained, and navigate future research directions. In this paper, we try to summarize how DS accelerated the transition to CE. We conducted a critical review of where and how DS has helped the CE transition with a focus on four areas including (1) characterizing socioeconomic metabolism, (2) reducing unnecessary waste generation by enhancing material efficiency and optimizing product design, (3) extending product lifetime through repair, and (4) facilitating waste reuse and recycling. We also introduced the limitations and challenges in the current applications and discussed opportunities to provide a clear roadmap for future research in this field. © 2024 American Chemical Society',\n",
       " 'The proceedings contain 60 papers. The topics discussed include: privacy-preservation robust federated learning with blockchain-based hierarchical framework; spatio-temporal hypergraph convolutional network based network traffic prediction; hash function based on quantum walks with two-step memory; adversarial analysis and methods for math word problems; object tracking based on adaptive multi-template fusing; analysis of spatial-temporal variability and heterogeneity of soil moisture; can deep learning large language models be used to unravel knowledge graph creation?; binary and multi-label machine learning models for discrete-time survival analysis: a case study to predict complications and mortality in Thai diabetic patients; power factor anomaly detection using data stream summaries; and consensus filter for distributed sensor networks with unknown colored noise.',\n",
       " 'Successful data-driven science requires complex data engineering pipelines to clean, transform, and alter data in preparation for machine learning, and robust results can only be achieved when each step in the pipeline can be justified, and its effect on the data explained. In this framework, we aim at providing data scientists with facilities to gain an in-depth understanding of how each step in the pipeline affects the data, from the raw input to training sets ready to be used for learning. Starting from an extensible set of data preparation operators commonly used within a data science setting, in this work we present a provenance management infrastructure for generating, storing, and querying very granular accounts of data transformations, at the level of individual elements within datasets whenever possible. Then, from the formal definition of a core set of data science preprocessing operators, we derive a provenance semantics embodied by a collection of templates expressed in PROV, a standard model for data provenance. Using those templates as a reference, our provenance generation algorithm generalises to any operator with observable input/output pairs. We provide a prototype implementation of an application-level provenance capture library to produce, in a semiautomatic way, complete provenance documents that account for the entire pipeline. We report on the ability of that reference implementation to capture provenance in real ML benchmark pipelines and over TCP-DI synthetic data. We finally show how the collected provenance can be used to answer a suite of provenance benchmark queries that underpin some common pipeline inspection questions, as expressed on the Data Science Stack Exchange. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.',\n",
       " 'Along with the development of data science, of course, there are also various innovative solutions to support data processing. These solutions often bring with them new data science trends that are rapidly changing the way we work and live our lives. The trend of data science is using a small dataset, which has a smaller volume and is easy to access and process. It is energy-efficient, has lower costs, and can work without an internet connection. Some of the research is to create tools and platforms that anyone can use to build their own Machine Learning (ML) applications. Currently, there are many innovations in big data processing that are rooted in geospatial data. Everyone carries at least one piece of hardware that monitors their movements that generate spatial data. Also, the drone and IoT industries are highly developed and collect spatial data in real-Time. So, there is a lot of data available that is rooted in spatial data. The approach in this research will be utilizing the Geospatial tools for data science to work with vector and raster data types and how to manipulate them, implement time series analysis on maps and perform geospatial analysis using vector data. © 2024 AIP Publishing LLC.',\n",
       " 'Artificial intelligence (AI) and data science (DS) are receiving a lot of attention in various fields. In the educational field, the need for education utilizing AI and DS is also being emerged. In this context, we have created an AI/DS integrating program that generates a compound classification/regression model using characteristics of compounds and predicts classification and boiling points of compounds from an unknown dataset. Students have experienced data collection and preprocessing, exploratory data analysis, modeling, and prediction. The No-Code-Low-Code Orange3 tool has been used for the process of modeling and prediction so that even beginners can easily perform Machine Learning (ML) analysis. The raw dataset containing 24 characteristics for 277,569 compounds went through data preprocessing process and became a well-refined dataset. The Random Forest model accurately predicted whether the type of compound in the unknown dataset was hydrocarbons, alcohols, or amines and predicted the boiling points of the some arbitrary compounds within the average error range of 4.49K. This activity will provide meaningful implications for how AI/DS technology could be integrated into each domain. © 2024 American Chemical Society and Division of Chemical Education, Inc.',\n",
       " 'Herein we report a method for a stereoconvergent synthesis of trisubstituted alkenes in two steps from simple ketone starting materials. The key step is a nickel-catalyzed reduction of the corresponding enol tosylates that predominantly relies on a monophosphine ligand to direct the stereoconvergent formation of either the E- or Z-trisubstituted alkene products. Reaction optimization was accomplished using a data science workflow including monophosphine training set design, statistical modeling, and multiobjective Bayesian optimization. The optimization campaign significantly improved access to both the E- and Z-trisubstituted products in up to ∼90:10 diastereoselectivity and >90% yield. After identifying superior ligands using training set design, only 25 reactions were required for each objective (E- and Z-isomer formation) to converge on improved reaction parameters from a search space of ∼30,000 potential conditions using the EDBO+ platform. Additionally, a hierarchical machine learning model was developed to predict the stereoselectivity of untested monophosphine ligands to achieve a validation mean absolute error (MAE) of 7.1% selectivity (0.21 kcal/mol). Ultimately, we present a synergistic data science workflow leveraging the integration of training set design, statistical modeling, and Bayesian optimization, thereby expanding access to stereodefined trisubstituted alkenes. © 2024 American Chemical Society',\n",
       " 'Complex systems are prevalent in various disciplines encompassing the natural and social sciences, such as physics, biology, economics, and sociology. Leveraging data science techniques, particularly those rooted in artificial intelligence and machine learning, offers a promising avenue for comprehending the intricacies of complex systems without necessitating detailed knowledge of underlying dynamics. In this paper, we demonstrate that multiscale entropy (MSE) is pivotal in describing the steady state of complex systems. Introducing the multiscale entropy dynamics (MED) methodology, we provide a framework for dissecting system dynamics and uncovering the driving forces behind their evolution. Our investigation reveals that the MED methodology facilitates the expression of complex system dynamics through a Generalized Nonlinear Schrödinger Equation (GNSE) that thus demonstrates its potential applicability across diverse complex systems. By elucidating the entropic underpinnings of complexity, our study paves the way for a deeper understanding of dynamic phenomena. It offers insights into the behavior of complex systems across various domains. © 2024 by the authors.',\n",
       " '[No abstract available]',\n",
       " 'OBJECTIVE: Perform a scoping review of supervised machine learning in pediatric critical care to identify published applications, methodologies, and implementation frequency to inform best practices for the development, validation, and reporting of predictive models in pediatric critical care. DESIGN: Scoping review and expert opinion. SETTING: We queried CINAHL Plus with Full Text (EBSCO), Cochrane Library (Wiley), Embase (Elsevier), Ovid Medline, and PubMed for articles published between 2000 and 2022 related to machine learning concepts and pediatric critical illness. Articles were excluded if the majority of patients were adults or neonates, if unsupervised machine learning was the primary methodology, or if information related to the development, validation, and/or implementation of the model was not reported. Article selection and data extraction were performed using dual review in the Covidence tool, with discrepancies resolved by consensus. SUBJECTS: Articles reporting on the development, validation, or implementation of supervised machine learning models in the field of pediatric critical care medicine. INTERVENTIONS: None. MEASUREMENTS AND MAIN RESULTS: Of 5075 identified studies, 141 articles were included. Studies were primarily (57%) performed at a single site. The majority took place in the United States (70%). Most were retrospective observational cohort studies. More than three-quarters of the articles were published between 2018 and 2022. The most common algorithms included logistic regression and random forest. Predicted events were most commonly death, transfer to ICU, and sepsis. Only 14% of articles reported external validation, and only a single model was implemented at publication. Reporting of validation methods, performance assessments, and implementation varied widely. Follow-up with authors suggests that implementation remains uncommon after model publication. CONCLUSIONS: Publication of supervised machine learning models to address clinical challenges in pediatric critical care medicine has increased dramatically in the last 5 years. While these approaches have the potential to benefit children with critical illness, the literature demonstrates incomplete reporting, absence of external validation, and infrequent clinical implementation. © 2024 Lippincott Williams and Wilkins. All rights reserved.',\n",
       " \"Fluidized beds are critical in numerous industrial applications, including chemical processing and energy production. Accurate modeling of solid particles and fluids within these beds is essential for process optimization and improved efficiency. In this study, we developed a machine learning model to predict the coefficient of restitution (COR) of solid particles in fluidized beds. Four variables—collision velocity, effective temperature, effective mass, and effective elastic modulus—were used in the model. Our dataset comprised 2446 data points from previous literature. We employed self-organizing map (SOM) and artificial neural network (ANN) approaches for data analysis. The initial ANN model, which did not incorporate data clustering, exhibited an impressive coefficient of determination (R-squared) of 0.989, indicating its high accuracy. To enhance the model further, we clustered the data into four groups using SOM and developed separate ANN models for each group. All four models achieved R-squared greater than 0.99, illustrating the effectiveness of data clustering. The resulting model can be integrated into simulation software, such as MFIX, to provide a more precise representation of fluidized bed behavior across various industrial settings. The findings emphasize the potential of machine learning models to enhance fluidized bed simulations, leading to increased efficiency and cost-effectiveness in industrial processes. Future studies should explore the inclusion of additional variables and extend the model's application to different industrial processes. Additionally, incorporating recommendations for optimizing fluidized bed behavior based on the model's predictions would provide valuable insights for process engineers. © 2024 The Society of Powder Technology Japan. Published by Elsevier B.V. and The Society of Powder Technology Japan.\",\n",
       " '[No abstract available]',\n",
       " 'Heavy-metal hazardous wastes (HMHW) have a large emission, high toxicity and strong resource properties. Its proper disposal and resource utilization is a key challenge. High-temperature melt vitrification technology is an effective solution to realize the detoxification and resource utilization of HMHW. This paper summarizes and analyzes the control parameters of the existing vitrification treatment of HMHW. The phase transformation process of temperature, slag content, and CAS ratio (CaO-Al2O3-SiO2) in the vitrification treatment of zinc leaching residue (ZLR) was investigated around the vitreous content. The common evolution characteristics of the formed vitrification products in infrared spectrum, UV–vis diffuse reflection spectrum, and microstructures were revealed. The idea of using the maximum UV–vis absorption intensity to qualitatively determine the vitreous content of the products is proposed. Moreover, the correlation between experimental parameters and vitreous content was quantified by combining the data science method, and the mapping of the whole process of ZLR vitrification treatment was constructed. This work provides reliable parameter guidance and method induction for the vitrification treatment of HMHW, especially for wastes with less glassy matrix such as Ca and Si. © 2024 Elsevier Ltd',\n",
       " '[No abstract available]',\n",
       " 'Learning from big Earth data via supervised machine learning has become a popular approach for ambient air quality mapping. However, knowledge gap remains as which level satellite data should be used as the critical proxy variable, and how to improve data-driven models with domain knowledge are also still elusive. By taking surface NO2concentration mapping as illustration, here we performed inter-comparison studies between a set of machine-learned surface NO2concentration estimation models established with different levels of satellite products, varying from Level 1 (L1) apparent radiance from TROPOMI on board Sentinel-5p to Level 2 (L2) NO2slant column density (SCD) and tropospheric vertical column density (VCD). TROPOMI bands sensitive to surface NO2were firstly pinpointed via radiative transfer simulations while band ratios between nine sensitive and adjacent insensitive channels were then calculated and used as the counterpart of raw radiance observations. The results indicated that the prediction model trained with L1 band ratios at few discrete channels yielded higher prediction accuracy (R2= 0.71, RMSE = 7.98 μg m−3) than that using raw L1 radiance data at all available bands (R2= 0.68, RMSE = 8.40 μg m−3), largely benefiting from the improved signal-to-noise ratio and reduced model complexity due to fewer band ratio inputs. Yet even higher modeling accuracies were attained with L2 data products, the model with SCD (R2= 0.78, RMSE = 6.54 μg m−3) were found to perform even slightly better than that of VCD (R2= 0.77, RMSE = 6.79 μg m−3), though the latter is supposed to better correlate with surface NO2variations. The modeling accuracy was further improved with the inclusion of solar zenith angle, aerosol optical depth, surface albedo and pressure that are highly associated with air mass factor, with R2improved to 0.80 and RMSE reduced to 6.28 μg m−3. Overall, our results not only provide actional guidance on satellite-based surface NO2concentration modeling but also underscore the critical importance of domain knowledge in improving machine-learned models to aid in large scale air quality surveillance. © 2024 Elsevier Ltd',\n",
       " 'In this study, I examine the institutional model of data science as a nascent profession undergoing an occupational founding phase. Drawing on interviews with sixty data scientists, senior managers, and professors from Israel as well as observations at the local professional community’s events, I argue that data scientists endorse an open institutional model, upholding largely internet-based institutions focusing on knowledge sharing, networking, and collaboration. This model grants data scientists expertise, autonomy, and authority vis-à-vis clients, employers, and states; provides them with continued credentialing independent of employing organizations; encourages the wide entry of new members; and helps them deal with the accelerated temporality of their field. This open model enables an omnivorous spreading of data science expertise and is used to challenge professionalization as an occupational-institutional model in other professions. Still, this model faces many challenges. © The Author(s), under exclusive licence to Springer Nature B.V. 2023.',\n",
       " 'Python has become the prime language for application development in the data science and machine learning domains. However, data scientists are not necessarily experienced programmers. Although Python lets them quickly implement their algorithms, when moving at scale, computation efficiency becomes inevitable. Thus, harnessing high-performance devices such as multi-core processors and graphical processing units to their potential is generally not trivial. The present narrative survey can be thought of as a reference document for such practitioners to help them make their way in the wealth of tools and techniques available for the Python language. Our document revolves around user scenarios, which are meant to cover most situations they may face. We believe that this document may also be of practical use to tool developers, who may use our work to identify potential lacks in existing tools and help them motivate their contributions. © 2023 Copyright held by the owner/author(s).',\n",
       " '[No abstract available]',\n",
       " 'This chapter explored the transformative impact of data science and VGI on public service provision. VGI, as user-generated content, has played a significant role in democratizing geographic information, empowering users to actively participate in data generation and curation. The integration of VGI into government operations has presented new opportunities for improving service delivery across various domains like education, health, transportation, and waste management. Additionally, data science has complemented VGI by utilizing advanced techniques such as AI, IoT, big data, and blockchain, thereby revolutionizing the entire landscape of government service delivery. However, the successful utilization of VGI in public sector delivery requires addressing critical challenges, including data quality, security, inclusivity, technological infrastructure, and specialized skill sets. Collaborative efforts involving governments, volunteers, and academia can enhance the quality of VGI data. © 2024, IGI Global. All rights reserved.',\n",
       " 'We live in an era defined by an avalanche of data, and the challenge lies not merely in its accumulation, but in gleaning meaningful insights from this digital deluge. This monumental task is particularly significant in the realm of government service delivery, were efficient and responsive governance hinges upon informed decision-making. As traditional methods struggle to cope with the sheer magnitude of information, a new paradigm is imperative-one that leverages the potent constructive collaboration of data science and machine learning. A redesign of the existing models of such a scale require a sound understanding of the mechanics of these technologies and their applications, which is exactly where the formidable reference book, Machine Learning and Data Science Techniques for Effective Government Service Delivery, plays a pivotal role. Amidst the intricate labyrinth of modern governance, this book emerges as a beacon of solution-driven insight. Rooted in the understanding that data is emerging as a currency in the 21st century, it navigates the terrain of data science and machine learning techniques, elevating the mechanics of government service delivery. As government bodies strive to meet the ever-evolving needs of citizens, these advanced methodologies offer a compelling antidote to inefficiencies that have plagued traditional systems. By providing a comprehensive roadmap to harness the potential of data-driven decision-making, the book equips professionals, researchers, and policymakers with the tools to cultivate a new era of effective, citizen-centric governance. Tailored to a diverse audience, encompassing experts in technology, academia, government, and beyond, this book bridges the gap between theory and actionable practice. With an array of topics ranging from foundational concepts to innovative applications, it offers a holistic understanding of how data science and machine learning can revolutionize service delivery. From forecasting to privacy safeguards, from scalable frameworks to intelligent insights, this book encapsulates the spectrum of possibilities that develop when innovation meets governance. For those seeking to navigate the uncharted waters of modern governance, this book holds as an indispensable guide, ushering in a future where data-driven precision paves the way for a more effective and responsive government. © 2024 by IGI Global. All rights reserved.',\n",
       " 'A new age in public administration has been brought about by the rapid advancement of technology in the twenty-first century, which is marked by the incorporation of machine learning (ML), data science, and artificial intelligence (AI) into government services. This chapter thoroughly analyzes how these technologies are revolutionizing e-government, providing creative answers to age-old problems and altering the dynamics of public service delivery. The first section of the chapter explores the foundations of AI, data science, and ML, explaining their main ideas and applications to e-government. It highlights how new technologies can revolutionize processes, improve decision-making, and offer data-driven insights for creating policies. The conversation also touches on practical uses for these technologies, including fraud detection, effective resource allocation, public health predictive analytics, and traffic management optimization. © 2024, IGI Global. All rights reserved.',\n",
       " 'Methods to access chiral sulfur(VI) pharmacophores are of interest in medicinal and synthetic chemistry. We report the desymmetrization of unprotected sulfonimidamides via asymmetric acylation with a cinchona-phosphinate catalyst. The desired products are formed in excellent yield and enantioselectivity with no observed bis-acylation. A data-science-driven approach to substrate scope evaluation was coupled to high throughput experimentation (HTE) to facilitate statistical modeling in order to inform mechanistic studies. Reaction kinetics, catalyst structural studies, and density functional theory (DFT) transition state analysis elucidated the turnover-limiting step to be the collapse of the tetrahedral intermediate and provided key insights into the catalyst-substrate structure-activity relationships responsible for the origin of the enantioselectivity. This study offers a reliable method for accessing enantioenriched sulfonimidamides to propel their application as pharmacophores and serves as an example of the mechanistic insight that can be gleaned from integrating data science and traditional physical organic techniques. © 2024 American Chemical Society',\n",
       " 'Data science is a science that is currently very popular and can be applied in various industries. Of course, the need for proficient people in this field is growing in demand by many businesses, including large corporations. Data science is a science that combines expertise in a particular field of science with programming, mathematics, and statistics skills. The goal is to extract knowledge or information from data. The problem that usually occurs in the data science process is at the data modeling stage, namely the accuracy of the mathematical model that will be used to obtain the estimated parameter values. To overcome this problem, this research was conducted to test the correctness of analytic derivatives in mathematical models and conduct data modeling using analytic derivatives that have been tested to obtain the estimated value of each parameter. The validation process is carried out using an analytic and numeric gradient comparison. If the comparison of the two values is close to zero, then the analytical derivatives used are correct. The parameter estimation method used is the Maximum Likelihood Estimation. In this research, it was concluded that the value of the analytic gradient is the same or close to the numeric gradient value for each parameter so that the analytic derivatives are correct and can be used for the next stage. With the results of this research, the expected implication is to get more accurate data modeling so that the conclusions obtained are more precise. © 2024 Author(s).',\n",
       " \"The role of higher education in Indonesia is an essential key in determining the ability of the Indonesian nation to continue to make progress and create prosperity for all Indonesians. A country's progress in catching up with other countries also depends heavily on two other factors, namely the quality of institutions and the availability of infrastructure. The universities must create the next generation of the nation that can lay the foundation for Indonesia to become a developed country. However, since the Covid-19 pandemic, this situation has dramatically affected the learning process in universities. To prevent the spread of Covid-19 in Indonesia, the government has implemented a hybrid learning system for higher education levels. This hybrid system requires Internet access by students and faculty to help support the learning process from campus. However, the current hybrid system has not effectively and efficiently replaced formal education. Therefore, the research team took the initiative to conduct this study to observe the impact of the Covid-19 pandemic on learning outcomes through regression analysis with single and multiple predictors. The method used is regression analysis, where the parameter estimation uses the Maximum Likelihood Estimation. The regression model with single and multiple predictors resulted in the same conclusion. The relationship between the predictor variables and the responses was positive. With the results of this study, the expected implication is the utilization of the developed model to obtain policy-making scenarios by the university to achieve the targets. © 2024 Author(s).\",\n",
       " '[No abstract available]',\n",
       " 'India the land of art and culture has always been the centre of tourism since ages and people welcome and serve their guest as a god as they believed in \"Atithi Devo Bhava\"which means \"Guest is like God\". Also government providing many subsidies and incentives on making people entrepreneur and encouraging hotel and tourism industry to come up with the best ambience and hospitality for their guest. Hospitality industry providing lots of opportunity in terms of job as well as revenue for development of states like Sikkim, Himachal Pradesh, Uttarakhand etc. Tourist places are mostly situated in the extreme interiors of rural areas lead by local entrepreneurs, where life is itself a challenge to deal with the extreme climatic conditions, not well-structured accommodations, poor safety guards and lack of medical facilities that are all intrinsic to meet the basic living standards. In Uttarakhand, such rural areas include religious sites Yamunotri, Gangotri, Kedarnath and Badrinath, Chopta, Chakarata, etc along with some semi and well-developed cities like Haridwar, Rishikesh, Dehradun, Nainital, etc which all remain flooded with tourists in the season. In this COVID-19 phase, the whole industry has suffered badly and left everyone bewildered to make any decision for the future. Central and the state governments had updated some and will update several guidelines and policies for the tourism industry in the coming days. Hotels with good financial conditions and market values can easily adopt new SOP\\'s and technologies to deal with the situations after the pandemic, but it will not be the same for the small or local entrepreneurs. This study covers the current and post COVID-19 problems faced by small and local entrepreneurs of Uttarakhand and proposals and plans needed by them to boost their business and tourism with the same theme of Atithi Devo Bhava. © 2024 Author(s).',\n",
       " 'India, the land of art and culture, has always been the center of tourism since ages and people welcome and serve their guest as a god as they believed in \"Atithi Devo Bhawah\"which means \"Guest is like God\". Also, the government provides many subsidies and incentives on making people entrepreneurs and encouraging the hotel and tourism industry to come up with the best ambience and hospitality for their guests. Hospitality industry provides lots of opportunity in terms of jobs as well as revenue for development of states like Sikkim, Himachal Pradesh, Uttarakhand etc. Tourist places are mostly situated in the extreme interiors of rural areas led by local entrepreneurs, where life is itself a challenge to deal with the extreme climatic conditions, not well-structured accommodations, poor safety guards and lack of medical facilities that are all intrinsic to meet the basic living standards. In Uttarakhand, such rural areas include religious sites Yamunotri, Gangotri, Kedarnath and Badrinath, Chopta, Chakarata, etc along with some semi and well-developed cities like Haridwar, Rishikesh, Dehradun, Nainital, etc which all remain flooded with tourists in the season. In this COVID-19 phase, the whole industry has suffered badly and left everyone bewildered to make any decision for the future. Central and the state governments had updated some and will update several guidelines and policies for the tourism industry in the coming days. Hotels with good financial conditions and market values can easily adopt new SOP\\'s and technologies to deal with the situations after the pandemic, but it will not be the same for the small or local entrepreneurs. This study covers the current and post COVID-19 problems faced by small and local entrepreneurs of Uttarakhand and proposals and plans needed by them to boost their business and tourism with the same theme of Atithi Devo Bhawah. © 2024 Author(s).',\n",
       " \"Purpose: The study aims to develop a model that supports the application of data science techniques for real estate professionals in the fourth industrial revolution (4IR) era. The present 4IR era gave birth to big data sets and is beyond real estate professionals' analysis techniques. This has led to a situation where most real estate professionals rely on their intuition while neglecting a rigorous analysis for real estate investment appraisals. The heavy reliance on their intuition has been responsible for the under-performance of real estate investment, especially in Africa. Design/methodology/approach: This study utilised a survey questionnaire to randomly source data from real estate professionals. The questionnaire was analysed using a combination of Statistical package for social science (SPSS) V24 and Analysis of a Moment Structures (AMOS) graphics V27 software. Exploratory factor analysis was employed to break down the variables (drivers) into meaningful dimensions helpful in developing the conceptual framework. The framework was validated using covariance-based structural equation modelling. The model was validated using fit indices like discriminant validity, standardised root mean square (SRMR), comparative fit index (CFI), Normed Fit Index (NFI), etc. Findings: The model revealed that an inclusive educational system, decentralised real estate market and data management system are the major drivers for applying data science techniques to real estate professionals. Also, real estate professionals' application of the drivers will guarantee an effective data analysis of real estate investments. Originality/value: Numerous studies have clamoured for adopting data science techniques for real estate professionals. There is a lack of studies on the drivers that will guarantee the successful adoption of data science techniques. A modern form of data analysis for real estate professionals was also proposed in the study. © 2022, Emerald Publishing Limited.\",\n",
       " 'Logica (= Logic + aggregation) is a freely available, open-source, feature-enhanced version of Datalog that automatically compiles logic rules to several SQL platforms, i.e., a lightweight, serverless SQLite engine, a multi-user client-server PostgreSQL system, and a highly scalable, parallel BigQuery instance. Logica combines the beginner-friendly declarative features of Datalog (intuitive, pattern-based queries in the style of QBE), with advanced analytical features needed by data science practitioners when processing large, real-world datasets. The system has been used for data science applications and training in industry, and in graduate-level courses in academia. Logica allows beginners to seamlessly progress from traditional (toy) examples to intermediate and advanced use cases. © 2024 Copyright held by the owner/author(s).',\n",
       " \"Attitudes play an important role in students' academic achievement and retention, yet quality tools to measure them are not readily available in the new field of data science. Through funding from the National Science Foundation, we are developing a family of instruments that measure attitudes toward data science in the context of an introductory, college-level course. This poster will showcase preliminary results discussing pilot instruments to assess instructors' attitudes toward teaching data science and an inventory which captures classroom characteristics. These instruments, based on Expectancy-Value Theory, will enable data science education researchers to evaluate pedagogical innovations and measure instructional effectiveness relating to student attitudes. We invite instructors of data science courses to join in this discussion and to use these instruments for their own data science education research projects. © 2024 Owner/Author.\",\n",
       " 'Learning-by-teaching is an active learning method that has the promise of engaging students and enhancing their learning. These benefits include the improvement of the metacognitive process, increased motivation and self-efficacy, and the opportunity to refine communication skills. These benefits are particularly valuable in data science education. Though not as widely studied, teaching assignments, where students demonstrate competency through teaching others can be used as a formative assessment tool. This paper describes an \"alternative midterm\"experiment conducted in an undergraduate introductory data science class. In this pilot, students were asked to demonstrate their newly learned data science skills by teaching them to individuals without data science backgrounds. The evidence of learning will be illustrated through an in-depth analysis of the students\\' teaching products. These products consist of two parts: (1) \"Teaching Preparation\"materials, such as Python Notebooks containing problems and solutions crafted by the students, and (2) \"Teaching Implementation\", video recordings of real-time interactions between the students and their \"tutees.\"The paper will also present qualitative feedback from the students, highlighting promising signs of engagement and acceptance. We will conclude by discussing the challenges and opportunities of implementing this assessment strategy on a larger scale. © 2024 Owner/Author.',\n",
       " \"Data Science is an in-demand skill in the job market. To meet the demand, universities are offering data science courses or programs. These courses/programs not only give students data science skills, but also awareness of the field, increasing the likelihood of their opting for data science as a career. In this work we look at how the students' prior academic experience affects the outcome of an introductory data science class that requires no pre-requisites. We conducted the study in an undergraduate introductory data science class (IS 296) at the University of Maryland, Baltimore County (UMBC). The course was adapted from University of California Berkeley's Data 8 course. A pre and post survey was conducted to measure four factors in light of Social Cognitive Career Theory (SCCT): self-efficacy, identity, motivation and belonging uncertainty of students as a data scientist before and after taking the class. Our results show that although the course was designed requiring no pre-requisites, students with no prior programming experience and no statistics experience showed decrease in the four factors as compared to students with some prior experience. © 2024 Owner/Author.\",\n",
       " 'Now, more than ever, there is a need for computer scientists and data scientists to be socially responsible about the algorithms they design and the products they build. We all have to conscientiously analyze the impact of our work, particularly on historically marginalized communities. As we look towards creating a more just future, this means we also need to provide our students appropriate training, so that we can all address existing social inequities and prevent further social inequities from creeping into our work. Data science1 provides an ideal opportunity to train students to identify existing bias and explore ways to address it. This special session equips educators with tools to bring about this training. The presenters will share their experiences in incorporating data science for social justice in their courses, introducing CS through a social justice lens, and interrogating the collection and use of data. Through moderated group discussions, this special session will collate ideas and strategies from the participants of their respective experiences. The presenters will create and share a repository of the collated ideas to enable a broad swath of educators to incorporate data science for social justice in their respective courses to train the future generation. © 2024 Owner/Author.',\n",
       " \"students to learn in order to succeed in an increasingly data-driven world. Foundational data literacy skills currently live in a number of subjects across K-12 (e.g., data collection and analysis in science classes, statistical calculations in mathematics/statistics, data visualization and communication in civics/social studies), however, a growing number of schools and districts are introducing stand-alone data science (DS) courses. Given the centrality of computing and programming in the contemporary practice of DS, many of these courses include topics historically reserved for computer science (CS) classes. Further, many CS courses include dedicated time for DS topics (e.g., AP Computer Science Principles' unit on Data). In many ways, DS educators and CS educators are working towards the same ends in complementary ways. However, at other times, the two disciplines are in tension, especially given the scarcity of time in K-12 student schedules for non-core subjects. This panel will explore what DS education and CS education can learn from each other, how each can contribute and advance the goals of the other, and how these two intertwined disciplines can productively live alongside each other in K-12 settings. © 2024 Owner/Author.\",\n",
       " 'Table transformations are a critical skill to master in order to fluently work with data. In introductory data science courses, however, students have found these transformations particularly challenging to learn. One complex transformation is the pivot transformation, which reorganizes a table based on aggregation and summarizing along selected columns and rows. Current assessments test student understanding in static scenarios. Thus, there is an opportunity to help students explicitly work through the steps and variables needed to express a pivot transformation in a randomizable manner. As such, we explore whether a dynamic digital assessment for the pivot transformation can effectively achieve mastery learning towards this skill. Our design is inspired by Parsons problems, in which answer components (pivot table output labels and values) can be composed into the output of a pivot transformation. A question can be derived from a small randomized dataset, and randomized Pandas code that operates upon the dataset and can be autograded. We plan to conduct a pilot study with data science students to investigate whether 1) using a programmable online platform to practice pivot tables helps improve performance on exams, and 2) randomization and instant feedback on the online platform contribute to improved student learning. © 2024 Owner/Author.',\n",
       " 'Data science education can help broaden participation in computer science (CS) because it provides rich, authentic contexts for students to apply their computing knowledge. Data literacy, particularly among underrepresented students, is critical to everyone in this increasingly digital world. However, the integration of data science into K-12 schools is nascent, and the pedagogical training of CS teachers in data science remains limited. Our research-practice partnership modified an existing data science unit to include two pedagogical techniques known to support minoritized students: rich classroom discourse and personally-relevant problem-solving. This paper describes the iterative design process we used to revise and pilot this new data science unit. © 2024 ACM.',\n",
       " 'Ensuring the fairness of machine learning (ML) systems for individuals with disabilities is crucial. Proactive measures are required to identify and mitigate biases in data and models, thereby preventing potential harm or bias against people with disabilities. While previous research on ML fairness education primarily concentrated on gender and race fairness, the domain of disability fairness has received comparatively little attention. Addressing this gap, we adopted a student-centric approach to craft a disability fairness teaching intervention. A focus group of students experienced in ML and accessible computing underscored the significance of engagement and scaffolding strategies for effectively learning intricate topics. Consequently, we crafted a disability fairness hands-on programming assignment that delves into uncovering disability bias with a lens that takes intersectionality into account. The assignment was tailored for an introductory undergraduate data science (DS) course. We employed reflective questions and surveys to gauge the effectiveness of our approach. The findings indicate the success of our approach in promoting a deeper understanding of disability fairness within the context of DS education. © 2024 ACM.',\n",
       " \"This experience report describes a book club model for an undergraduate-level Big Data Analytics course. Course learning outcomes included communicating ethical implications of data and models and working collaboratively with other students in crafting solutions by listening and demonstrating. Students read Weapons of Math Destruction by Cathy O'Neil, individually answered reading questions, and collaborated on activities in three class meetings. Students' participation and activity completion rates exceeded 90%, indicating engagement with the book club model. To understand students' experiences with the activities, students' work and survey responses were analyzed. The book club activities expanded students' understanding of bias in data and models, the potential misuse and harm when using and creating software, and how models can target users. In addition to providing activities for a specific book, this paper can serve as a template for using the book club model in any computing course. © 2024 Owner/Author.\",\n",
       " \"ChatGPT is a conversational AI platform that can produce code to solve problems when provided with a natural language prompt. Prior work on similar AI models has shown that they perform well on typical intro-level Computer Science problems. However, little is known about the performance of such tools on Data Science (DS) problems. In this work, we assess the performance of ChatGPT on assignments from three DS courses with varying difficulty levels. First, we apply the raw assignment prompts provided to the students and find that ChatGPT performs well on assignments with dataset(s) descriptions and progressive question prompts, which divide the programming requirements into sub-problems. Then, we perform prompt engineering on the assignments for which ChatGPT had low performance. We find that the following prompt engineering techniques significantly increased ChatGPT's performance: breaking down abstract questions into steps, breaking down steps into multiple prompts, providing descriptions of the dataset(s), including algorithmic details, adding specific instructions to entice specific actions, and removing extraneous information. Finally, we discuss how our findings suggest potential changes to curriculum design of DS courses. © 2024 ACM.\",\n",
       " 'The prevalence of data across all disciplines and the large workforce demand from industry has led to the rise in interest of data science courses. Educators are increasingly recognizing the value of building communities of practice and adapting and translating courses and programs that have been shown to be successful and sharing lessons learned in increasing diversity in data science education. We describe and analyze our experiences translating a lower-division data science curriculum from one university, University of California, Berkeley, to another setting with very different student populations and institutional context, University of Maryland, Baltimore County (UMBC). We present our findings from student interviews across two semesters of the course offering at UMBC specifically focusing on the challenges and positive experiences that the students had in the UMBC course. We highlight lessons learned to reflect on the existing large scale program at UC Berkeley, its adaptation and opportunities for increasing diversity in new settings. Our findings emphasize the importance of adapting courses and programs to existing curricula, student populations, cyberinfrastructure, and faculty and staff resources. Smaller class sizes open up the possibility of more individualized assignments, tailored to the majors, career interests, and social change motivations of diverse students. While students across institutional contexts may need varying degrees of support, we found that often students from diverse backgrounds, if engaged deeply, show significant enthusiasm for data science and its applications. © 2024 ACM.',\n",
       " \"Data Science (DS) has emerged as a new academic discipline where students are introduced to data-centric thinking and generating data-driven insights through programming. Unlike traditional introductory Computer Science (CS) education, which focuses on program syntax and core CS topics (e.g., algorithms and data structures), introductory DS education emphasizes skills such as analyzing data to gain insights by making effective use of programming libraries (e.g., re, NumPy, pandas, scikit-learn). To better understand learners' needs and pain points when they are introduced to DS programming, we investigated a large online course on data manipulation designed for graduate students who do not have a CS or Statistics undergraduate degree. We qualitatively analyzed students' incorrect code submissions for computational notebook-based assignments in Python. We identified common mistakes and grouped them into the following themes: (1) programming language and environment misconceptions, (2) logical mistakes due to data or problem-statement misunderstanding or incorrectly dealing with missing values, (3) semantic mistakes due to incorrect use of DS libraries, and (4) suboptimal coding. Our work provides instructors insights to understand student needs in introductory DS courses and improve course pedagogy, and recommendations for developing assessment and feedback tools to support students in large courses. © 2024 ACM.\",\n",
       " 'Large language models (LLMs) are a new asset class in the machine-learning landscape. Here we offer a primer on defining properties of these modeling techniques. We then reflect on new modes of investigation in which LLMs can be used to reframe classic neuroscience questions to deliver fresh answers. We reason that LLMs have the potential to (1) enrich neuroscience datasets by adding valuable meta-information, such as advanced text sentiment, (2) summarize vast information sources to overcome divides between siloed neuroscience communities, (3) enable previously unthinkable fusion of disparate information sources relevant to the brain, (4) help deconvolve which cognitive concepts most usefully grasp phenomena in the brain, and much more. © 2024 The Author(s)',\n",
       " 'Examining the ethical aspects of artificial intelligence (AI) and data science (DS) recognizes their impressive progress in innovation while emphasizing the pressing necessity to tackle intricate ethical dilemmas. The chapter provides a detailed framework for navigating the changing environment, beginning with an examination of the increasing ethical challenges. The study highlights transparency, fairness, and responsibility as crucial for cultivating confidence in AI systems. The chapter emphasizes the urgent requirement to address problems such as algorithmic bias and privacy breaches with strong mitigation techniques. Furthermore, it promotes flexible policies that strike a balance between innovation and ethical safeguards. The examination of societal effects, particularly on various socioeconomic groups, economies, and cultures, is conducted thoroughly, with a focus on equity and the protection of individual rights. Finally, to proactively tackle future ethical challenges in technology, it is advisable to employ proactive solutions such as implementing AI ethics by design. © 2024, IGI Global. All rights reserved.',\n",
       " 'Artificial intelligence is becoming more and more widespread in our increasingly connected world. Artificial intelligence is slowly but surely modifying the way we live and work, from self-driving cars to automated customer service agents. As artificial intelligence becomes more sophisticated, the ethical implications of its use become more complex. There are several key issues to consider regarding the ethics of artificial intelligence, such as data privacy, algorithmic bias, and socioeconomic inequality. The rapid development of AI brings with it several ethical issues. However, we must remain vigilant in protecting our fundamental rights and freedoms. We must ensure that artificial intelligence is not used to discriminate against vulnerable groups or invade our privacy. We must also be careful that AI does not become a tool for the powerful to control and manipulate the masses. But while there are risks, the author believes the potential benefits of AI are too great to ignore. © 2024, IGI Global. All rights reserved.',\n",
       " 'This chapter examined the profound influence of data science and volunteered geographic information (VGI) on the delivery of public services. Volunteered geographic information, being material created by users, has had a substantial impact on making geographic information accessible to everybody, enabling people to actively engage in the creation and management of data. The incorporation of VGI into government operations has introduced novel prospects for enhancing service provision in diverse sectors such as education, health, transportation, and waste management. In addition, data science has enhanced VGI by using sophisticated methodologies like artificial intelligence (AI), internet of things (IoT), big data, and blockchain, thereby transforming the whole framework of government service provision. Nevertheless, in order to effectively use VGI in public sector services, it is essential to tackle significant obstacles such as data accuracy, safeguarding, inclusiveness, technical framework, and specialized expertise. © 2024, IGI Global. All rights reserved.',\n",
       " 'The history of artificial intelligence (AI) and data science has their origins in the 1940s and 1950s respectively. However, it has been through many changes throughout its history. AI is a vast and fascinating subject. There are many more elements to discover and understand. This chapter aims to outline the history of AI and data science, from its origin to its current developments. It will also explore the ethical considerations within AI and data science, such as bias and fairness, transparency, data privacy, etc. In the end, the chapter sheds light on the ethical concerns regarding the implementation of AI and the security concerns that data science poses. The chapter also provides insights into the role of individuals, government, and society in mitigating these issues. This chapter aims to furnish the reader with the scientific foundation and essential understanding required for embarking on the journey to comprehend the realm of artificial intelligence and data science. © 2024, IGI Global. All rights reserved.',\n",
       " 'Real-world clinical evaluation of traditional Chinese medicine (RWCE-TCM) is a method for comprehensively evaluating the clinical effects of TCM, with the aim of delving into the causality between TCM intervention and clinical outcomes. The study explored data science and causal learning methods to transform RWD into reliable real-world evidence, aiming to provide an innovative approach for RWCE-TCM. This study proposes a 10-step data science methodology to address the challenges posed by diverse and complex data in RWCE-TCM. The methodology involves several key steps, including data integration and warehouse building, high-dimensional feature selection, the use of interpretable statistical machine learning algorithms, complex networks, and graph network analysis, knowledge mining techniques such as natural language processing and machine learning, observational study design, and the application of artificial intelligence tools to build an intelligent engine for translational analysis. The goal is to establish a method for clinical positioning, applicable population screening, and mining the structural association of TCM characteristic therapies. In addition, the study adopts the principle of real-world research and a causal learning method for TCM clinical data. We constructed a multidimensional clinical knowledge map of \"disease-syndrome-symptom-prescription-medicine\"to enhance our understanding of the diagnosis and treatment laws of TCM, clarify the unique therapies, and explore information conducive to individualized treatment. The causal inference process of observational data can address confounding bias and reduce individual heterogeneity, promoting the transformation of TCM RWD into reliable clinical evidence. Intelligent data science improves efficiency and accuracy for implementing RWCE-TCM. The proposed data science methodology for TCM can handle complex data, ensure high-quality RWD acquisition and analysis, and provide in-depth insights into clinical benefits of TCM. This method supports the intelligent translation and demonstration of RWD in TCM, leads the data-driven translational analysis of causal learning, and innovates the path of RWCE-TCM. © Wolters Kluwer Health, Inc. All rights reserved.',\n",
       " 'We contribute to the Science in the Forest, Science in the Past series by investigating the specific practices of “data science”, a set of contemporary methods associated with large-scale data processing infrastructure that shares many characteristics with artificial intelligence technologies. We offer a critical history of data science in relation to the engagement of Western publics with environmental policy through “citizen science”, and contrast those developments with the authors’ different experiences and perspectives of applying aspects of data science and citizen science in collaboration with local communities in South America and Africa. © The Author(s) 2024.',\n",
       " 'This article introduces process mining as an innovative approach to enterprise data analysis, offering a systematic method for extracting, analyzing, and visualizing digital traces within information systems. The technique establishes connections within data, forming intricate process maps that serve as a foundation for the comprehensive analysis, interpretation, and enhancement of internal business processes. The article presents a methodical procedure designed to analyze processes using process mining. This methodology was validated through a case study conducted in the Fluxicon Disco software (version 3.6.7) application environment. The primary objective of this study was to propose and practically validate a methodical procedure applied to industrial practice data. Focusing on the evaluation and optimization of manufacturing processes, the study explored the integration of a software tool to enhance efficiency. The article highlights key trends in the field, providing valuable insights into process flows and identifying areas for improvement. The results contribute to the growing body of knowledge in process mining, emphasizing its applicability in fostering a more efficient and competitive manufacturing environment. In the model example, we successfully achieved a reduction in the time required for production cycles by 15% and improved resource utilization by 20%. This resulted in an increased process efficiency and a potential reduction in the required number of workers by up to 10%. These outcomes offer promising evidence of the advantages of our method and its application in an industrial setting. © 2024 by the authors.',\n",
       " 'Current research on the career satisfaction of graduates limits educational institutions in devising methods to attain high career satisfaction. Thus, this study aims to use data science models to understand and predict career satisfaction based on information collected from surveys of university alumni. Five machine learning (ML) algorithms were used for data analysis, including the decision tree, random forest, gradient boosting, support vector machine, and neural network models. To achieve optimal prediction performance, we utilized the Bayesian optimization method to fine-tune the parameters of the five ML algorithms. The five ML models were compared with logistic and ordinal regression. Then, to extract the most important features of the best predictive model, we employed the SHapley Additive exPlanations (SHAP), a novel methodology for extracting the significant features in ML. The results indicated that gradient boosting is a marginally superior predictive model, with 2–3% higher accuracy and area under the receiver operating characteristic curve (AUC) compared to logistic and ordinal regression. Interestingly, concerning low career satisfaction, those with the worst scores for the phrase “how frequently applied knowledge, skills, or technological tools from the academic training” were less satisfied with their careers. To summarize, career satisfaction is related to academic training, alumni satisfaction, employment status, published articles or books, and other factors. © 2024 by the authors.',\n",
       " 'Research in lean production has recently focused on linking lean production to Industry 4.0 by discussing the positive relationship between them. In the context of Industry 4.0, data science plays a fundamental role, and operations management research is dedicating particular attention to this field. However, the literature on the empirical implementation of data science to lean production is still under-investigated and details are lacking in most of the reported contributions. In this study, multiple case studies were conducted involving the Italian manufacturing sector to collect evidence of the application of data science to support lean production and to understand it. The results provide empirical proof of the link and examples of a variety of data science techniques and tools that can be combined to support lean production practices. The findings offer insights into the applications of the traditional lean plan–do–check–act cycle, supporting feedback on performance metrics, total productive maintenance, total quality management, statistical process control, root cause analysis for problem-solving, visual management, and Kaizen. © 2024 by the authors.',\n",
       " 'Wireless networks play a pivotal role in various domains, including industrial automation, autonomous vehicles, robotics, and mobile sensor networks. This research investigates the critical issue of packet loss in modern wireless networks and aims to identify the conditions within a network’s environment that lead to such losses. We propose a packet status prediction model for data packets that travel through a wireless network based on the IEEE 802.15.4 standard and are exposed to five different types of interference in a controlled experimentation environment. The proposed model focuses on the packetization process and its impact on network robustness. This study explores the challenges posed by packet loss, particularly in the context of interference, and puts forth the hypothesis that specific environmental conditions are linked to packet loss occurrences. The contribution of this work lies in advancing our understanding of the conditions leading to packet loss in wireless networks. Data are retrieved with a single CC2531 USB Dongle Packet Sniffer, whose pieces of information on packets become the features of each packet from which the classifier model will gather the training data with the aim of predicting whether a packet will unsuccessfully arrive at its destination. We found that interference causes more packet loss than that caused by various devices using a WiFi communication protocol simultaneously. In addition, we found that the most important predictors are network strength and packet size; low network strength tends to lead to more packet loss, especially for larger packets. This study contributes to the ongoing efforts to predict and mitigate packet loss, emphasizing the need for adaptive models in dynamic wireless environments. © 2024 by the authors.',\n",
       " 'Data science has become increasingly popular due to emerging technologies, including generative AI, big data, deep learning, etc. It can provide insights from data that are hard to determine from a human perspective. Data science in finance helps to provide more personal and safer experiences for customers and develop cutting-edge solutions for a company. This paper surveys the challenges and opportunities in applying data science to finance. It provides a state-of-the-art review of financial technologies, algorithmic trading, and fraud detection. Also, the paper identifies two research topics. One is how to use generative AI in algorithmic trading. The other is how to apply it to fraud detection. Last but not least, the paper discusses the challenges posed by generative AI, such as the ethical considerations, potential biases, and data security. © 2023 by the authors. Licensee MDPI, Basel, Switzerland.',\n",
       " 'Translational bioinformatics and data science play a crucial role in biomarker discovery as it enables translational research and helps to bridge the gap between the bench research and the bedside clinical applications. Thanks to newer and faster molecular profiling technologies and reducing costs, there are many opportunities for researchers to explore the molecular and physiological mechanisms of diseases. Biomarker discovery enables researchers to better characterize patients, enables early detection and intervention/prevention and predicts treatment responses. Due to increasing prevalence and rising treatment costs, mental health (MH) disorders have become an important venue for biomarker discovery with the goal of improved patient diagnostics, treatment and care. Exploration of underlying biological mechanisms is the key to the understanding of pathogenesis and pathophysiology of MH disorders. In an effort to better understand the underlying mechanisms of MH disorders, we reviewed the major accomplishments in the MH space from a bioinformatics and data science perspective, summarized existing knowledge derived from molecular and cellular data and described challenges and areas of opportunities in this space. © 2024 Oxford University Press. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'Background: Aldosterone plays a key role in the neurohormonal drive of heart failure. Systematic prioritization of drug targets using bioinformatics and database-driven decision-making can provide a competitive advantage in therapeutic R&D. This study investigated the evidence on the druggability of these aldosterone targets in heart failure. Methods: The target disease predictability of mineralocorticoid receptors (MR) and aldosterone synthase (AS) in cardiac failure was evaluated using Open Targets target-disease association scores. The Open Targets database collections were downloaded to MongoDB and queried according to the desired aggregation level, and the results were retrieved from the Europe PMC (data type: text mining), ChEMBL (data type: drugs), Open Targets Genetics Portal (data type: genetic associations), and IMPC (data type: genetic associations) databases. The target tractability of MR and AS in the cardiovascular system was investigated by computing activity scores in a curated ChEMBL database using supervised machine learning. Results: The medians of the association scores of the MR and AS groups were similar, indicating a comparable predictability of the target disease. The median of the MR activity scores group was significantly lower than that of AS, indicating that AS has higher target tractability than MR [Hodges-Lehmann difference 0.62 (95%CI 0.53–0.70, p < 0.0001]. The cumulative distributions of the overall multiplatform association scores of cardiac diseases with MR were considerably higher than with AS, indicating more advanced investigations on a wider range of disorders evaluated for MR (Kolmogorov-Smirnov D = 0.36, p = 0.0009). In curated ChEMBL, MR had a higher cumulative distribution of activity scores in experimental cardiovascular assays than AS (Kolmogorov-Smirnov D = 0.23, p < 0.0001). Documented clinical trials for MR in heart failures surfaced in database searches, none for AS. Conclusions: Although its clinical development has lagged behind that of MR, our findings indicate that AS is a promising therapeutic target for the treatment of cardiac failure. The multiplatform-integrated identification used in this study allowed us to comprehensively explore the available scientific evidence on MR and AS for heart failure therapy. © 2024 Elsevier Ltd',\n",
       " 'Phase transformations are a challenging problem in materials science, which lead to changes in properties and may impact performance of material systems in various applications. We introduce a general framework for the analysis of particle growth kinetics by utilizing concepts from machine learning and graph theory. As a model system, we use image sequences of atomic force microscopy showing the crystallization of an amorphous fluoroelastomer film. To identify crystalline particles in an amorphous matrix and track the temporal evolution of the particle dispersion, we have developed quantitative methods of 2D analysis. 700 image sequences were analyzed using a neural network architecture, achieving 0.97 pixel-wise classification accuracy as a measure of the correctly classified pixels. The growth kinetics of isolated and impinged particles were tracked throughout time using these image sequences. The relationship between image sequences and spatiotemporal graph representations was explored to identify the proximity of crystallites from each other. The framework enables the analysis of all image sequences without the requirement of sampling for specific particles or timesteps for various materials systems. © The Author(s) 2024.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'Context: Data can guide decision-making to improve the health of communities, but potential for use can only be realized if public health professionals have data science skills. However, not enough public health professionals possess the quantitative data skills to meet growing data science needs, including at the Centers for Disease Control and Prevention (CDC). Program: The Data Science Upskilling (DSU) program increases data science literacy among staff and fellows working and training at CDC. The DSU program was established in 2019 as a team-based, project-driven, on-the-job applied upskilling program. Learners, within interdisciplinary teams, use curated learning resources to advance their CDC projects. The program has rapidly expanded from upskilling 13 teams of 31 learners during 2019-2020 to upskilling 36 teams of 143 learners during 2022-2023. Evaluation: All 2022-2023 cohort respondents to the end-of-project survey reported the program increased their data science knowledge. In addition, 90% agreed DSU improved their data science skills, 93% agreed it improved their confidence making data science decisions, and 96% agreed it improved their ability to perform data science work that benefits CDC. Discussion: DSU is an innovative, inclusive, and successful approach to improving data science literacy at CDC. DSU may serve as an upskilling model for other organizations. Copyright © 2024 Wolters Kluwer Health, Inc. All rights reserved.',\n",
       " \"Customer attrition in the banking industry occurs when consumers quit using the goods and services offered by the bank for some time and, after that, end their connection with the bank. Therefore, customer retention is essential in today's extremely competitive banking market. Additionally, having a solid customer base helps attract new consumers by fostering confidence and a referral from a current clientele. These factors make reducing client attrition a crucial step that banks must pursue. In our research, we aim to examine bank data and forecast which users will most likely discontinue using the bank's services and become paying customers. We use various machine learning algorithms to analyze the data and show comparative analysis on different evaluation metrics. In addition, we developed a Data Visualization RShiny app for data science and management regarding customer churn analysis. Analyzing this data will help the bank indicate the trend and then try to retain customers on the verge of attrition. © 2023 Xi'an Jiaotong University\",\n",
       " 'The field of data ethics is concerned with the ethical considerations surrounding data, algorithms, and associated practices, with the aim of identifying ethical solutions. The application of ethical principles to the handling of data, algorithms, and practices can facilitate the identification and delineation of ethical quandaries within the domain of data science. The present study focuses on the topic of data ethics, specifically pertaining to the processes of data collection, data model construction, evaluation, and deployment. This study introduces a comprehensive framework designed to facilitate the management of ethical considerations in data science projects. In order to authenticate the framework, a case study was conducted and our perspectives on its practical implementation were presented. The description of the scope of future research is also provided. © 2023 The Authors. Engineering Reports published by John Wiley & Sons Ltd.',\n",
       " 'This communication presents a novel approach to set up a machine learning-ready database for epoxidation reactions, focusing on vanadium catalysts. Utilising data driven analysis, we identified key reaction yield trends through chemical descriptors, providing insights for catalyst design and reaction optimisation. © 2024 The Royal Society of Chemistry.',\n",
       " \"In this study, we explore the roles of AI-assisted ChatGPT (Generative Pre-trained Transformer) in the field of data science. AI-assisted ChatGPT, a powerful language model, is fine-tuned using domain-specific data for specialised data science tasks, such as sentiment analysis and named entity recognition (NER). The results reveal significant reductions in model size and memory usage with minor trade-offs in inference time, providing valuable resource-efficient deployment. Various data augmentation methods, including back-translation, synonym replacement, and contextual word embeddings, are employed to augment the training dataset. The study's results are subjected to rigorous statistical analysis, including paired t-tests and ANOVA tests, to determine the significance of the findings. The research concludes with insightful suggestions and future scope, including advanced fine-tuning strategies, model optimization techniques, and ethical considerations. © 2024 E3S Web of Conferences\",\n",
       " 'In the information age, organizations are more powerful as they have more information. However, having information is not enough. It needs to be compiled and organized so that it can be used. This compiled information can be used in many areas of an organization, including the recruitment of new employees. Organizations are always looking for ways to improve productivity and profitability. The COVID-19 pandemic has made this even more important. To do this, they need employees with the right skills for the job. This is where data science comes in. Data science is the field of study that analyses and processes data so that it can be used to make and create informed decisions. This study aims to investigate how data science can be used to help organizations hire new employees. The project will explore how data science can be used to identify the skills and qualifications that are most important for a particular role, screen candidates more effectively, and make better hiring decisions. © 2024, IGI Global. All rights reserved.',\n",
       " 'The interplay between Municipal Solid Waste (MSW) Management and data science unveils a panorama of opportunities and challenges, set against the backdrop of rising global waste and evolving technological landscapes. This article threads through the multifaceted aspects of incorporating data science into MSW management, unearthing key findings, novel knowledge, and instigating a call to action for stakeholders (e.g. policymakers, local authorities, waste management professionals, technology developers, and the general public) across the spectrum. Predominant challenges like the enigmatic nature of “black-box” models and tangible knowledge gaps in the sector are scrutinized, ushering in a narrative that emphasizes transparent, stakeholder-inclusive, and policy-adaptive approaches. Notably, a conscious shift towards “white-box” and “grey-box” data science models has been spotlighted as a pivotal response to transparency issues. Furthermore, the discourse highlights the necessity of crafting data science solutions that are specifically moulded to the nuanced challenges of MSW management, and it underscores the importance of recalibrating existing policies to be reflexive to technological advancements. A resolute call echoes for stakeholders to not just adapt but immerse themselves in a continuous learning trajectory, championing transparency, and fostering collaborations that hinge on innovative, data-driven methodologies. Thus, as the realms of data science and MSW management entwine, the article sheds light on the potential transformation awaiting waste management paradigms, contingent on the nurtured amalgamation of technological advances, policy alignment, and collaborative synergy. © 2023 Elsevier Inc.',\n",
       " 'Data Science for Agricultural Innovation and Productivity explores the transformation of agriculture through data-driven practices. This comprehensive book delves into the intersection of data science and farming, offering insights into the potential of big data analytics, machine learning, and IoT integration. Readers will find a wide range of topics covered in 10 chapters, including smart farming, AI applications, hydroponics, and robotics. Expert contributors, including researchers, practitioners, and academics in the fields of data science and agriculture, share their knowledge to provide readers with up-to-date insights and practical applications. The interdisciplinary emphasis of the book gives a well-rounded view of the subject. With real-world examples and case studies, this book demonstrates how data science is being successfully applied in agriculture, inspiring readers to explore new possibilities and contribute to the ongoing transformation of the agricultural sector. Sustainability and future outlook are the key themes, as the book explores how data science can promote environmentally conscious agricultural practices while addressing global food security concerns. Key Features: - Focus on data-driven agricultural practices - Comprehensive coverage of modern farming topics with an interdisciplinary perspective - Expert insights - Sustainability and future outlook - Highlights practical applications Data Science for Agricultural Innovation and Productivity is an essential resource for researchers, data scientists, farmers, agricultural technologists, students, educators, and anyone with an interest in the future of farming through data-driven agriculture. Readership Researchers, data scientists, farmers, agricultural technologists, students, educators, and general readers. © 2024, Bentham Books imprint. All rights reserved.',\n",
       " 'Healthcare systems have made rapid progress towards combining data science with precision medicine, particularly in pharmacogenomics. With the lack of predictability in medication effectiveness from patient to patient, acquiring the specifics of their genotype would be highly advantageous for patient treatment. Genotype-guided dosing adjustment improves clinical decision-making and helps optimize doses to deliver medications with greater efficacy and within safe margins. Current databases demand extensive effort to locate relevant genetic dosing information. To address this problem, Patient Optimization Pharmacogenomics (POPGx) was constructed. The objective of this paper is to describe the development of POPGx, a tool to simplify the approach for healthcare providers to determine pharmacogenomic dosing recommendations for patients taking multiple medications. Additionally, this tool educates patients on how their allele variations may impact gene function in case they need further healthcare consultations. POPGx was created on Konstanz Information Miner (KNIME). KNIME is a modular environment that allows users to conduct code-free data analysis. The POPGx workflow can access Clinical Pharmacogenomics Implementation Consortium (CPIC) guidelines and subsequently be able to present relevant dosing and counseling information. A KNIME representational state transfer (REST) application program interface (API) node was established to retrieve information from CPIC and drugs that are exclusively metabolized through CYP450, and these drugs were processed simultaneously to demonstrate competency of the workflow. The POPGx program provides a time-efficient method for users to retrieve relevant, patient-specific medication selection and dosing recommendations. Users input metabolizer gene, genetic allele data, and medication list to retrieve clear dosing information. The program is automated to display current guideline recommendations from CPIC. The integration of this program into healthcare systems has the potential to revolutionize patient care by giving healthcare practitioners an easy way to prescribe medications with greater efficacy and safety by utilizing the latest advancements in the field of pharmacogenomics. Copyright: © 2024 Behdani et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.',\n",
       " \"Based on experiences in data-based projects, it was hypothesized that traditional project approaches often fail to ensure consistency, traceability, and transparency, contributing to a low success rate of such projects. This hypothesis was tested by compiling documented challenges from various data-based projects and comparing methods from literature and practice. The comparison enabled the formulation of objectives and led to the development of a novel method, focusing on standardization, regular exchange, and accountability to enhance consistency, traceability, and transparency of project-relevant objects. It also accommodates existing procedures for handling data-based projects. Application of this method allows for meticulous planning on multiple levels and iterative progress. Findings support the initial hypothesis, suggesting the method's potential to improve success rates in data-based projects. © 2024, The Authors.\",\n",
       " 'Objective The unprecedented events of 2020 required a pivot in scientific training to better prepare the biomedical research workforce to address global pandemics, structural racism, and social inequities that devastate human health individually and erode it collectively. Furthermore, this pivot had to be accomplished in the virtual environment given the nation-wide lockdown. Methods These needs and context led to leveraging of the San Francisco Building Infrastructure Leading to Diversity (SF BUILD) theories of change to innovate a Virtual BUILD Research Collaboratory (VBRC). The purpose of VBRC was to train Black, Indigenous, and people of color (BIPOC) students to apply their unique perspectives to biomedical research. These training activities were evaluated using a pre-post survey design that included both validated and new psychosocial scales. A new scale was piloted to measure culturally relevant pedagogy. Results VBRC scholars increased science identity on two items: thinking of myself as a scientist (+1point, p = 0.006) and belonging to a community of scientists (+1point, p = 0.069). Overall, scholars perceived stress also decreased over VBRC (-2.35 points, p = 0.02). Post VBRC, scholars had high agency scores (μ = 11.02, Md = 12, range = 6–12, σ = 1.62) and cultural humility scores (μ = 22.11, Md = 23, range = 12–24, σ = 2.71). No notable race/ethnic differences were found in any measures. Conclusions Taken together, our innovative approach to data science training for BIPOC in unprecedented times shows promise for better preparing the workforce critically needed to address the fundamental gaps in knowledge at the intersection of public health, structural racism, and biomedical sciences. © 2024 Ceberio et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.',\n",
       " 'The growing demand for data scientists in both the global and Dutch labour markets has led to an increase in data science and artificial intelligence (AI) master programs offered by universities. However, there is still a lack of clarity regarding the specific skills of data scientists. This study addresses this issue by employing Correlated Topic Modeling (CTM) to analyse the content of 41 master programs offered by 11 Dutch universities and an interuniversity combined program. We assess the differences and similarities in the core skills taught by these programs, determine the subject-specific and general nature of the skills, and provide a comparison between the different types of universities offering these programs. Our analysis reveals that data processing, statistics, research, and ethics are the core competencies in Dutch data science and AI master programs. General universities tend to focus on research skills, while technical universities lean more towards IT and electronics skills. Broad-focussed data science and AI programs generally concentrate on data processing, information technology, electronics, and research, while subject-specific programs give priority to statistics and ethics. This research enhances the understanding of the diverse skills of Dutch data science graduates, providing valuable insights for employers, academic institutions, and prospective students. © 2024 Mol et al.',\n",
       " 'Dementia with Lewy bodies (DLB) is a significant public health issue. It is the second most common neurodegenerative dementia and presents with severe neuropsychiatric symptoms. Genomic and transcriptomic analyses have provided some insight into disease pathology. Variants within SNCA, GBA, APOE, SNCB, and MAPT have been shown to be associated with DLB in repeated genomic studies. Transcriptomic analysis, conducted predominantly on candidate genes, has identified signatures of synuclein aggregation, protein degradation, amyloid deposition, neuroinflammation, mitochondrial dysfunction, and the upregulation of heat-shock proteins in DLB. Yet, the understanding of DLB molecular pathology is incomplete. This precipitates the current clinical position whereby there are no available disease-modifying treatments or blood-based diagnostic biomarkers. Data science methods have the potential to improve disease understanding, optimising therapeutic intervention and drug development, to reduce disease burden. Genomic prediction will facilitate the early identification of cases and the timely application of future disease-modifying treatments. Transcript-level analyses across the entire transcriptome and machine learning analysis of multi-omic data will uncover novel signatures that may provide clues to DLB pathology and improve drug development. This review will discuss the current genomic and transcriptomic understanding of DLB, highlight gaps in the literature, and describe data science methods that may advance the field. © 2024 by the authors.',\n",
       " 'Nature Reviews Psychology is interviewing individuals with doctoral degrees in psychology who pursued non-academic careers. We spoke with Christiane Ahlheim about her journey from a postdoctoral research associate to a data scientist. © Springer Nature America, Inc. 2024.',\n",
       " 'Artificial Intelligence (AI) and data science will play a crucial role in improving environmental sustainability, but the energy requirements of these methods will have an increasingly negative effect on the environment without sustainable design and use. © Springer Nature Limited 2024.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " \"This study builds on the existing poverty literature and leverages data from disparate sources including both big data sources such as satellite images and traditional data sources such as the federal, state, and local agencies to develop a context-specific poverty prediction model using design science. We examine whether and to what extent infrastructure development as measured from the satellite images as well as spatial spillovers helps predict the poverty rate of a given census tract. We also develop and implement a Vector Autoregression (VAR) based ensemble model that combines predictions from daytime and nighttime imaging with adjacent tracts' poverty rates and other economic and demographic factors identified in prior literature. Our results show that daytime imaging and spatial network features have significant predictive value and that a combination of these features gives the best predictive power. In addition, we find that the skewness of poverty rates among adjacent census tracts, not the average, is a significant predictor showing the importance of distribution of poverty around a region. Our work has major implications for researchers using deep learning and network analysis for policy development and decision making. © 2023 Elsevier B.V.\",\n",
       " 'As a nascent field within the academy, the contours, attributes, and bounties of data science are still indeterminate and contested. We studied how participants in an initiative to establish data science at a large American research university defined data science and articulated their relationships to the field. We discuss two contrasting visions for data science among our research participants. One vision is a transdisciplinary view portraying data science as a phenomenon with transcendent, appropriative, and impositional qualities that sits apart from academic domains. Another view of data science—one that was far more prevalent among our research subjects—casts data science as grounded, relational, and adaptive, emerging from crosspollination of numerous academic domains. We argue that this latter formulation represents a more quotidian reality of data science and positions the field as an extradiscipline, defined as a field that exists to facilitate the exchange of knowledge, skills, tools, and methods from an indeterminate and fluctuating set of disciplinary perspectives while conserving the boundaries of those disciplines. We argue that the dueling transdisciplinary and extradisciplinary visions for data science have important implications for how the field will mature, and that the extradiscipline concept opens novel directions for studying academic knowledge production in STS, contributing additional precision to the literature on disciplinarity and its permutations. © The Author(s) 2023.',\n",
       " 'Despite the abundance of studies focused on how higher education institutions (HEIs) are implementing sustainable development (SD) in their educational programmes, there is a paucity of interdisciplinary studies exploring the role of technology, such as data science, in an SD context. Further research is thus needed to identify how SD is being deployed in higher education (HE), generating positive externalities for society and the environment. This study aims to address this research gap by exploring various ways in which data science may support university efforts towards SD. The methodology relied on a bibliometric analysis to understand and visualise the connections between data science and SD in HE, as well as reporting on selected case studies showing how data science may be deployed for creating SD impact in HE and in the community. The results from the bibliometric analysis unveil five research strands driving this field, and the case studies exemplify them. This study can be considered innovative since it follows previous research on artificial intelligence and SD. Moreover, the combination of bibliometric analysis and case studies provides an overview of trends, which may be useful to researchers and decision-makers who wish to explore the use of data science for SD in HEIs. Finally, the findings highlight how data science can be used in HEIs, combined with a framework developed to support further research into SD in HE. © 2023 ERP Environment and John Wiley & Sons Ltd.',\n",
       " 'In this article, we reconsider elements of Agre’s critical technical practice approach (Agre, 1997) for critical technical practice approach for reflexive artificial intelligence (AI) research and explore ways and expansions to make it productive for an operationalization in contemporary data science. Drawing on Jörg Niewöhner’s co-laboration approach, we show how frictions within interdisciplinary work can be made productive for reflection. We then show how software development environments can be repurposed to infrastructure reflexivities and to make co-laborative engagement with AI-related technology possible and productive. We document our own co-laborative engagement with machine learning and highlight three exemplary critical technical practices that emerged out of the co-laboration: negotiating comparabilities, shifting contextual attention and challenging similarity and difference. We finally wrap up the conceptual and empirical elements and propose Reflexive Data Science (RDS) as a methodology for co-laborative engagement and infrastructured reflexivities in contemporary AI-related research. We come back to Agre’s ways of operationalizing reflexivity and introduce the building blocks of RDS: (1) organizing encounters of social contestation, (2) infrastructuring a network of anchoring devices enabling reflection, (3) negotiating timely matters of concern and (4) designing for reflection. With our research, we aim at contributing to the methodological underpinnings of epistemological and social reflection in contemporary AI research. © The Author(s) 2022.',\n",
       " 'Over the past two decades, the community of data science, computer vision and programming has evolved rapidly and new programming techniques have replaced the computationally expensive techniques. This is achieved with the aid of smart programming languages, smart computers and intelligent minds. The neural networks are replaced by the deep neural networks which are comprised of several layers and neurons, the direct large data “classification” has been replaced by the transfer learning tools, which are computationally more efficient and accurate as long as the user has the clear vision of synchronizing the new problem with the pre-trained model. Artificial intelligence tools are much improved since the discovery of transfer learning tools and the programming time of several days or weeks for the deep networks has now reduced to few minutes or hours. This article presents detailed insight of transfer learning frame work with the aid of some useful programming tools. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2022.',\n",
       " 'Traditional plastic deformation theories are based on the phenomenological approach which mainly rely on the experience of the developers and are obtained by fitting the experimental data, which have severe limitations. With the increasing complexity of microstructures and mechanical properties of materials, it has become extremely difficult to develop more complex phenomenological constitutive models. Non-universal material models, which are difficult to relate to material properties and manufacturing processes, have become one of the main problems limiting plastic forming and processing, and also pose a serious challenge to the development of plasticity theory. The development of artificial intelligence and data science opened up new possibilities in materials and mechanical sciences. With the rapid development of material design and application, the traditional explicit constitutive model is not needed, the theoretical modelling approaches to plastic deformation that reflect the relationship between the microstructures and macroscopic properties of materials come into being. Among them, the new paradigm modelling theories represented by mechanistically informed neural networks, efficient multi-scale clustering analysis and model-free data-driven computational mechanics have their distinctive features and significant advantages, which are the most promising simulation methods. Through the summary of these three aspects, the development status and future trend of plastic theoretical modeling and multi-scale simulation technology development driven by artificial intelligence and data science were discussed. © 2024 Beijing Res. Inst. of Mechanical and Elec. Technology. All rights reserved.',\n",
       " 'In contemporary times, data science has made significant strides across various commercial domains, spanning business, finance, space science, healthcare, telecommunications, and the Internet of Things (IoT). The IoT emerges as a pivotal platform, orchestrating the convergence of people, processes, data, and physical objects to enhance our daily lives.. In light of these considerations, this chapter explores diverse frameworks for synchronized data processing, leveraging the strengths of various platforms. Numerous challenges impede the seamless integration of cloud computing, IoT, and data science collaboration. The integration of cloud and IoT offers a promising avenue to surmount these challenges, harnessing the wealth of data resources available in the cloud. This chapter presents a comprehensive overview of the technologies involved in merging data science with cloud-based IoT; this would expand the cloud capabilities and scope to scale for higher data storage and accessibility along with examining their advantages and confronting the associated challenges. © 2024, IGI Global. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " \"Due to HVAC systems, residential and commercial building energy consumption has constantly increased. Energy loads, transportation, storage, and user behavior affect building energy use. Technology can now precisely monitor, gather, and store this process's massive data. This technology can meaningfully analyse and use such data. Data science to improve energy efficiency is a hot topic right now. This article discusses how energy management practitioners, particularly in the construction industry, have used data science to solve their most complex challenges. The paper also talks about the pros and cons of fully connected devices and new computer technologies. © 2024 Author(s).\",\n",
       " 'The epidemic of the coronavirus, often known as SARS, has been determined to be the most lethal biological outbreak ever recorded. The genesis is still a mystery to the globe since it is not quite apparent where it came from in the first place. The World Health Organization (WHO) has declared it a worldwide pandemic because of the widespread and rapid spread of the virus, which has resulted in a significant number of deaths throughout the world. However, in order to slow the spread of the virus, various preventative steps were taken. This article has an emphasis on the uses of data science in terms of management, security, and future planning, all of which contributed significantly to the provision of essential services throughout the pandemic period. © 2024 Author(s).',\n",
       " \"Open science is a global movement happening across all research fields. Enabled by technology and the open web, it builds on years of efforts by individuals, grassroots organizations, institutions, and agencies. The goal is to share knowledge and broaden participation in science, from early ideation to making research outputs openly accessible to all (open access). With an emphasis on transparency and collaboration, the open science movement dovetails with efforts to increase diversity, equity, inclusion, and belonging in science and society. The US Biden-Harris Administration and many other US government agencies have declared 2023 the Year of Open Science, providing a great opportunity to boost participation in open science for the oceans. For researchers day-to-day, open science is a critical piece of modern analytical workflows with increasing amounts of data. Therefore, we focus this article on open data science-the tooling and people enabling reproducible, transparent, inclusive practices for data-intensive research-and its intersection with the marine sciences. We discuss the state of various dimensions of open science and argue that technical advancements have outpaced our field's culture change to incorporate them. Increasing inclusivity and technical skill building are interlinked and must be prioritized within the marine science community to find collaborative solutions for responding to climate change and other threats to marine biodiversity and society. © 2024 Annual Reviews Inc.. All rights reserved.\",\n",
       " 'Interactions between catalysts and substrates can be highly complex and dynamic, often complicating the development of models to either predict or understand such processes. A dirhodium(II)-catalyzed C-H insertion of donor/donor carbenes into 2-alkoxybenzophenone substrates to form benzodihydrofurans was selected as a model system to explore nonlinear methods to achieve a mechanistic understanding. We found that the application of traditional methods of multivariate linear regression (MLR) correlating DFT-derived descriptors of catalysts and substrates leads to poorly performing models. This inspired the introduction of nonlinear descriptor relationships into modeling by applying the sure independence screening and sparsifying operator (SISSO) algorithm. Based on SISSO-generated descriptors, a high-performing MLR model was identified that predicts external validation points well. Mechanistic interpretation was aided by the deconstruction of feature relationships using chemical space maps, decision trees, and linear descriptors. Substrates were found to have a strong dependence on steric effects for determining their innate cyclization selectivity preferences. Catalyst reactive site features can then be matched to product features to tune or override the resultant diastereoselectivity within the substrate-dictated ranges. This case study presents a method for understanding complex interactions often encountered in catalysis by using nonlinear modeling methods and linear deconvolution by pattern recognition. © 2023 The Authors. Published by American Chemical Society',\n",
       " 'The proceedings contain 99 papers. The topics discussed include: interpretable model drift detection; a closer look at consistency regularization for semi-supervised learning; VeNoM: approximate subgraph matching with enhanced neighborhood structural information; predicting business process events in presence of anomalous IT events; breadth-first search approach for mining serial episodes with simultaneous events; robust training of temporal GNNs using nearest neighbor based hard negatives; DSDF: coordinated look-ahead strategy in multi-agent reinforcement learning with noisy agents; guiding offline RL using a safety expert; cost-sensitive trees for interpretable reinforcement learning; model-based safe reinforcement learning using variable horizon rollouts; and semantic annotation of relational schemas using a probabilistic generative model.',\n",
       " \"In the context of Massive Open Online Courses (MOOCs) for data science, the efficacy of feedback mechanisms in enhancing learning outcomes remains a critical area of investigation. This study explores the differential impacts of formative and binary feedback on learner performance within a data-driven MOOC environment. Over 4,000 participants in a four-week data science program were divided into two groups. One group received formative feedback, offering detailed insights and guidance, while the other group received binary feedback, indicating only right or wrong answers. Our analysis focused on three programming exercises of increasing complexity, assessing how each type of feedback influenced the learners' performance. Results indicate minimal differences in performance between the two feedback groups, suggesting that the type of feedback may not significantly impact learner outcomes in environments where participants are already actively engaged and the tasks are not overly challenging. The findings challenge the assumption that more detailed feed-back universally enhances learning, highlighting the importance of aligning feedback strategies with learner proficiency and task difficulty. This study contributes to the ongoing discourse in programming education by suggesting that MOOC providers might optimize resource allocation by tailoring feedback mechanisms to the specific needs of the learner population, thereby enhancing both educational efficiency and effectiveness. © 2024 IEEE.\",\n",
       " \"Dr. Preeti Bajaj first began talking about fuzzy genetic hybrid intelligence and intelligent machine design while working toward her master's degree in 1998 and her Ph.D. degree in 2001. In fact, Bajaj, whose research was in intelligent transportation, started designing smart vehicles and publishing research on vehicle health monitoring not only before the Internet of Things became commonplace but before the term was even coined in 1999. © 2007-2011 IEEE.\",\n",
       " 'This study presents a mobile application (app) that facilitates undergraduate students to learn data science using their own full-body motion data. The app captures a user’s movements through the built-in camera of a mobile device and processes the images for data generation using BlazePose, an open-source computer vision model for real-time pose estimation. Students can be entirely involved in the data collection process through the app. Consequently, the motion data is contextually rich and holds personal relevance for them. This connection allows students to establish a direct relationship between their body movements and the corresponding motion data, facilitating a deeper understanding of the data. The app then takes advantage of this motion data as a data source to demonstrate various concepts and techniques in data science. As examples of the proposed learning framework, we introduce two learning modules, one focused on principal component analysis and the other on k-means clustering. To reduce learning demands, the app also provides various visual aids, such as interactive graphs and figures, that simplify the learning by visualizing the geometric interpretation of the motion data. Strategies to encompass other data science methods are also discussed for further improvement. © Association for Educational Communications & Technology 2024.',\n",
       " 'The rise of interest in data science raises questions about what changes may be needed in how we address the teaching of statistics with primary children. In this paper, we use a broad definition of data science to outline three foundational aspects that could be addressed during the schooling years: (1) reasoning with data; (2) computational thinking; and (3) inquiry-based learning. Drawing on video data and artefacts from a primary classroom in Australia (children aged 9–10\\xa0years old), we examined how a class generated and grappled with a complex, ambiguous problem about the risks and benefits of their activities in cyberspace. The interdisciplinary lesson sequence engaged the children with non-standard data, data privacy, and ethical issues in age-appropriate ways. The analysis used an iterative process that annotated video logs, identified and transcribed critical events linked to these three foundational aspects, and coded these data to make connections within and across the data. The findings suggest a process for introducing children to age-appropriate problems that can be addressed by data science that reflect contexts in their world. Implications of the study provide teachers, researchers, and curriculum developers suggestions for embedding data science education into primary school. © Mathematics Education Research Group of Australasia, Inc. 2024.',\n",
       " 'Early detection of stuck pipes during drilling operations is crucial and challenging. Some of the existing studies on the stuck pipe detection have adopted supervised machine learning approaches that employ datasets for “stuck” and “normal”. However, for early detection before the occurrence of stuck pipe, the application of ordinal binary classification in supervised machine learning has presented several elemental concerns, such as limited stuck pipe data, and lack of an exact “stuck sign” before occurrence. Our previous studies proposed unsupervised machine learning approaches using data on the normal activities with long short-term memories and autoencoder, the mixture probability model, and graph attention model, and presented the possibility of predicting a stuck pipe. On the other hand, there observed false positives responding the operation of drilling equipment or changes in drilling data. We introduce a data science approach, including machine learning, incorporating physical knowledge to overcome frequent false positives. This study initially introduces the stuck pipe predictions using unsupervised machine learning approaches, and a typical false positive case. Subsequently, we present machine learning integrating physical knowledge and demonstrate early stuck pipe detection using field data containing stuck pipe events. The results show significant suppression of false positives. Our machine learning approach is expected to contribute to considerable reduction of nonproductive time in drilling operations, potentially preventing well abandonment. Copyright © 2024 by ASME.',\n",
       " 'The authors aimed to forecast the cumulative COVID-19 confirmed cases and deaths by the most appropriate model of the top five impacted countries and three South Asian countries incorporating the climate factors as covariates.\\xa0Different statistical and data science methods are used in this study. The results of the model selection criteria depict that the ELM algorithm is adequate for France, Germany, and Spain, while the MLP algorithm tends to have a better forecast for India and Pakistan. In Sri Lanka, Italy, and the United States, the well-known ARIMAX model tends to be a good match for death predictions. The findings showed that the inclusion of the meteorological variables improves the accuracy of modeling both COVID-19 cases and deaths, for all the chosen countries’ cumulative confirmed cases except Italy and Sri Lanka. However, in the case of modeling deaths, it is observed that the inclusion of meteorological variables was not able to enhance the forecasting accuracy of the model in each of the selected countries. Though no single model was found to be suitable for all countries, the authors identify the most appropriate models for each country for forecasting and make the sixty-day-ahead forecast for each country. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.',\n",
       " \"Time series refers to a range of observations taken at a specific time period. For instance, a set of logins recorded for a regular period of each user. Similarly, time series data refers to a set of values arranged in the form of stock prices, sensor data, application telemetry, and click stream data. Anomaly is an unusual activity which is different from normal behavior. There are different methods adopted to find anomalies and trends in time series data in data science. Irrespective of technological advances in anomaly detection and system monitoring, there is still the risk of false positives. There are several anomaly detection models - some are generic while some are domain-specific. However, there is still a lack of a sufficient algorithm that can work with various datasets. Hence, this study will discuss various types of anomalies and components of time series data. This study will also propose a framework based on machine learning methods for detecting anomalies and selecting appropriate models for time series data analysis. This study will also present a taxonomy to define several aspects of 'anomaly detection in time-series data. Performing anomaly detection regarding the context or types of activities a system is exposed to is one of the major challenges in existing techniques for anomaly detection. This study will help researchers to understand the emerging approaches of 'time series anomaly detection' and computational methods. © 2024 IEEE.\",\n",
       " 'Background: Social determinants of health (SDOH) are critical drivers of health disparities and patient outcomes. However, accessing and collecting patient-level SDOH data can be operationally challenging in the emergency department (ED) clinical setting, requiring innovative approaches. Objective: This scoping review examines the potential of AI and data science for modeling, extraction, and incorporation of SDOH data specifically within EDs, further identifying areas for advancement and investigation. Methods: We conducted a standardized search for studies published between 2015 and 2022, across Medline (Ovid), Embase (Ovid), CINAHL, Web of Science, and ERIC databases. We focused on identifying studies using AI or data science related to SDOH within emergency care contexts or conditions. Two specialized reviewers in emergency medicine (EM) and clinical informatics independently assessed each article, resolving discrepancies through iterative reviews and discussion. We then extracted data covering study details, methodologies, patient demographics, care settings, and principal outcomes. Results: Of the 1047 studies screened, 26 met the inclusion criteria. Notably, 9 out of 26 (35%) studies were solely concentrated on ED patients. Conditions studied spanned broad EM complaints and included sepsis, acute myocardial infarction, and asthma. The majority of studies (n=16) explored multiple SDOH domains, with homelessness/housing insecurity and neighborhood/built environment predominating. Machine learning (ML) techniques were used in 23 of 26 studies, with natural language processing (NLP) being the most commonly used approach (n=11). Rule-based NLP (n=5), deep learning (n=2), and pattern matching (n=4) were the most commonly used NLP techniques. NLP models in the reviewed studies displayed significant predictive performance with outcomes, with F1-scores ranging between 0.40 and 0.75 and specificities nearing 95.9%. Conclusions: Although in its infancy, the convergence of AI and data science techniques, especially ML and NLP, with SDOH in EM offers transformative possibilities for better usage and integration of social data into clinical care and research. With a significant focus on the ED and notable NLP model performance, there is an imperative to standardize SDOH data collection, refine algorithms for diverse patient groups, and champion interdisciplinary synergies. These efforts aim to harness SDOH data optimally, enhancing patient care and mitigating health disparities. Our research underscores the vital need for continued investigation in this domain. © Ethan E Abbott, Donald Apakama, Lynne D Richardson, Lili Chan, Girish N Nadkarni.',\n",
       " 'The proceedings contain 97 papers. The topics discussed include: an integrated model for predicting student achievement efficiency using data envelopment analysis and genetic programming approach; an unsupervised machine learning approach for heart disease prediction; explainable convolutional neural network for corn disease classification; enhancing mental health detection in Malay social media: a comparative study of RNN architectures and attention mechanisms; a review on deep learning and hybrid model for forecasting residential and commercial buildings energy consumption; token-based path allocation for high throughput and scalability in indoor autonomous mobile robot systems; crop pest and diseases classification using ResNet and inception network; and the development of instruments to measure students’ behavioral intention towards adopting artificial intelligence (AI) technologies in educational settings.',\n",
       " \"Tumors are abnormal growths of tissue that can either be malignant or benign. Malignant tumors indicate cancer, while benign tumors are generally minimally harmful. The difference between the two is not evidence at first glance, and many tests must be conducted to identify it. Tumors need to be determined at an early stage, considering that treatments are most effective during the early stages of cancer when it has still not yet infected a lot of cells. Computer Vision is a technology that enables computers to simulate the 'perceiving the real world' phenomena. With it, computers can create analyses and conclusions based on different images provided. With computer vision, a good data set is essential. Data Science makes use of different preprocessing techniques that aid in analyzing data. Applying different preprocessing methods to a data set allows the creation of high-quality data that can be used as a training data set for computer vision models. There are various computer vision models; the models that will be discussed are CapsNet, VGG16, ResNet50, and GoogLeNet. Concurrently, their accuracy and performance would be compared to each other. Furthermore, the importance of data science relative to computer vision will also be discussed. The increase in accuracy of having preprocessed high-quality data as a data set for computer vision models will also be measured. © 2024 IEEE.\",\n",
       " 'The PERN stack is a robust technology stack for modern web application development. It comprises PostgreSQL, Express.js, React, and Node.js. This project introduces the GenDatr web application, showcasing Integrant Labs’ innovative pursuits. As a Single Page Application (SPA), it seamlessly integrates new data insights and predictive models, enhancing user experience. PostgreSQL ensures robust data management, Express.js facilitates efficient backend server and API development, React enables dynamic and user-friendly interfaces, Node.js supports scalable server-side execution. By employing Agile Methodology, we ensured iterative progress and adaptability throughout the project’s phases. Together, these components ensure the GenDatr application is robust, efficient, and scalable, reflecting GenDatr’s commitment to advancing research and development through an immersive, interactive platform. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'Currently, data security is one of the most concerning research topics. The traditional RSA encryption system has become vulnerable to quantum algorithms such as Grover and Shor, leading to the development of new security systems for the quantum. As a result, quantum cryptography is gaining importance as a key element of future communication security. This study focuses on quantum key distribution protocols for data quantum encryption, aiming to achieve quantum robustness in all stages of quantum cryptography communication processes. Quantum cryptography communication requires robust quantum encryption not only between end-nodes but also between all components. Therefore, this study demonstrates the process of end-to-end data quantum encryption and proves the overall quantum robustness in this process. © 2024 River Publishers.',\n",
       " 'This study analyzed data from the QS World University Ranking website, compiling a comprehensive dataset of 4220 records and 25 attributes. The data was used to examine the spread of universities across continents, their sizes, research intensity, and status. European universities had the largest representation with 1449 ranked institutions. The study also projected the future number of universities in each continent for the next five years using linear regression and exponential smoothing models. Significant correlations were discovered between academic reputation, employer reputation, citations per faculty, and overall scores. Machine learning regression algorithms were used to predict university scores based on input parameters. Random forest and gradient boosting were found superior in terms of precision and explanatory power. The study presents insights into trends in global higher education and offers tools to anticipate future changes in academia. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'Theory-guided data science (TGDS) uses problem-specific domain knowledge to ensure physical consistency and enhance the performance of data science models. However, despite the increasing popularity of TGDS, there is a lack of studies comparing TGDS methods on concrete data science tasks, and their deployment remains difficult. To address this gap, we present a comparative study of existing TGDS paradigms applied to time-series prediction, in the context of dynamical systems modeling. Our concrete use case is lithium-ion battery modeling. Applying TG DS paradigms in this context is not trivial, and we describe how to organize existing domain knowledge and to implement the TGDS paradigms. We uniquely introduce and evaluate essential criteria for TGDS methods (domain knowledge and intellectual effort), often missing in use-case-specific publications. Our study findings reveal that while effective TGDS methods require substantial domain knowledge, their implementation does not necessarily require significant effort. © 2024 IEEE.',\n",
       " 'The proceedings contain 71 papers. The topics discussed include: federated feature distillation CLIP for photovoltaic panels defect detection; accurate estimation of cross-excitation in multivariate Hawkes process models of infectious diseases; deep contrastive active learning for out-of-domain filtering in dialog systems; using annealing to accelerate triangle inequality k-means; adversarial robustness in graph neural networks: recent advances and new frontier; a metaverse platform for air pollution analysis in supporting smart and sustainable city development; validating arbitrary shaped clusters - a survey; MicroPPO: safe power flow management in decentralized micro-grids with proximal policy optimization; and diffusion models for cross-domain image-to-image translation with paired and partially paired datasets.',\n",
       " 'The emergence of social media platforms has altered patterns of interaction between individuals and decision-makers. To explore the impact of such changes, this study conducts an opinion mining of public reactions in Twitter to the 2020 International Booker Prize shortlist. Over 13,000 tweets were collected and analysed to examine whether public’s emotions and responses to a list of nominees are akin to or influence the judges’ decisions about the winning novelist. A data science lifecycle for sentiment analysis and topic modelling is proposed to classify tweet sentiments and identify the dominant topics in relation to the six shortlisted literary works both before and after the announcement of the winner. The findings show a marked discrepancy between readers’ preference and the judges’ decision as the prize was granted to one of the least heeded nominees. This difference reinforces the suspicion that the literary prizes are filtered through judges’ personal views. The proposed digital model in this study can assist critics, book club judges, literary prize-givers, and publishing industries in better decision making. © 2024 Inderscience Enterprises Ltd.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'Data science is a rapidly growing technology in the technical world that fulfills the requirements for data and various data aspects. The core of all emerging technologies is data science, which includes machine learning, artificial intelligence, robotics, edge computing, and blockchain technology. In this review paper, we consider the detailed concept on data science, such as where the data is generated, the skills to handle data, its growth, how it works, and the impact of data science on other technologies. The basic aim of this review paper is to provide a basic summary of data science that everyone easily understands. © Grenze Scientific Society, 2024.',\n",
       " 'Logica (= Logic + aggregation) is a freely available, open-source, feature-enhanced version of Datalog that automatically compiles logic rules to a number of popular SQL platforms (DuckDB, SQLite, PostgreSQL, and BigQuery). Logica combines beginner-friendly declarative features of Datalog with advanced analytical features needed by data science and ML practitioners when processing real-world data. Since Logica is built on top of mature SQL implementations, these features can be executed robustly and scalably. Logica allows beginners to seamlessly progress from simple textbook examples to intermediate and advanced use cases. We introduce Logica with examples that combine aggregation, recursion, and negation in interesting and powerful ways. Additional advanced examples (maximum flow, matrix inversion, etc.) are demonstrated in an online notebook. Logica source programs are compiled into (a) self-contained SQL scripts (for non-recursive and shallow-recursive problems) or (b) Python-driven iterations of SQL queries (when deep recursion is needed). Logica’s practical and theoretical expressive power thus extends both SQL and (pure) Datalog. The Logica system has been used for data science applications and training in industry, and in graduate-level courses in academia. © 2024 Copyright for this paper by its authors.',\n",
       " \"The widespread use of data mining is a direct result of the practice's first success in more public arenas like marketing, e-commerce, and retail. Discoveries in healthcare are among them. Data is abundant in healthcare systems. In spite of this, there is a lack of efficient analysis tools for uncovering latent connections in data. In particular, this study aims to describe the Nave Bayes and decision tree classifiers used in our research to predict cardiac events. It has been shown, through experimentation on the same dataset, that Decision Tree is more effective than Bayesian classification when it comes to predictive data mining. © 2024 IEEE.\",\n",
       " 'While imperative programming is prevalent in software engineering and education, the declarative nature of logic programming can play a vital role in helping students further develop problem-solving and conceptual-modeling skills. Logica, an open-source logic programming language, extends Datalog by incorporating support for numerical computations, including aggregation. It serves as a comprehensive execution environment, compiling programs into iterative SQL queries that can be executed locally via DuckDB or SQLite, or in the cloud through PostgreSQL and Google BigQuery. Logica can also be accessed from within Python and Jupyter Notebooks, allowing it to be used seamlessly within typical data-science tool chains. These intuitive features make Logica an accessible and practical tool for students to learn logic programming. In this paper, we propose a new course, complete with lecture materials centered around Logica, aimed at teaching students declarative data science and engineering while covering topics such as knowledge representation and reasoning, database queries, and constraint-based programming. © 2024 Copyright for this paper by its authors.',\n",
       " 'The proceedings contain 72 papers. The special focus in this conference is on Computational Intelligence in Data Science. The topics include: Chat Bot in Banking Sector Using Machine Learning and Natural Language Processing; lecter-A Large Language Model Chatbot for Cognitive Behavioral Therapy; Evaluating the Language Translation Accuracy of GPT-3.5 Using Prompt Engineering; multi-camera Enhanced Real-Time Content-Aware Vehicle Detection; COOL: Classification of Online Offensive Language Using Machine Learning and Deep Learning; improved Evaluator for Subjective Answers Using Natural Language Processing; Self-harm Detection from Texts: A Comparative Study Utilizing BERT, Machine Learning, and Deep Learning Approaches; neuro-Evolution-Based Language Model for Text Generation; offensive Language Detection on Telugu Language; User Story Based Automated Test Case Generation Using NLP; anticipating Future College Admission Cutoffs: An Innovative Predictive Model Incorporating Student Reviews and Historical Admissions Cutoff Data Using Machine Learning; sentiment Analysis for Stock Prediction Using Mass Media Sources; CADFRA: Coronary Artery Disease Feature Reduction with Autoencoder for Optimistic and Effective Classification; machine Learning Based Alzheimer’s Disease Detection: A Comprehensive Approach; supervised Learning of Procedures from Tutorial Videos; De-noising of Low Dose CT Liver Images Using Improved Discrete Wavelet Transform; a-Eye Tracker: Human Eye Defect Tracker and Analyzing Software; design and Analysis of Structural Health Monitoring System for the Diagnosis of Morphological Deformities of Bolted Structures; Empowering Medical Image Analysis: Unveiling Anomalies Through GANs and BiGAN’s Models; pneumonia Detection Using Chest X-Rays: A Comprehensive Review; realistic Avatar Control Through Video-Driven Animation for Augmented Reality; CNN-Based Skin Lesion Classification for Melanoma Detection.',\n",
       " 'This chapter attempts to examine how visual analytics can strengthen the link between entrepreneurial knowledge and business education. Using visual analytics to map knowledge serves a crucial role to address the issue of dispersed knowledge across time, place, and people. Visual analytics entails knowledge type and function, target group and situation, and visualization format for knowledge creation and transfer, which provides a fundamental basis to apply six cognitive skills (interpretation, analysis, evaluation, inference, explanation, and self-regulation) for critical thinking. First, this chapter provides an overview of the important concepts: entrepreneurial knowledge and visual analytics in business education. Second, social media data on #cancelstudentdebt is examined with visual analytics to reveal entrepreneurial knowledge to identify opportunities for stakeholder engagement. Lastly, implications for academia and practitioners are discussed by highlighting the advantages of using visual analytics: integrated intervention mix; competition analysis and action; systematic planning and evaluation; insight-driven segmentation; and co-creation through social marketing. © Eleri Rosier, Adam Lindgreen, Antonia Erz, Ben Marder and Sylvia von Wallpach 2024.',\n",
       " 'Advancements in synthetic organic chemistry are closely related to understanding substrate and catalyst reactivities through detailed mechanistic studies. Traditional mechanistic investigations are labor-intensive and rely on experimental kinetic, thermodynamic, and spectroscopic data. Linear free energy relationships (LFERs), exemplified by Hammett relationships, have long facilitated reactivity prediction despite their inherent limitations when using experimental constants or incorporating comprehensive experimental data. Data-driven modeling, which integrates cheminformatics with machine learning, offers powerful tools for predicting and interpreting mechanisms and effectively handling complex reactivities through multiparameter strategies. This review explores selected examples of data-driven strategies for investigating organic reaction mechanisms. It highlights the evolution and application of computational descriptors for mechanistic inference. © 2024 The Chemical Society of Japan and Wiley-VCH GmbH.',\n",
       " 'Purpose: This paper presents the design and implementation of collaborative data science framework (CoDS), a knowledge management system for consolidating data science activities in an enterprise. Design/methodology/approach: The development of the CoDS framework is grounded on the design science research methodology for information systems research. In our case study, we first designed the initial framework for CoDS based on a systematic literature review. Then, we collected the expert opinions of eight data scientists to validate the need for generic content for such a knowledge management system. In the second iteration, a portfolio prototype is developed by the same data scientists as a part of our technical action research. Finally, a survey is conducted with 57 data analyst candidates in the last iteration. Findings: Using the CoDS portfolio strengthened the communication among data scientists and stakeholders to improve development and scaling activities. It eased the reuse or modification of existing analytical solutions in other company processes. Practical implications: The CoDS presents a platform on which business details, data-related knowledge, modeling procedures and deployment steps are shared for (1) mediating and scaling ongoing projects, (2) enriching knowledge transfer among stakeholders, (3) facilitating ideation of new products and (4) supporting the onboarding of new employees and developers. Originality/value: This study proposes a novel structure and a roadmap for creating a data science knowledge management system for the collaboration of all stakeholders in an enterprise. © 2024, Emerald Publishing Limited.',\n",
       " 'In the original article, the Fig.\\xa010 published incorrectly. The correct Fig.\\xa010 is given below. (Figure presented.) Top 20 parameters The original article has been corrected. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.',\n",
       " 'In the original article, the Table\\xa01 is published incorrectly. The correct Table\\xa01 is given below. (Table presented.) Comparison among different approaches for quality analysis based on consumer satisfaction Methodology Year Procedure Data source Attribute type Manual Automatic Questionnaire Online reviews Service Quality Product Quality SERVQUAL [12] 1988 ✓ × ✓ × ✓ × RSQS (Retail Service Quality) [13] 1996 ✓ × ✓ × ✓ × Customer reviews Investigation [14] 2004 ✓ × × ✓ ✓ × Consumer Surveys [34] 2012 ✓ × ✓ × ✓ × Sentiment analysis of customer reviews [15] 2013 × ✓ × ✓ ✓ × Key-e-Service [21] 2016 ✓ × ✓ × ✓ × Diagnosis of customer reviews [7] 2016 × ✓ × ✓ ✓ × Impact of product quality [32] 2016 ✓ × ✓ × × ✓ Consumer Review LDA [33] 2017 × ✓ × ✓ ✓ × Effects of product quality [31] 2018 ✓ × ✓ × × ✓ Measuring service quality from unstructured data [35] 2019 × ✓ × ✓ ✓ × Perceived quality from consumer’s perspective [1] 2021 × ✓ × ✓ × ✓ Purchase Decisions and Customer Satisfaction [2] 2023 × ✓ × ✓ ✓ × Proposed 2024 × ✓ × ✓ ✓ ✓ The original article has been corrected. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.',\n",
       " 'With the increasing computational power and availability of scalable distributed technologies, big data has become a critical asset for for-profit and nonprofit organizations, which leverage data science to build artificial intelligence (AI) systems. The latest developments call for human-centric, iterative, continuous, and perhaps transparent planning actions to evaluate and align AI systems considering the interconnected data-related, political, technological, and societal viewpoints beyond the organizational level with the participation of multiple for-profit and nonprofit organizations or governments. The emerging data science roadmapping (DSR), developed by customizing technology roadmapping, can enable such initiatives because it provides a human-centric and agile platform incorporating data layers as strategic planning elements considering the data science life cycle. However, the technology management literature must exemplify its adaptations beyond the organizational level in more diverse planning scenarios for data and AI professionals to plan well-coordinated operations at the industrial and governmental divisions by considering the interconnected technological, political, and sociological perspectives. Accordingly, this chapter explores the pseudo case study of the networked organizations’ strategic planning for AI by customizing DSR beyond the organizational level. We synthesize technology management literature with experiment-based best practices from the industry in a human-centric process, revealing implications for research and practice. © 2025 The Institute of Electrical and Electronics Engineers, Inc.',\n",
       " 'The COVID-19 pandemic has instigated an urgent demand for precise and effective prediction models to assist in disease management and containment. In recent years, convolutional neural networks (CNNs) have emerged as potent instruments for analyzing health data and forecasting COVID-19-related outcomes. Nevertheless, the primary objective of this research is to systematically investigate and elucidate the challenges and limitations associated with the utilization of CNNs in COVID-19 health data prediction, providing an exhaustive perspective from the realm of data science research. The study delves into diverse data-related issues, model architecture constraints, and concerns regarding generalization that may impact the efficacy of CNNs in predicting COVID-19 outcomes. By illuminating these challenges, this research endeavors to offer guidance to researchers and practitioners in making well-informed decisions while employing CNNs for predicting COVID-19 health data. © 2024 Scrivener Publishing LLC.',\n",
       " 'Sleep health, a vital component of human well-being, is often overlooked in today’s fast-paced world, leading to a surge in the prevalence of sleep disorders that affect a large global population. Sleep disorders have emerged as a pressing health concern that not only causes significant adverse impacts on patients’ health and quality of life but also places a substantial economic burden on society. The advent of the fourth industrial revolution marks the onset of a new era in sleep health, characterized by the convergence of digital technologies and unprecedented access to data related to sleep disorders. Artificial Intelligence (AI) and Data Science (DS), two pillars of this technological revolution, are poised to unleash their transformative power in the multifaceted realm of sleep disorders. The synergy of AI and DS represents a transformative opportunity to not only unravel the complex tapestry of sleep disorders but also to illuminate the path toward more precise diagnosis, personalized treatment strategies, and a deeper understanding of sleep disorders, ultimately empowering sleep health. This chapter provides an extensive review of recent advancements in the applications and methodologies of AI and DS in sleep disorders from a multitude of perspectives. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'The proceedings contain 59 papers. The topics discussed include: enhancing parking efficiency: an innovative vehicle scheduling algorithm; social media governance and fake news detection integrated with artificial intelligence governance; improving sign language recognition performance using multimodal data; wear detection for a cutting tool based on feature extraction and multivariate regression; squeeze and excitation block based neural architecture search with randomization for CNN construction; applying machine learning to language problem analysis; causal rule forest: toward interpretable and precise treatment effect estimation; FeaMod: enhancing modularity, adaptability and code reuse in embedded software development; glioma grading using machine learning techniques: model optimization and web deployment; and a data science solution to integrate weather data for energy consumption analysis.',\n",
       " 'Energy efficiency and identification of consistent energy consumption patterns are crucial for reliability of a modern power grid. In this paper, we present a data science solution that integrates weather data with historical energy consumption data for energy consumption analysis. Consequently, the solution predicts temporal energy consumption patterns via techniques like frequent pattern mining, traditional machine learning, and deep learning. Our solution integrates meteorological and environmental conditions over time series, analyzes them, forecasts energy consumption, and examines how weather conditions affect energy usage variation. Evaluation results on a real-world dataset show that our solution identifies several distinct frequent patterns with frequent pattern mining, and it reveals a significant relationship between irradiance and energy consumption, as well as a positive correlation between temperature and energy usage. Moreover, our solution predicts and compares energy consumption for a specific year using linear regression, decision tree, random forest, and gradient boosting models with daily weather data. Additionally, we applied a long short-term memory (LSTM) model to analyze energy consumption as time-series data, uncovering patterns based on given time steps. These results demonstrate the practicality of our data science solution for energy consumption analysis. © 2024 IEEE.',\n",
       " 'This paper presents a data-driven approach using machine learning to create a model for predicting the behavior of the cementation exponent \"m\" (it is a function of the shape and distribution of pores) in Mexican Naturally Fractured Reservoirs (NFRs), aiming to reduce uncertainty in reserves calculations and risks on decision-making for wells lacking core data information. The purpose is to predict \"m\" with limited measurements by machine learning models trained with core data and special logs from key wells. Different authors have addressed the ranges of \"m\" based on the type of porosity or rock texture; however, most of these studies are not from Mexican fields. Information was compiled from 1,154 data points obtained from cores in different porous systems, including matrix, fractures, unconnected vugs, and connected vugs. Probabilistic and statistical theorems were used to handle complex and nonlinear relationships between porosity and the cementation exponent \"m\". Data science revealed patterns that were not evident or were influenced by multiple factors. Finally, test examples of partitioned porosity data and several measured cementation exponent core well data have been provided to verify the effectiveness and adaptability for the determination of the cementation exponent \"m\" through the so-called SPP (Statistical Porosity Partitioning) method. This also provides algorithm support for different porous systems (matrix, fractures and vugs), achieving a MAAPE (Mean Arctangent Absolute Percentage Error) of 0.22 between the originals and the predicted values. The first example applying the SPP method is through the development of an equation to analyze a triple-porosity NFR (intergranular, vugs and fractures), successfully predicting the porosity exponent, with an accuracy of around 70% compared to the core-measured data. The second example demonstrates the success of the equation\\'s performance in a dual-porosity NFR (matrix-fractures). Next, the third example was conducted in a well with connected vugs and matrix, achieving a 100% prediction accuracy of the cementation exponent \"m\" value compared to the core-measured data. Finally, the fourth case shows an example of an exploratory well without any core analysis, and the results of applying our model achieved a better understanding of the reservoir behavior. The input features for cementation exponent identification are effective porosity and discrete porous system log, with resulting values of 1, 2, 3, and 4, depending on the type of NFR present. The SPP method was then applied to wells lacking specific core information to obtain a cementation exponent \"m\" exclusively for naturally fractured reservoirs (NFR), thus reducing uncertainty in estimation of hydrocarbon reserves. Copyright 2024, Society of Petroleum Engineers.',\n",
       " 'The proceedings contain 72 papers. The special focus in this conference is on Computational Intelligence in Data Science. The topics include: Chat Bot in Banking Sector Using Machine Learning and Natural Language Processing; lecter-A Large Language Model Chatbot for Cognitive Behavioral Therapy; Evaluating the Language Translation Accuracy of GPT-3.5 Using Prompt Engineering; multi-camera Enhanced Real-Time Content-Aware Vehicle Detection; COOL: Classification of Online Offensive Language Using Machine Learning and Deep Learning; improved Evaluator for Subjective Answers Using Natural Language Processing; Self-harm Detection from Texts: A Comparative Study Utilizing BERT, Machine Learning, and Deep Learning Approaches; neuro-Evolution-Based Language Model for Text Generation; offensive Language Detection on Telugu Language; User Story Based Automated Test Case Generation Using NLP; anticipating Future College Admission Cutoffs: An Innovative Predictive Model Incorporating Student Reviews and Historical Admissions Cutoff Data Using Machine Learning; sentiment Analysis for Stock Prediction Using Mass Media Sources; CADFRA: Coronary Artery Disease Feature Reduction with Autoencoder for Optimistic and Effective Classification; machine Learning Based Alzheimer’s Disease Detection: A Comprehensive Approach; supervised Learning of Procedures from Tutorial Videos; De-noising of Low Dose CT Liver Images Using Improved Discrete Wavelet Transform; a-Eye Tracker: Human Eye Defect Tracker and Analyzing Software; design and Analysis of Structural Health Monitoring System for the Diagnosis of Morphological Deformities of Bolted Structures; Empowering Medical Image Analysis: Unveiling Anomalies Through GANs and BiGAN’s Models; pneumonia Detection Using Chest X-Rays: A Comprehensive Review; realistic Avatar Control Through Video-Driven Animation for Augmented Reality; CNN-Based Skin Lesion Classification for Melanoma Detection.',\n",
       " 'This book presents statistics and data science methods for risk analytics in quantitative finance and insurance. Part I covers the background, financial models, and data analytical methods for market risk, credit risk, and operational risk in financial instruments, as well as models of risk premium and insolvency in insurance contracts. Part II provides an overview of machine learning (including supervised, unsupervised, and reinforcement learning), Monte Carlo simulation, and sequential analysis techniques for risk analytics. In Part III, the book offers a non-technical introduction to four key areas in financial technology: artificial intelligence, blockchain, cloud computing, and big data analytics. Key Features: • Provides a comprehensive and in-depth overview of data science methods for financial and insurance risks. • Unravels bandits, Markov decision processes, reinforcement learning, and their interconnections. • Promotes sequential surveillance and predictive analytics for abrupt changes in risk factors. • Introduces the ABCDs of FinTech: Artificial intelligence, blockchain, cloud computing, and big data analytics. • Includes supplements and exercises to facilitate deeper comprehension. © 2025 Taylor & Francis Group, LLC.',\n",
       " 'The Internet of Vehicles (IoV) is creating innovation in the business and scholarly worlds. Countless vehicles are expected to be part of the IoV by 2035. Every vehicle in the environment creates huge amounts of information. Right now, overviews of utilizing machine learning in the IoV in the setting of big data are scarce. In this chapter, we present a study and investigate the hypothetical point of view of the application of machine learning (ML) in the cognitive radio (CR)-based IoV in conjunction with data science. The chapter has shown significant research gaps across ML, IoV, and data science. Examining ML in conjunction with data science in the IoV is a new research area requiring dynamic consideration from specialists to understand this new idea completely. Applications of ML and data science in the context of the CR-IoV are presented. This chapter can guide scientists on additional advancement of the utilization of ML in the CR-IoV in conjunction with data science. © 2025 selection and editorial matter, Syed Hashim Raza Bukhari, Muhammad Maaz Rehan, and Mubashir Husain Rehmani.',\n",
       " 'The applications of data science are rapidly growing with the expansion of technology in Industry 4.0. Cognitive radio came up as a game changer in the radio and wireless communication industry. On the other end, Internet of Things (IoT) technology also combined with vehicular ad hoc network (VANET) and evolved as IoV-based systems. With further advancement in the application of cognitive radios, IoV-based systems combined with CR and formed the CR-IoV. In this chapter, we will discuss the role and application of data science in the CR-IoV. An introduction will be given in Section 6.1, Section 6.2 will cover the overview and concept of data science, data science approaches and applications in the CR-IoV will be discussed in Section 6.3, Section 6.4 will cover issues and challenges, and Section 6.5 will give the conclusion of the chapter. © 2025 selection and editorial matter, Syed Hashim Raza Bukhari, Muhammad Maaz Rehan, and Mubashir Husain Rehmani.',\n",
       " 'The proceedings contain 7 papers. The topics discussed include: miniLB: a performance portability study of Lattice-Boltzmann simulations; performance analysis on DNA alignment workload with Intel SGX multithreading; assessing large language models inference performance on a64-core RISC-V CPU with silicon-enabled vectors; distributed resource orchestration at the edge based on consensus; DF-Threads: a scalable and efficient execution paradigm for edge computing and HPC; and high-performance computation on a rust-based distributed ABM engine.',\n",
       " 'These days, age of modern e-commerce, the utilization of data science techniques has emerged as an essential means of more effectively categorizing clients into groups and simplifying the process of targeted marketing. It provides an explanation of the significance of data science in the realm of e-commerce and how it plays a crucial part in identifying intricate patterns within extremely big databases. Complex algorithms and statistical methodologies can be utilized by companies that offer products or services online in order to categorize their clients into distinct groups. They are able to gain a great deal of knowledge about the likes and dislikes of their customers as a result of this. E-commerce companies acquire the knowledge necessary to efficiently segment their consumer base into groups, which provides them with a wealth of information regarding the manner in which customers behave and the things that they prefer. Not only does it contribute to the existing body of knowledge on e-commerce, but it also demonstrates how data science can be utilized to enhance marketing strategies and the segmentation of customers. © 2024 IEEE.',\n",
       " 'Cloud platforms offer distinct advantages, but questions remain about how to ethically and efficiently manage human genomic data in the cloud. Data governance needs to be adapted to ensure transparency and security for research participants, as well as equitable and sustainable access for researchers. © Springer Nature Limited 2024.',\n",
       " \"Epilepsy care generates multiple sources of high-dimensional data, including clinical, imaging, electroencephalographic, genomic, and neuropsychological information, that are collected routinely to establish the diagnosis and guide management. Thanks to high-performance computing, sophisticated graphics processing units, and advanced analytics, we are now on the cusp of being able to use these data to significantly improve individualized care for people with epilepsy. Despite this, many clinicians, health care providers, and people with epilepsy are apprehensive about implementing Big Data and accompanying technologies such as artificial intelligence (AI). Practical, ethical, privacy, and climate issues represent real and enduring concerns that have yet to be completely resolved. Similarly, Big Data and AI-related biases have the potential to exacerbate local and global disparities. These are highly germane concerns to the field of epilepsy, given its high burden in developing nations and areas of socioeconomic deprivation. This educational paper from the International League Against Epilepsy's (ILAE) Big Data Commission aims to help clinicians caring for people with epilepsy become familiar with how Big Data is collected and processed, how they are applied to studies using AI, and outline the immense potential positive impact Big Data can have on diagnosis and management. © 2024 The Author(s). Epileptic Disorders published by Wiley Periodicals LLC on behalf of International League Against Epilepsy.\",\n",
       " 'Failed to fetch page (status 404)',\n",
       " 'Purpose: This study aims to analyze the performance of quality indices to continuously validate a predictive model focused on the control chart classification. Design/methodology/approach: The research method used analytical statistical methods to propose a classification model. The project science research concepts were integrated with the statistical process monitoring (SPM) concepts using the modeling methods applied in the data science (DS) area. For the integration development, SPM Phases I and II were associated, generating models with a structured data analysis process, creating a continuous validation approach. Findings: Validation was performed by simulation and analytical techniques applied to the Cohen’s Kappa index, supported by voluntary comparisons in the Matthews correlation coefficient (MCC) and the Youden index, generating prescriptive criteria for the classification. Kappa-based control charts performed well for m\\xa0=\\xa05 sample amounts and n\\xa0=\\xa0500 sizes when Pe is less than 0.8. The simulations also showed that Kappa control requires fewer samples than the other indices studied. Originality/value: The main contributions of this study to both theory and practitioners is summarized as follows: (1) it proposes DS and SPM integration; (2) it develops a tool for continuous predictive classification models validation; (3) it compares different indices for model quality, indicating their advantages and disadvantages; (4) it defines sampling criteria and procedure for SPM application considering the technique’s Phases I and II and (5) the validated approach serves as a basis for various analyses, enabling an objective comparison among all alternative designs. © 2024, Emerald Publishing Limited.',\n",
       " 'Data Science: A First Introduction with Python focuses on using the Python programming language in Jupyter notebooks to perform data manipulation and cleaning, create effective visualizations, and extract insights from data using classification, regression, clustering, and inference. It emphasizes workflows that are clear, reproducible, and shareable, and includes coverage of the basics of version control. Based on educational research and active learning principles, the book uses a modern approach to Python and includes accompanying autograded Jupyter worksheets for interactive, self-directed learning. The text will leave readers well-prepared for data science projects. It is designed for learners from all disciplines with minimal prior knowledge of mathematics and programming. The authors have honed the material through years of experience teaching thousands of undergraduates at the University of British Columbia. Key Features: • Includes autograded worksheets for interactive, self-directed learning. • Introduces readers to modern data analysis and workflow tools such as Jupyter notebooksand GitHub, and covers cutting-edge data analysis and manipulation Python libraries suchas pandas, scikit-learn, and altair. • Is designed for a broad audience of learners from all backgrounds and disciplines. © 2025 Tiffany Timbers, Trevor Campbell, Melissa Lee, Joel Ostblom and Lindsey Heagy.',\n",
       " 'Failed to fetch page (status 404)',\n",
       " \"This paper focuses on a leading-edge approach for maintaining industrial equipment by attaching the power of two key advancements: Internet of Things (IoT) and data science. Our main focus is to develop a system capable of predicting potential failures of industrial machinery, revolutionizing traditional maintenance practices. This paper highlights the transformative potential of predicting problems before they occur, highlighting the significant benefits such as time savings, improved operational efficiency and cost reduction. Our methodology involves installing IoT sensors on various machines within a factory. These sensors provide us real-time data and insights into the operational status of the machine. Sophisticated computer programs are further used to analyze this data and identify any anomalies that could indicate impending malfunction. Our results show that this predictive system effectively prevents premature failures and helps in extending the life of the machines. In addition to proving the system's efficacy across a range of sectors and machine kinds, this research guarantees the safety of both humans and machines. The study gives insights into using IoT and data science for machine care, even if its main focus is on using the system for industrial air compressors. With an emphasis on the real benefits for users in terms of increased machine dependability and cost-effectiveness, the ultimate goal is to assist in optimizing machine performance, reducing costs, and improving factory operations. © 2024 IEEE.\",\n",
       " 'The recent COVID-19 pandemic stimulated unprecedented linkage of datasets worldwide, and while injury is endemic rather than pandemic, there is much to be learned by the injury prevention community from the data science approaches taken to respond to the pandemic to support research into the primary, secondary and tertiary prevention of injuries. The use of routinely collected data to produce real-world evidence, as an alternative to clinical trials, has been gaining in popularity as the availability and quality of digital health platforms grow and the linkage landscape, and the analytics required to make best use of linked and unstructured data, is rapidly evolving. Capitalising on existing data sources, innovative linkage and advanced analytic approaches provides the opportunity to undertake novel injury prevention research and generate new knowledge, while avoiding data waste and additional burden to participants. We provide a tangible, but not exhaustive, list of examples showing the breadth and value of data linkage, along with the emerging capabilities of natural language processing techniques to enhance injury research. To optimise data science approaches to injury prevention, injury researchers in this area need to share methods, code, models and tools to improve consistence and efficiencies in this field. Increased collaboration between injury prevention researchers and data scientists working on population data linkage systems has much to offer this field of research. © Author(s) (or their employer(s)) 2024.',\n",
       " \"This paper advances the field of phobia detection by introducing an innovative algorithm that utilizes the synergies of Data Science, Augmented Reality/Virtual Reality (AR/VR), and Electroencephalogram (EEG) technologies. Our system fills in important gaps in existing diagnostic methods, which are brought about by the rising prevalence of phobic disorders and the need for tailored therapeutic approaches. Our suggested system's complex algorithm is based on machine learning models such as Logistic Regression, Support Vector Classifier, Random Forest Classifier, and Extreme Gradient Boosting. By processing EEG data, these models allow for precise and effective phobia detection, severity categorization, and therapeutic intervention. With robust performance and accuracy rates of 96.44% for Support Vector Classifier, 98.24% for Random Forest Classifier, 98.59% for Extreme Gradient Boosting, and 96.48% for Logistic Regression, our algorithm has been trained on a variety of datasets. This demonstrates how well the algorithm can identify subtle patterns linked to phobic reactions. © 2024 IEEE.\",\n",
       " 'Single center studies are limited by bias, lack of generalizability and variability, and inability to study rare conditions. Multicenter observational research could address many of those concerns, especially in hand surgery where multicenter research is currently quite limited; however, there are numerous barriers including regulatory issues, lack of common terminology, and variable data set structures. The Observational Health Data Sciences and Informatics (OHDSI) program aims to surmount these limitations by enabling large-scale, collaborative research across multiple institutions. The OHDSI uses the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) to standardize health care data into a common language, enabling consistent and reliable analysis. The OMOP CDM has been transformative in converting multiple databases into a standardized code with a single vocabulary, allowing for coherent analysis across multiple data sets. Building upon the OMOP CDM, OHDSI provides an extensive suite of open-source tools for all research stages, from data extraction to statistical modeling. By keeping sensitive data local and only sharing summary statistics, OHDSI ensures compliance with privacy regulations while allowing for large-scale analyses. For hand surgery, OHDSI can enhance research depth, understanding of outcomes, risk factors, complications, and device performance, ultimately leading to better patient care. © 2024 American Society for Surgery of the Hand',\n",
       " 'In section 4.5 Recommendation 5: Use Technology to Explore Concepts and Analyze Data, the last paragraph and Figure 6 should be updated to the following text. (Figure presented.) Of the instructors who have students use software to analyze data, GUI-Based software is being used more than any other tool. A similar number of instructors indicated using Excel as those that indicated using Syntax-Driven software. (For those who indicated Other, the most commonly mentioned software used in courses but not listed were the Rossman Chance applets and Google Sheets.) The online version of the article has been corrected. © 2024 The Author(s). Published with license by Taylor & Francis Group, LLC.',\n",
       " 'The proceedings contain 292 papers. The topics discussed include: enhanced detection of autism spectrum disorder using ResNet: a deep learning approach; research on the application of enhanced wild horse optimization algorithm with generative adversarial networks in mechanical design and manufacturing; development of cloud monitoring platform software based on ga-bp hybrid algorithm; spam email detection using HAN-LSTM network optimized with novel hybrid bonobo optimizer and hunger games search algorithm; advancing ophthalmic diagnostics: employing deep learning for precision detection of retinal damage in OCT images; novel learning approaches for feature representation and classification using learning approaches; and using generative adversarial networks for anomaly detection in network traffic: advancements in AI cybersecurity.',\n",
       " 'Attention deficit hyperactivity disorder (ADHD) is a prevalent neurodevelopmental disorder. A key challenge associated with this condition is achieving an early diagnosis. The current study seeks to anticipate and delineate the assessments offered by both parents and teachers concerning a child’s behavior and overall functioning with the Behavior Rating Inventory of Executive Function-2 (BRIEF-2). Mothers, fathers, and teachers of 59 children diagnosed or in the process of being assessed for ADHD participated in this study. The responses provided by 59 mothers, 59 fathers, and 57 teachers to the BRIEF-2 questionnaire were collected. The performance of various feature selection techniques, including Lasso, decision trees, random forest, extreme gradient boosting, and forward stepwise regression, was evaluated. The results indicate that Lasso stands out as the optimal method for our dataset, striking an ideal balance between accuracy and interpretability. A repeated validation analysis reveals an average positive correlation exceeding 0.5 between the inattention/hyperactivity scores reported by informants (mother, father, or teacher) and the predictions derived from Lasso. This performance is achieved using only approximately 18% of the BRIEF-2 items. These findings underscore the usefulness of variable selection techniques in accurately characterizing a patient’s condition while employing a small subset of assessment items. This efficiency is particularly valuable in time-constrained settings and contributes to improving the comprehension of ADHD. © 2024 the author(s),',\n",
       " 'Introduction to Data Science in Biostatistics: Using R, the Tidyverse Ecosystem, and APIs defines and explores the term \"data science\"and discusses the many professional skills and competencies affiliated with the industry. With data science being a leading indicator of interest in STEM fields, the text also investigates this ongoing growth of demand in these spaces, with the goal of providing readers who are entering the professional world with foundational knowledge of required skills, job trends, and salary expectations. The text provides a historical overview of computing and the field\\'s progression to R as it exists today, including the multitude of packages and functions associated with both Base R and the tidyverse ecosystem. Readers will learn how to use R to work with real data, as well as how to communicate results to external stakeholders. A distinguishing feature of this text is its emphasis on the emerging use of APIs to obtain data. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'Within the global economy, the built environment makes a major contribution to economic activity and has a significant impact on sustainability. In addition, the built environment plays a significant social role as it influences where we live along with how we work, entertain, and learn. Even though the built environment plays such a significant social and economic role, it has been slow in adopting emerging technologies to make the way we design, build, utilize, and decommission more efficient while having less of a negative impact on the environment. This paper explores how we can integrate data science and Cyber Physical Systems into the built environment to achieve greater efficiencies. This paper introduces some of the industry challenges along with an overview of emerging technologies. In addition, the paper includes qualitative interview data that is driving the development of a working model that integrates emerging technologies to achieve budget, schedule, and sustainability efficiencies. Finally, the paper discusses how it is envisaged that integrating these technologies will improve decision-making within the built environment lifecycle. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.',\n",
       " '[No abstract available]',\n",
       " 'This paper helps to explore the intricate task of natural and precise image grouping for effective organization and retrieval. High precision in image classification is challenging due to the complexity of images and the vast array of defining features. Deep learning-based artificial intelligence, a rapidly progressing field, plays a crucial role in various industries such as image classification, computer vision, text mining, voice recognition, and medical scan analysis. Deep convolutional neural networks (CNNs) excel in advanced image categorization and processing, particularly for high-resolution images. This study introduces a deep quantum neural network (QNN) technique for binary image categorization, delving into the latest research on image classification using cross-entropy functions, deep learning, and convolutional neural networks. © 2024 IEEE.',\n",
       " 'Background: Data science skills are highly relevant for clinicians working in an era of big data in healthcare. However, these skills are not routinely taught, representing a growing unmet educational need. This education report presents a structured short course that was run to teach clinicians data science and the lessons learnt. Methods: A 1-day introductory course was conducted within a tertiary hospital in London. It consisted of lectures followed by facilitated pair programming exercises in R, an object-oriented programming language. Feedback was collated and participant responses were graded using a Likert scale. Results: The course was attended by 20 participants. The majority of participants (69%) were in higher speciality cardiology training. While more than half of the participants (56%) received prior training in statistics either through formal taught programmes (e.g., a Master’s degree) or online courses, the participants reported several barriers to expanding their skills in data science due to limited programming skills, lack of dedicated time, training opportunities and awareness. After the short course, there was a significant increase in participants’ self-rated confidence in using R for data analysis (mean response; before the course: 1.69 ± 1.0, after the course: 3.2 ± 0.9, p =.0005) and awareness of the capabilities of R (mean response; before the course: 2.1 ± 0.9, after the course: 3.6 ± 0.7, p =.0001, on a 5-point Likert scale). Conclusion: This proof-of-concept study demonstrates that a structured short course can effectively introduce data science skills to clinicians and supports future educational initiatives to integrate data science teaching into medical education. © The Author(s) 2024.',\n",
       " 'A survey is presented of a series of projects designed and implemented over the past 20 years by operations research and data sciences teams based at the University of Buenos Aires and the University of Chile. The projects were undertaken at the request of entities in both the public and private sectors and addressed problems in logistics and transport, production planning, efficient allocation of human resources, shift scheduling, tender auction management, public health and education, among other areas. In each case, a summary is given of the mathematical techniques used, the models and algorithms developed, and the results and impacts obtained. The principal difficulties encountered and the factors behind the projects’ success are also discussed. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.',\n",
       " 'This paper discusses the expansion and proliferation of programming languages that have been developed for data science. The paper gives background information on the technological development that led to the growth and expansion of data science languages. It then explains the common features among these languages that distinguish them from general-purpose programming languages. The paper is intended for professionals experienced in working with general-purpose programming languages and may want to enhance their working knowledge of Data Science Programming Languages. The paper further explains the main features that Data Science programming language offers, which may not be readily available in earlier programming languages . © 2024 International Association for Computer Information Systems. All rights reserved.',\n",
       " 'There is wide agreement that ethical considerations are a valuable aspect of a data science curriculum, and to that end, many data science programs offer courses in data science ethics. There are not always, however, explicit connections between data science ethics and the centuries-old work on ethics within the discipline of philosophy. Here, we present a framework for bringing together key data science practices with ethics topics. The ethics topics were collated from 16 data science ethics courses with public-facing syllabi and reading lists. We encourage individuals who are teaching data science ethics to engage with the philosophical literature and its connection to current data science practices, which are rife with potentially morally charged decision points. © 2024 The Author(s). Published with license by Taylor & Francis Group, LLC.',\n",
       " 'In the ever changing environment of education, the skills learned and applied in computer and data science (CADS) have expanded beyond the reach of siloed, single-subject teaching in the classroom. The need for interdisciplinary learning and constructs have increased as real-world experiences have become a necessity and time has become a scarcity. The progression toward this reality has been accelerated in a post-pandemic world. This paper reviews literature regarding this topic, looks at barriers impacting student learning, and provides resources to assist educators in weaving plugged, unplugged, and hybrid components into their interdisciplinary lessons to amplify the impact of CADS. It also provides a scenario for and a vision of learning if educators can utilize an interdisciplinary approach to their lesson and unit plans. The sample template provided focuses on a middle and high school education model where multiple teachers are responsible for teaching the same students in a team. The curriculum discussed in the template is based on topics covered in the New Jersey State Standards for eighth grade students, but is flexible enough to be molded for the needs of various grade levels and different state and local standards. By using multimodal techniques and generating greater synergy through interdisciplinary lesson constructs, more robust learning environments are created for both students and educators. © 2024 IEEE.',\n",
       " 'The research aims to optimize agriculture using data science techniques. Agriculture is a critical sector for sustaining life on earth, and optimizing it can enhance food security and increase the profitability of farmers. Data science techniques such as machine learning, data analysis, and modeling to analyze agricultural data and derive insights are the key areas in this work. The project will focus on enhancing crop productivity, reducing wastage, and improving resource utilization. The findings of the project can help policymakers, farmers, and stakeholders to make informed decisions and take appropriate actions. The project aims to support the sustainable agriculture practices and promote the efficient use of resources for the benefit of both farmers and consumers. © 2024 IEEE.',\n",
       " \"The surge in data science courses highlights an educational shift towards incorporating data skills as a fundamental component of a well-rounded academic curriculum. This growth indicates a recognition of the critical role that data plays in shaping our understanding of complex issues in today's data-driven society. It also underscores the need for data science education to be accessible, representative, and tailored to a wide array of learners and professionals. The work to date to establish a data science foundational knowledge framework represents a pivotal step in formalizing and enhancing data science education. Yet, these frameworks have been largely designed for those pursuing data science careers. The field is in need of a data science framework that focuses on the essential introductory knowledge and skills for non-CS students to build a solid foundation in data science. This paper provides a description of the literature review process and experiences the research team has drawn from to develop such a framework. It also positions this framework for future research in studying effectiveness and alignment to the K-12 space. © 2024 IEEE.\",\n",
       " 'Geographic information, spatial analysis and geospatial technologies play an important role in understanding changes in planetary health and in defining the drivers contributing to different health outcomes both locally and globally. Patterns influencing health outcomes and disease in the environment are complex and require an understanding of the ecology of the disease and how these interact in space and time. Knowing where and when diseases are prevalent, who is affected and what may be driving these outcomes is important for determining how to respond. In reality, we all would like to be healthy and live in healthy places. In this book, epidemiology and public health are integrated with spatial data science to examine health issues in dynamically changing environments. This is too broad a field to be completely covered in one book, and so, it has been necessary to be selective with the topics, methods and examples used to avoid overwhelming introductory readers while at the same time providing sufficient depth for geospatial experts interested in health and for health professionals interested in integrating geospatial elements for health analysis. A variety of geographic information (some novel, some volunteered, some authoritative, some big and messy) is used with a mix of methods consisting of spatial analysis, data science and spatial statistics to better understand health risks and disease outcomes. Key Features: Makes spatial data science accessible to health Integrates epidemiology and disease ecology with spatial data science Integrates theoretical geographic information science concepts Provides practical and applied approaches for examining and exploring health and disease risks Provides spatial data science skill development ranging from map making to spatial modelling. © 2025 Justine Blanford.',\n",
       " 'This article documents the authors’ experience developing an Argentinean website in tribute to Diego Maradona (301060.exactas.uba.ar) that leverages the popularity of football in South America (and the world) to illustrate the application of data science models in sports analytics. In particular, we demonstrate their use in computing probabilities associated with various events (winning matches, advancing rounds, and becoming champions) of the FIFA World Cup Qatar 2022. Building on Dixon and Cole’s 1997 seminal model, we develop a competing Poisson model that incorporates for each participating team its attack and defense strengths as well as home-advantage effects. The calibration of the model considers match importance levels and emphasizes the recency of a team’s performance. Evaluations of the model’s results on various prediction accuracy and error metrics indicate that its performance equals or betters the traditional Poisson model and is similar to established betting sites. Our website featuring the model received over 30,000 visits from 11,000 users across 10 countries during the 2022 World Cup and garnered significant media coverage in Argentina. This successful endeavor underlines the potential of mathematics for predicting football match outcomes but also showcases its potential for countless practical applications and its ability to capture the attention and interest of a wide audience. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2024.',\n",
       " 'Data science and environmental analytics provide valuable tools for converting increasingly extensive environmental and health “big data” into knowledge, interpretable by domain experts, for the purposes of informing improved water supply quality for the benefit of public health. Specifically focussing on artificial intelligence/machine learning, we highlight recent developments in the generation of pseudocontours maps of chemical contaminants in groundwater and how objective cost-based criteria can be innovatively used to create more meaningful such maps to inform water resource management decisions. © 2024 Elsevier Inc. All rights are reserved including those for text and data mining AI training and similar technologies.',\n",
       " 'Tackle the most challenging problems in science and engineering with these cutting-edge algorithms Multi-objective optimization problems (MOPs) are those in which more than one objective needs to be optimized simultaneously. As a ubiquitous component of research and engineering projects, these problems are notoriously challenging. In recent years, evolutionary algorithms (EAs) have shown significant promise in their ability to solve MOPs, but challenges remain at the level of large-scale multi-objective optimization problems (LSMOPs), where the number of variables increases and the optimized solution is correspondingly harder to reach. Evolutionary Large-Scale Multi-Objective Optimization and Applications constitutes a systematic overview of EAs and their capacity to tackle LSMOPs. It offers an introduction to both the problem class and the algorithms before delving into some of the cutting-edge algorithms which have been specifically adapted to solving LSMOPs. Deeply engaged with specific applications and alert to the latest developments in the field, it’s a must-read for students and researchers facing these famously complex but crucial optimization problems. The book’s readers will also find: • Analysis of multi-optimization problems in fields such as machine learning, network science, vehicle routing, and more • Discussion of benchmark problems and performance indicators for LSMOPs • Presentation of a new taxonomy of algorithms in the field Evolutionary Large-Scale Multi-Objective Optimization and Applications is ideal for advanced students, researchers, and scientists and engineers facing complex optimization problems. © 2024 by The Institute of Electrical and Electronics Engineers, Inc. All rights reserved.',\n",
       " 'Improvements in Public Health Services use advanced analytical methods to address kidney-related illnesses early through extensive data research and to enhance public health outcomes. The difficulty would be an extreme dependence on data, which could distort physical condition assessment by ignore kidney patient aspect like existence and socioeconomic conditions. To overcome this problem, use the machine learning (ML) approach in the proposed method. Differential Evolution optimization with Upgraded logistic regression (DEO-ULR) utilizedfor the kidney health evaluation. A diagnosis of CKD has comprehensive health information dataset available in open source Kaggle website. The gathered data is preprocessed using Min-Max normalization and employed feature selectedutilizing Recursive Feature Elimination (RFE). The suggested method is also compared the other traditional algorithms, and this study is experiment with in the Python platform. The findings show the suggested technique achieve enhanced performance in accuracy, precision, F1-Score and recall. The study demonstrates the probable of data science in improvingpublic health for kidney health assessment through advanced analytics, resulting in more precise diagnoses, efficient treatment strategies, and earlier detection for patients. © 2024, Jacobs Verlag. All rights reserved.',\n",
       " 'The arrival of cheaper computing power and the proliferation of digital devices, wearables, and innumerable other systems armed with sensors has heralded the “big data” era. The definition of “big data” varies by domain and individual expert opinion. It is commonly accepted to be characterized by data and datasets that are extremely large, such that new techniques and computational approaches are required for processing and analysis. © 2024 Elsevier Inc. All rights reserved.',\n",
       " 'With unprecedented and growing interest in data science education, there are limited educator materials that provide meaningful opportunities for learners to practice statistical thinking, as defined by Wild and Pfannkuch, with messy data addressing real-world challenges. As a solution, Nolan and Speed advocated for bringing applications to the forefront in undergraduate statistics curriculum with the use of in-depth case studies to encourage and develop statistical thinking in the classroom. Limitations to this approach include the significant time investment required to develop a case study—namely, to select a motivating question and to create an illustrative data analysis—and the domain expertise needed. As a result, case studies based on realistic challenges, not toy examples, are scarce. To address this, we developed the Open Case Studies (opencasestudies.org) project, which offers a new statistical and data science education case study model. This educational resource provides self-contained, multimodal, peer-reviewed, and open-source guides (or case studies) from real-world examples for active experiences of complete data analyses. We developed an educator’s guide describing how to most effectively use the case studies, how to modify and adapt components of the case studies in the classroom, and how to contribute new case studies (opencasestudies.org/OCS_Guide). © 2024 The Author(s). Published with license by Taylor & Francis Group, LLC.',\n",
       " 'The NFDI4DataScience (NFDI4DS) project aims to enhance the accessibility and interoperability of research data within Data Science (DS) and Artificial Intelligence (AI) by connecting digital artifacts and ensuring they adhere to FAIR (Findable, Accessible, Interoperable, and Reusable) principles.To this end, this poster introduces the NFDI4DS Ontology, which describes resources in DS and AI and models the structure of the NFDI4DS consortium.Built upon the NFDICore ontology and mapped to the Basic Formal Ontology (BFO), this ontology serves as the foundation for the NFDI4DS knowledge graph currently under development. © 2024 Copyright for this paper by its authors.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'Consistent with the PROTECT essentials, the third element in Figure 1 of Zhang et al. (2024) should read ‘operation’ (not ‘governance’). Below is the corrected Figure 1. Further explanation of the figure is in the published paper. (Figure presented) © 2024 The Author(s).',\n",
       " '[No abstract available]',\n",
       " 'Background: Learning and teaching interdisciplinary health data science (HDS) is highly challenging, and despite the growing interest in HDS education, little is known about the learning experiences and preferences of HDS students. Objective: We conducted a systematic review to identify learning preferences and strategies in the HDS discipline. Methods: We searched 10 bibliographic databases (PubMed, ACM Digital Library, Web of Science, Cochrane Library, Wiley Online Library, ScienceDirect, SpringerLink, EBSCOhost, ERIC, and IEEE Xplore) from the date of inception until June 2023. We followed the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines and included primary studies written in English that investigated the learning preferences or strategies of students in HDS-related disciplines, such as bioinformatics, at any academic level. Risk of bias was independently assessed by 2 screeners using the Mixed Methods Appraisal Tool, and we used narrative data synthesis to present the study results. Results: After abstract screening and full-text reviewing of the 849 papers retrieved from the databases, 8 (0.9%) studies, published between 2009 and 2021, were selected for narrative synthesis. The majority of these papers (7/8, 88%) investigated learning preferences, while only 1 (12%) paper studied learning strategies in HDS courses. The systematic review revealed that most HDS learners prefer visual presentations as their primary learning input. In terms of learning process and organization, they mostly tend to follow logical, linear, and sequential steps. Moreover, they focus more on abstract information, rather than detailed and concrete information. Regarding collaboration, HDS students sometimes prefer teamwork, and sometimes they prefer to work alone. Conclusions: The studies’ quality, assessed using the Mixed Methods Appraisal Tool, ranged between 73% and 100%, indicating excellent quality overall. However, the number of studies in this area is small, and the results of all studies are based on self-reported data. Therefore, more research needs to be conducted to provide insight into HDS education. We provide some suggestions, such as using learning analytics and educational data mining methods, for conducting future research to address gaps in the literature. We also discuss implications for HDS educators, and we make recommendations for HDS course design; for example, we recommend including visual materials, such as diagrams and videos, and offering step-by-step instructions for students. © 2024 JMIR Publications Inc.. All rights reserved.',\n",
       " 'Churn prediction involves finding loss of customers over a service for a specific period of time. Prediction of churn rate gives a leeway for many companies. In telecommunication sector churn rate is important because it can be used to predict potentially customers who can retained. In this work we delve into machine learning approaches to predict churn rate. We have experimented with five different algorithms which are Logistic regression, KNN, SVM, Random Forest, XG Boost algorithm and lastly neural network model. Among the five ML approaches XG boost gives the highest accuracy of 82% while KNN give the least accuracy of 76%. To further improvise the performance, we have used a neural network model which gives a training accuracy of 92%. © 2024 IEEE.',\n",
       " \"High levels of consumer indebtedness have been a concern in recent years, as they have contributed to rising household indebtedness and put pressure on the country's financial stability. The general objective of this study was to present actions to reduce the individual's indebtedness. To achieve this objective, exploratory and explanatory research was carried out. First, there was a review of the literature to find variables and/or models that explain indebtedness, and in a second part, with a quantitative character via structural equations, a questionnaire was applied to a sample of 114 individuals from the Federal District. The variables that most influenced were financial literacy (21.1%), materialism (5.45), and risk perception (3.4%). Among the actions to improve indebtedness are to make the individual literate, explain the risks associated with short-term investments and promises of immediate gain, and the consequences of materialism, which can help minimize indebtedness. © 2024 The Authors.\",\n",
       " 'Innovative approaches are needed for managing risk and system change in healthcare. This paper presents a case study of a project that took place over two years, taking a systems approach to managing the risk of healthcare acquired infection in an acute hospital setting, supported by an Access Risk Knowledge Platform which brings together Human Factors Ergonomics, Data Science, Data Governance and AI expertise. Evidence for change including meeting notes and use of the platform were studied. The work on the project focused on first systematically building a rich picture of the current situation from a transdisciplinary perspective. This allowed for understanding risk in context and developing a better capability to support enterprise risk management and accountability. From there a linking of operational and risk data took place which led to mapping of the risk pattern in the hospital. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.',\n",
       " 'Cybersecurity plays a vital role in tackling threats and minimizing potential harm. The primary objective of the research is to investigate the impact of intervention on the energy market, specifically the unavailability of resources. This network serves as a small-scale representation of energy and reserve markets, encompassing both of the renewable recourse and lines. This is done by executing multiple situations in the network. After an attack in the system, minimizing costs is achieved through a problem solution approach. The CPLEX solver has used for solving MILP model. Furthermore, the gained optimal information is used for prediction and analysis, enabling the utilization of data science techniques, such as charts and machine learning methods, to accurately predict and identify various scenarios. The attack causes a shift in the peak and minimum of the load. Additionally, the examination of three machine learning algorithms has revealed the usage of support vector machine (SVM) in conjunction with neural networks such as RBF, a second-degree polynomial, and third-degree polynomial. According to the results, the highest performance accuracy of 86% is achieved by integrating SVM with a neural network structure. © 2024 IEEE.',\n",
       " 'The advent of the Big Data era has necessitated a transformational shift in statistical research, responding to the novel demands of data science. Despite extensive discourse within statistical communities on confronting these emerging challenges, we offer our unique perspectives, underscoring the extended responsibilities of statisticians in pre-analysis and post-analysis tasks. Moreover, we propose a new definition and classification of Big Data based on data sources: Type I Big Data, which is the result of aggregating a large number of small datasets via data sharing and curation, and Type II Big Data, which is the Real-World Data (RWD) amassed from business operations and practices. Each category necessitates distinct data preprocessing and preparation (DPP) methods, and the objectives of analysis as well as the interpretation of results can significantly diverge between these two types of Big Data. We further suggest that the statistical communities should consider adopting and rapidly incorporating new paradigms and cultures by learning from other disciplines. Particularly, beyond Breiman’s (Stat Sci 16(3):199–231, 2021) two modeling cultures, statisticians may need to pay more attention to a newly emerging third culture: the integration of algorithmic modeling with multi-scale dynamic modeling based on fundamental physics laws or mechanisms that generate the data. We draw from our experience in numerous related research projects to elucidate these novel concepts and perspectives. © This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply 2024.',\n",
       " 'Implementing artificial Intelligence (AI) and data science holds significant potential for optimizing Midstream and Downstream operations in the petroleum industry, fostering margin enhancement opportunities and overall value chain optimization. This involves streamlining processes such as production, logistics, pipeline monitoring, refinery operations, plant safety, and environmental impact reduction. Artificial intelligence involves developing intelligent systems that operate autonomously, learn from experiences, and improve performance with increasing knowledge. On the other hand, data science focuses on extracting, cleaning, and analyzing data to drive valuable insights. In the midstream stage, which pertains to crude or refined petroleum transportation, digital twin technology plays a crucial role, offering applications like pipeline monitoring and logistics optimization. Downstream is the refinery, processing and purifying of crude oil and natural gas. Companies like Shell and Chevron leverage in real-time data from sensors, cameras, and drones to create dynamic representations of operations and equipment for real-time monitoring and analysis. However, maintaining accurate and up-to-date digital twin models poses challenges. This is where AI techniques like optimization (model creation and updating), generative modelling, data analytics, predictive analytics, and decision-making come to play. For instance, according to Abn resource, Shell Lubricants introduced the AI-powered Chatbot tool, Shell LubeChat, in 2018, enhancing customer service for business-to-business lubricant clients and optimizing profits. Cognite Data Fusion, a data solution, codifies industrial knowledge into software for seamless integration with existing ecosystems, facilitating scalability from proofs of concepts to operation-driven scenarios. Maintenance systems often struggle to intelligently schedule tasks without contextualized data from various sources provided by Cognite Data Fusion. Artificial intelligence and Data science solutions are instrumental in helping midstream and downstream operators achieve optimal performance, contributing to both profitability and sustainability. Copyright © 2024, Society of Petroleum Engineers.',\n",
       " \"This paper explores the importance of workflow automation in unlocking the business value of data science in Nigeria's oil and gas industry, specifically in the midstream and downstream sectors. It provides an in-depth analysis of the current state of data science and workflow automation in the industry and presents a case study to demonstrate the benefits of implementing these technologies. The paper also proposes a framework for achieving better outcomes on data science projects and offers recommendations for leveraging workflow automation to enhance and deliver optimal value creation through data science. Copyright © 2024, Society of Petroleum Engineers.\",\n",
       " 'The proceedings contain 64 papers. The topics discussed include: artificial intelligence-powered large language transformer models for opioid abuse and social determinants of health detection for the underserved population; brain stroke prediction using visual geometry group model; stochastic simulation agent for unknown inventory demands in healthcare supply management; dynamic price prediction for revenue management system in hospitality sector; an effective prediction of events in social networks using influence score of communities; logical rule set to data acquisition and database semantics; a data-driven approach for predictive maintenance of impellers in flexible impeller pumps using prophet; and objective evaluation of sleep disturbances in older adults with cognitive impairment using a bed sensor system and self-organizing map analysis.',\n",
       " 'National Aeronautics and Space Administration (NASA) is spearheading an innovative digital engineering approach to integrate, communicate, and facilitate the research of Urban Air Mobility (UAM) operations. The UAM vision is one in which advanced technologies and new operational procedures enable practical and cost-effective air transport as an integrated mode of movement of people and goods throughout metropolitan areas. To safely support UAM operations at scale in the National Airspace System (NAS), NASA’s Air Traffic Management-Exploration (ATM-X) project has been conducting research that evolves the UAM air traffic management system towards a highly automated and operationally flexible system of the future. The complexity of UAM airspace evolution to accommodate the increasing tempo of UAM operations over time is managed through the UAM airspace research roadmap, which is a system engineering approach to the R&D of complex system-of-systems, where system’s interdependencies make it nearly impossible to define requirements for individual elements of the system in isolation. These interdependencies form a knowledge graph (node-link network) with a highly complex structure far beyond the human user’s ability to extract insights for project management’s research portfolio assessment. This study applies advanced data analytics in knowledge graph to the UAM knowledge graph to facilitate the portfolio assessment. © 2024, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.',\n",
       " \"In addition to examining the transformative effects of data science on businesses and societies, the research sought to uncover the major factors influencing the adoption of data science, the obstacles that organizations must overcome in order to use it successfully. In this study, both quantitative and qualitative methodologies were employed. A survey tool is required to gather information regarding the use, consequences, and challenges of data science, which is becoming more and more common in both industry and society. The data were introduced using descriptive statistics, and the relevant hypotheses were evaluated using the t-test and regression at the 0.05 significant level. Adoption of data science has a significant impact on retail consumer satisfaction, rejecting the null hypothesis. Rather than the type of business or the quantity of the company's budget for data science activities, data science acceptance or difficulties is mostly dictated by company size and prior expertise with data analytics. It not only serves as a record of the current situation, but it also advances knowledge by highlighting 11 positive indications and 16 bad signs related to analytical skills. Associations should attempt to adjust chiefs' attitudes to transform information into data and enhance the business. It is necessary to plan for IT investments, along with strategy and governance frameworks. Numerous empirical studies that highlight various perspectives on the advantages and difficulties of DS for business are available. © 2024, Collegium Basilea. All rights reserved.\",\n",
       " 'The authors regret the missing reference “Rieck, B. (2020). Topological Data Analysis for Machine Learning, Lecture 2, https://bastian.rieck.me/talks/ECML_PKDD_2020_Lecture_2.pdf” for figures 1, 2, 3, 4, 5, 6 of the paper. The authors would like to apologise for any inconvenience caused. © 2024 The Authors',\n",
       " 'This research paper explores the application of Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) for data dimensionality reduction through feature selection. The study focuses on a dataset containing information about the dimensions of electronic components’ footprints. The rapid growth and complexity of high-dimensional datasets present challenges in effectively utilizing the information they contain. Dimensionality reduction and feature selection play vital roles in uncovering valuable information and enhancing the efficiency of data science applications. The paper applies PSO and GA algorithms to the dataset, with specific parameters set for each optimization technique. The results demonstrate that both PSO and GA are effective in feature selection, significantly reducing data dimensionality and improving classification accuracy. PSO outperforms GA in terms of mean accuracy, variance of accuracy, and execution time, making it a more efficient and effective optimization method for this specific classification problem. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'The proceedings contain 15 papers. The special focus in this conference is on Southwest Data Science. The topics include: How Does Normalization Impact Clustering?; quantitative Stock Market Modeling Using Multivariate Geometric Random Walk; disease Similarity and Disease Clustering; composition Analysis and Identification of Ancient Glass Products; data Entropy-Based Imbalanced Learning; analysis of the Changes in Microbial Community During the Fermentation of Feng-Flavour Baijiu; enhanciGraph: Visualizing Enhancer-Gene Interactions; event-Triggered Control for\\xa0Synchronization of\\xa0Chaotic Delayed Lur’e Systems with\\xa0Stochastic Cyber-Attacks; in-Game Win Prediction Models for\\xa0Cricket; finite-Time Outer Average Synchronization Between Two Coupled Heterogeneous Complex Dynamical Networks and Its Application in Secure Communication; Classification of\\xa0In-Situ Solar Wind Data Measured by\\xa0Solar Orbiter/SWA-PAS and\\xa0HIS Using Machine Learning; node Classification with\\xa0Multi-hop Graph Convolutional Network; pyDaskShift: Automatically Convert Loop-Based Sequential Programs to\\xa0Distributed Parallel Programs.',\n",
       " 'The integration of data science into Geographic Information Systems (GIS) has facilitated the evolution of these tools into complete spatial analysis platforms. The adoption of machine learning and big data techniques has equipped these platforms with the capacity to handle larger amounts of increasingly complex data, transcending the limitations of more traditional approaches. This work traces the historical and technical evolution of data science and GIS as fields of study, highlighting the critical points of convergence between domains, and underlining the many sectors that rely on this integration. A GIS application is presented as a case study in the disaster management sector where we utilize aerial data from Troia, Portugal, to emphasize the process of insight extraction from raw data. We conclude by outlining prospects for future research in integration of these fields in general, and the developed application in particular. © 2024 IEEE.',\n",
       " 'The role of data as a valuable resource has caused significant transformations in various areas of life. Data Science (DS) aims to extract knowledge from data and thus, has gained attraction from organizations aiming to optimize existing processes and uncover previously unknown potentials. DS can be beneficially integrated into the business processes of Original Equipment Manufacturers (OEMs). Therefore, in this study, a structured literature review is conducted to assess the current state-of-the-art of DS in OEMs, especially in the automotive industry and in procurement, offering valuable insights for both researchers and practitioners in DS and OEMs. Several financial, operative, and strategic potentials of DS in the context of OEMs are identified and described. Examples are operational cost reduction, supplier selection and evaluation, forecasts of product demand, and promoted collaboration between stakeholders. Nevertheless, the literature also suggests several challenges in the execution of DS projects. It was observed that OEMs face both technological and procedural obstacles in this area, including the lack of data-driven work culture, inappropriate systems, and deficits with data collection and integration. Mitigating these challenges will be valuable in improving the success rates of DS projects. Further measures to enrich the results of this article are provided. Due to the rapidly evolving character of DS, the application possibilities and challenges might change in the future. © 2013 IEEE.',\n",
       " 'Generative Artificial Intelligence, also known as Generative AI has enhanced the capabilities of what is known to be AI to the world by its capability to generate text, image, or other forms of media through the extensive use of Large Language Models (LLMs). While the roots of LLM can be traced back to the Markovian theories proposed in 1906, Generative AI is the outcome of advancements in transformer-based deep neural networks which extend the machine learning paradigm. Fundamentally, there is a definitive need to store, manage, and use data to generate data that matches consumer needs. Generative AI-based tools today free users from the need to master programming language. This has paved the way for easy access to data generation, analytics, and subsequent dissemination of findings as per the consumer’s needs, often with or without a subscription fee. The ethical frameworks of AI are built on the four key principles, namely, beneficence, non-maleficence, autonomy, and justice. In recent years, explicability, which incorporates intelligibility and accountability, was added as the fifth crucial principle in the framework. The use of AI by organizations have also led to reputational, regulatory, and legal risks, resulting in widespread discussions on Ethical AI or the ethical use of AI. In India, the Indian Council of Medical Research (ICMR, India) has released guidelines for the Ethical Use of AI in Healthcare. Protecting data privacy at the individual (or citizen) level has been one of the crucial challenges in the healthcare sector. With the advent of Generative AI, these challenges have also experienced a multiplicative effect. In addition, the post-COVID-19 era has led to increased use of digital health technologies, fueling data privacy and security risks apart from misinformation (leading to infodemics) and bias. Such concerns often affect developing countries, especially in the healthcare sector. The present research provides a state-of-the-art review of the ethical frameworks for Generative AI in healthcare. The study also provides an overview of privacy-preserving Generative AI paradigms, enabling the policy-makers (government, private, and other not-for-profit entities) to plan, propose, and disseminate policies that preserve the privacy of the data shared at an individual level. The study will benefit researchers by developing methodologies that align with the ethical framework for Generative AI, thus aligning with the principles of using AI for Good. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'This contribution shows that Gaussian Mixture Models can be considered generalizations of self-organizing maps. More precisely, we demonstrate that the training of self-organizing maps is an approximation to the training of Gaussian Mixture Models by gradient descent. As a consequence, the scores of a trained SOM can be treated as log-likelihoods of a GMM with tied, spherical covariance and used, e.g., for outlier detection, whereas sampling from trained SOMs is not well-defined. Furthermore, we outline how SGD-trained GMMs can be generalized to diagonal and more expressive covariance matrices and how this benefits typical data science applications such as outlier detection, sampling and generative classification. Source codes are available on the author’s web site or upon request. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " \"Introduction: “Data scientists” quickly became ubiquitous, often infamously so, but they have struggled with the ambiguity of their novel role. This article studies data science's collective definition on Twitter. Methods: The analysis responds to the challenges of studying an emergent case with unclear boundaries and substance through a cultural perspective and complementary datasets ranging from 1,025 to 752,815 tweets. It brings together relations between accounts that tweeted about data science, the hashtags they used, indicating purposes, and the topics they discussed. Results: The first results reproduce familiar commercial and technical motives. Additional results reveal concerns with new practical and ethical standards as a distinctive motive for constructing data science. Discussion: The article provides a sensibility for local meaning in usually abstract datasets and a heuristic for navigating increasingly abundant datasets toward surprising insights. For data scientists, it offers a guide for positioning themselves vis-à-vis others to navigate their professional future. Copyright © 2024 Brandt.\",\n",
       " 'Failed to fetch page (status 404)',\n",
       " 'Despite the fact that most police departments in the U.S. serve jurisdictions with fewer than 10,000 residents, policing practices in small towns are understudied. This is due in part to data limitations and technological barriers that exist in the small-town context. In this article we focus on one small town police department in New England with a history of misconduct, and develop a comprehensive data science pipeline that addresses the stages from design and collection to reporting. We present the reader with specific tools in the open-source Python ecosystem for replicating this pipeline. Once these data are processed, we perform two statistical analyses in an attempt to better understand the provisions of service by the small-town police department of focus. First, we perform ecological inference to estimate the rate at which residents are placing calls for service. Second, we model wait times using a negative binomal regression model to account for overdispersion in the data. We discuss data and model limitations arising through the pipeline creation and analysis process. © 2024 American Statistical Association.',\n",
       " 'Abstract section not found',\n",
       " 'Abstract section not found',\n",
       " 'Climate change is one of the most acute global problems, the consequences of which are becoming more and more noticeable every year and are the subject of scientific debate, as well as discussions in business and society. Advanced tools for modeling and forecasting not only the physical indicators of this problem, but also the social, economic, and biological-evolutionary ones are necessary to reduce risks and gain control over the situation. This paper focuses on the social factors of climate change and their modeling and assessment since it is the involvement of society that is crucial in the implementation and enforcement of relevant initiatives, policies, and legislation. A review of methods and tools for such modeling and forecasting will allow us to analyze their evolution and identify the most effective ones in terms of accuracy and resource use. Particular attention is paid to the topic of multimodal data fusion because the problem of climate change in many aspects, including social, is multidimensional and caused by the cumulative effect of many factors. This paper emphasizes the importance of synthesizing climate and social data streams for evidence-based policymaking. The presented frameworks can be extended to broader applications in socio-ecological modeling and decision support systems. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'Students in statistics, data science, analytics, and related fields study the theory and methodology of data-related topics. Some, but not all, are exposed to experiential learning courses that cover essential parts of the life cycle of practical problem-solving. Experiential learning enables students to convert real-world issues into solvable technical questions and effectively communicate their findings to clients. We describe several experiential learning course designs in statistics, data science, and analytics curricula. We present findings from interviews with faculty from the U.S., Europe, and the Middle East and surveys of former students. We observe that courses featuring live projects and coaching by experienced faculty have a high career impact, as reported by former participants. However, such courses are labor-intensive for both instructors and students. We give estimates of the required effort to deliver courses with live projects and the perceived benefits and tradeoffs of such courses. Overall, we conclude that courses offering live-project experiences, despite being more time-consuming than traditional courses, offer significant benefits for students regarding career impact and skill development, making them worthwhile investments. Supplementary materials for this article are available online. © 2024 The Author(s). Published with license by Taylor & Francis Group, LLC.',\n",
       " 'Abstract section not found',\n",
       " \"The study explores the integration of the Phind-CodeLlama model into data science frameworks to facilitate intuitive and inclusive analysis of tabular data. By leveraging Phind-CodeLlama's comprehensive knowledge base and custom-crafted prompts, the platform optimizes data management operations and significantly improves accessibility to data analysis tools. The research investigates the transformative potential of Phind-CodeLlama in improving data-driven decision-making processes by democratizing access to data insights. Through its user-friendly CSV data manipulation capabilities, the platform is designed to empower users from various backgrounds, especially those without a domain-specific background in data science, to conduct effective data analysis, formulate insightful questions, and create compelling visualizations. The study contributes to the advancement of user-friendly data analysis platforms, fostering a more democratized approach to data-driven decision-making. © 2024 IEEE.\",\n",
       " 'Pavlyshyn et al. (2024) excluded Sergey Vasylchuk who was intended to be listed as a co-author at the time of publication. This error occurred when the article was being formatted by one member of the authorial team. All authors acknowledge missing this detail during proofing, as the content and materials of the article and the accuracy of the calculations were prioritized. Sergey Vasylchuk was a permanent member of the working group, contributing to the collection of research data, the sampling the main parameters, the introduction of the idea of correlations and tracking dependencies between publications on social networks, and changing the distribution of stake in the Cosmos blockchain. He contributed to weekly meetings with all authors for over a year after which the results were composed for publication. Sergey’s credentials and metadata have been included as a co-author in this correction. © 2024 The Author(s).',\n",
       " \"The rapid digitalization of the mobility and transport ecosystem generates an escalating volume of data as a by-product, presenting an invaluable resource for various stakeholders. This mobility and transport data can fuel data-driven services, ushering in a new era of possibilities. To facilitate the development of these digitalized mobility services, we propose a novel conceptual framework for Mobility Data Science. Our approach seamlessly merges two distinct research domains: 1) mobility and transport science, and 2) data science. Mobility Data Science serves as a connective tissue, bridging the digital layers of physical mobility and transport artefacts such as people, goods, transport means, and infrastructure with the digital layer of data-driven services. In this paper, we introduce our conceptual framework, shaped by insights from domain experts deeply immersed in the mobility and transport ecosystem. We present a practical application of our framework in guiding the implementation of a driving style detection service, demonstrating its effectiveness in translating theoretical concepts into real-world solutions. Furthermore, we validate our framework's versatility by applying it to various real-world cases from the scientific literature. Our demonstration showcases the framework's adaptability and its potential to unlock value by harnessing mobility and transport data, enabling the creation of impactful data-driven services. We believe our framework offers valuable insights for researchers and practitioners: It provides a structured approach to comprehend and leverage the potential of mobility and transport data for developing impactful data-driven services, which we refer to as digitalized mobility services. © 2013 IEEE.\",\n",
       " 'This study explores the synergy between large language models and classical statistics in contemporary data science. In the field of large language models, we find there is no one-size-fits-all model which satisfies the needs of other scientists. There are differences in the soft results which may be a limitation on their application. To analyze these differences and lack of robustness, we propose a robust methodology that integrates classical statistical experimental design principles with the these advanced models, aiming to identify statistically significant differences among their outcomes. In particular, an experimental design is presented in which the main factors, levels, treatments and interactions that influence the predictions made by different models of complex natural language processing are identified. The main aim of this research is to better understand the influence of some controlled factors that are used in com-plex natural language processing models by applying classical statistical techniques, providing a comprehensive perspective on the relative effectiveness of different zero-shot classification models. It aims to offer practitioners insights into when and where certain models may be more or less sensitive, facilitating informed decision-making in applying these advanced language models. Additionally, computational results obtained from a pilot dataset are presented. These results illustrate the entire process of the proposed methodology, highlighting the importance of considering statistical evidence when making decisions. © 2024 IEEE.',\n",
       " 'This book introduces empirical methods for machine learning with a special focus on applications in natural language processing (NLP) and data science. The authors present problems of validity, reliability, and significance and provide common solutions based on statistical methodology to solve them. The book focuses on model-based empirical methods where data annotations and model predictions are treated as training data for interpretable probabilistic models from the well-understood families of generalized additive models (GAMs) and linear mixed effects models (LMEMs). Based on the interpretable parameters of the trained GAMs or LMEMs, the book presents model-based statistical tests such as a validity test that allows for the detection of circular features that circumvent learning. Furthermore, the book discusses a reliability coefficient using variance decomposition based on random effect parameters of LMEMs. Lastly, a significance test based on the likelihood ratios of nested LMEMs trained on the performance scores of two machine learning models is shown to naturally allow the inclusion of variations in meta-parameter settings into hypothesis testing, and further facilitates a refined system comparison conditional on properties of input data. The book is self-contained with an appendix on the mathematical background of generalized additive models and linear mixed effects models as well as an accompanying webpage with the related R and Python code to replicate the presented experiments. The second edition also features a new hands-on chapter that illustrates how to use the included tools in practical applications. © The Editor(s) (if applicable) and The Author(s).',\n",
       " \"The new government faces an urgent challenge: revitalising the UK's crumbling public services without major increases in public spending. While technological change holds promise, UK digital government initiatives have failed to reach their full potential over the past twenty-five years. This article argues that the latest generation of ‘data-intensive’ technologies, including data science and AI, can succeed where past efforts have faltered. We provide a roadmap for how to harness the power of recent technologies for a more productive and equitable public sector, and pinpoint the organisational changes necessary to develop progressive, technologically enhanced public services. © 2024 The Author(s). The Political Quarterly published by John Wiley & Sons Ltd on behalf of The Political Quarterly Publishing Co. Ltd.\",\n",
       " \"The complexity and size of biological datasets have led to an exponential increase in the demand for advanced data analysis tools in biological research. Traditional statistical methods and machine learning algorithms usually fail to capture complicated patterns in these datasets. The paper fills that gap by proposing a novel deep learning-based method for mining biological data. The suggested method overcomes the constraints of existing systems by utilizing Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to extract hierarchical features and temporal correlations, respectively. Preprocessing techniques, supervised learning, transfer learning, and ensemble procedures are utilized to boost the system's accuracy (0.95), precision (0.94), recall (0.96), and F1score (0.95) when compared to other systems. The results indicate significant gains in accuracy across a variety of datasets, with the proposed system displaying computational efficiency, training in 8 hours and inference in 1 second, versus 15 hours and 5 seconds for conventional systems. The study underlines the critical need for new approaches in the field, as well as the transformational potential of the suggested system to accelerate medical, genetic, and biotechnology discoveries. © 2024 IEEE.\",\n",
       " 'In a society based on data-driven, data inclusion and data access play a significant role in societal development. A called democratization of data through open access, Open Data, must be nurtured by countries to empower their citizens, entrepreneurs, companies, industries, academics, and organizations, in general. Open Data Scoring System is an evaluation system that ranks countries in 22 categories of openness in data, divided into the 3 pillars of sustainability. In this paper, we will present the importance of Industry 4.0 and its relation to sustainability and the role of Data Science in Industry 4.0 assuming an Open Design approach. Then, an analysis is made considering the Gross Domestic Product (GDP) of the most relevant countries worldwide, the USA and China, concerning the six (6) higher ranked categories of openness data of these countries, supported by the Open Data Scoring System from 2015 to 2020. Our findings reveal that in the USA and China the main categories are seven (7), five (5), and 2 (two) categories of economic, social, and environmental sustainability, respectively. Through a correlations and co-occurrences analysis of the open data scoring worldwide reveals that the most significant categories are four (4) economic, one (1) social, and two (2) environmental. © 2024 The Author(s). Published by Elsevier B.V.',\n",
       " 'This study involves a data science and machine learning course partnered with an engineering communication course - referred to here as an authentically integrated communication model - and offers insights into such a model for engineering educators. In these partnered courses, student teams apply data science and machine learning tools to conduct data analysis and write two data science reports. Through qualitative coding and corpus analysis methods, rhetorical moves that students make in the data science report genre were identified. Twelve out of 57 total final reports were randomly chosen and coded, then six corpora of excerpts related to two codes and subcodes were created to generate keyword lists. These codes were \"results,\"\"discussion,\"as well as \"ineffective\"and \"effective\"subcodes for each main code. The total codes were then compared to one another according to students\\' enrollment in both courses. Overall, students\\' reports in the engineering communication class more often contained effective results and effective discussion excerpts. Keywords along with example sentences are provided to demonstrate greater context for the use of language in the data science report genre. © 2024 IEEE.',\n",
       " 'The recent excitement surrounding artificial intelligence (AI) in health care underscores the importance of physician engagement with new technologies. Future clinicians must develop a strong understanding of data science (DS) to further enhance patient care. However, DS remains largely absent from medical school curricula, even though it is recognized as vital by medical students and residents alike. Here, we evaluate the current DS landscape in medical education and illustrate its impact in medicine through examples in pathology classification and sepsis detection. We also explore reasons for the exclusion of DS and propose solutions to integrate it into existing medical education frameworks. © The Author(s) under exclusive licence to International Association of Medical Science Educators 2024.',\n",
       " 'The effective implementation of data science projects (DSP) relies heavily on risk management, where understanding and managing potential risks are important factors. When implementing DSP, understanding and evaluating current and future risks significantly boosts the chances of success. Current research lacks a comprehensive analysis of potential risks to prevent early failure. This paper aims to fill this gap by conducting a systematic literature review, identifying the top ten risks in DSP and mapping them to the Risk Breakdown Structure (RBS) of the Project Management Body of Knowledge (PMBOK) Guide. The literature process follows the guidelines of Webster and Watson and is documented according to the recommendations of vom Brocke et al. As a result, 248 risks were identified and categorized in the RBS to determine the sources of risk. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " \"Secure and Efficient Web Service Recommendations performance is the breadth and depth of this study. With the goal of recommending services to consumers that are highly relevant to their interests, this study primarily aims to facilitate service consumption for individual usage. In the future, when it comes to service composition, it will be possible to suggest services that connect certain points in a planned process with suitable services. In order to enhance the Web service discovery process, it is possible to conduct a thorough evaluation of Wikipedia's usefulness as a source of semantics. Service quality forecasting in a mobile computing context is complicated and requires attention. The cloud environment can easily accommodate the requested tasks. A cloud provider's implicit and explicit data may be used to find the right services for a company's processes. The relationships between services may also be deduced from the messages exchanged in the cloud. Last but not least, we present and implement a new architectural model for service selection systems that makes use of implicit feedbacks, suggests a CF-based service selection algorithm that takes into account user requirements during the ranking process, and employs the proposed Web service technique, support-based kernel, and enhanced vector-based ranking method. By taking into account the interests of all other users, the suggested model is able to solve the cold start issue for new users and new services. We proposed data science approach (WSDS) allows us to provide a tailored way for predicting the quality of service (QoS) of online services. We create a homomorphic hash-based user verification method and use the Byzantine agreement to weed out untrustworthy individuals. Next, we assess the suggested WSDS on a dataset of actual web services, and we use matrix factorization to improve prediction accuracy. The suggested strategy is much more successful than conventional procedures, according to the experimental findings. © 2024 IEEE.\",\n",
       " \"In today's professional world, a wide range of digital competencies are essential, with a focus on data science skills and knowledge of pre-processing data, developing features and building models. This expertise plays a crucial role in analyzing data sets of different sizes in various application contexts. Given the increasing demand for bringing these skills together in the work environment, which is characterized by an insufficient number of highly skilled individuals, smart solutions are being developed to strengthen these competencies. This study investigates the integration of data science competencies in the architecture of modern automation frameworks (AutoFM). In this way, it aims to encourage a discussion on the potential of these systems to facilitate the acquisition of data science skills by individuals, thus contributing to a broader discourse on workforce empowerment. For this purpose, a systematic literature search according to Webster and Watson was conducted to identify suitable automation frameworks corresponding to the data science competencies. Using the method of qualitative content analysis, the automation frameworks AutoPrep, AutoGluon-Tabular, AutoClust, and DeepEye were examined and interpreted to determine which of the functions mentioned could represent skills from the EDISON Data Science competence framework. The results show that the AutoFMs theoretically cover characteristics of the required skills of a data scientist such as data visualization in some competence groups, but cannot cover the entire competence. Some skills can therefore be performed by AutoFMs in an interaction between humans and technology, while other skills are not yet supported. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.\",\n",
       " '[No abstract available]',\n",
       " 'As big data, artificial intelligence (AI) and technology are infiltrating our lives at an exponential rate, challenges are being raised in term of the consequences, trust and bias of these advancements. This paper describes and evaluates the introduction of design thinking into the Data Science programmes. Curriculum was developed to highlight the issues of non-inclusive design, drawing on well received contemporary literature, research and using enquiry based learning to unpack the social and technical reasons for this. Students were introduced to ethical frameworks used to mitigate against design bias. Additionally, the university’s strategy for Equality, Diversity and Inclusivity (EDI) was delivered as a contextualised workshop. The student experience was evaluated via a survey, concluding that the students had developed an appreciation for inclusivity both in terms of design thinking as team contribution. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'Data sources are the raw information that is required for analysis and modeling in data sciences. They assist data scientists in making proper conclusions, proving hypotheses, and making rational decisions. It is always preferable if analytical results can be obtained from multiple reliable sources. Thus, it is essential to assess such data sources in data sciences about various critical characteristics and factors. Nevertheless, the application of all-inclusive multi-attribute decision-making methodologies for the selection of data sources for integration has not received adequate attention in the existing literature and research. Thus, this article explains a new multi-attribute decision-making method through the model of the complex hesitant fuzzy rough set, which is the complex hesitant fuzzy rough multi-attribute decision-making method. This methodology would handle the evaluated values of attributes that have uncertainty, hesitancy, and roughness altogether. Besides, this study introduces several properties of complex hesitant fuzzy rough sets and develops several aggregation operators in the framework of complex hesitant fuzzy rough set and their properties. Subsequently, a case study of data source selection in data science is explained to explain the relevance of the developed multi-attribute decision-making framework in data sciences. Finally, the comparison of the devised theory with prevailing theories is interpreted. © 2013 IEEE.',\n",
       " 'The technological shift we witness today is indicated by the emergence of a global infrastructure combining information systems with the existing physical infrastructure where this integration is increasingly enabled by artificial intelligence (AI). The safety and control of this fast-paced progress cannot be ensured without global governance, and the need for global regulation is becoming more and more visible. However, global rule-making raises additional ethical dilemmas that require the cooperation of not only politicians and legal experts, but also IT professionals and data science specialists to be solved. Understanding the ethical views and commitment of information system and AI development experts, especially those from the data science field is crucial for governmental actors in their efforts to bring AI under control. Furthermore, universities with data science programs need to be aware of this unfolding challenge. The research reported here investigated two groups of data science students - both full-time students and postgraduate students with substantial work experience - to shad light on their views and commitment to AI and data related ethical standards. The key finding indicates that while students expect governments and regulatory agencies to take charge in forming (global) AI policies, they consider themselves and their employers to be responsible for implementing and adhering to them. © 2024 Copyright for this paper by its authors.',\n",
       " 'The intent of this research is to explore the application of advanced machine learning technologies to improve personalization within the e-commerce field. As the number of online shopping platforms is growing rapidly, the ability to provide the users with personalized inferences and optimal experiences is becoming crucial. By applying artificial intelligence and machine learning technologies, such as deep learning and reinforcement learning, the present research explores the creation of an intelligent recommendation system that can understand the preferences and behavior of an individual. The process of building different machine learning models, such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), and Deep Reinforcement Learning (DRL), will be analyzed to investigate their capability to predict the future responses. Therefore, the study aims to define the criteria for evaluating their performance in terms of optimized decision-making and personalization within online shopping environments. Additional goals of the research include the investigation of the effect of feature engineering and preprocessing on their performance and the study of the implications and best strategies related to their deployment within the actual environments of e-commerce. By attaining these goals, the present study seeks to provide the reader with valuable insights and recommendations symbiotic for these types of business. © 2024 IEEE.',\n",
       " 'In Lux AI Challenge Season 2, this project uses a turn-based strategic game called Soft Actor-Critic (SAC). In a dynamic environment, SAC, a cutting-edge deep reinforcement learning algorithm, maximizes lichen growth and resource collec- tion. Building factories, cultivating lichen, and vying for resources in the 2D grid that forms the Lux universe requires strategic decision-making. A good option for handling the intricacies of the Lux game is the SAC algorithm because of its sample efficiency and stability. To provide insights into the application of sophisticated machine learning in strategic gaming scenarios, this endeavor investigates the performance of SAC, compares it with traditional methods, and makes proposed improvements. © 2024 IEEE.',\n",
       " \"Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to facilitating the prediction over tables in data science, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the establishment of a universal pretraining protocol for tables with varied structures, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a straightforward yet effective method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic table element with a module, termed TabUnit. This is subsequently followed by a Transformer encoder to refine the representation. Moreover, our model is designed to facilitate pretraining and finetuning through the utilization of free-form prompts. In order to implement the pretraining phase, we curated an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the Kaggle platform. This research primarily centers on classification and regression tasks involving tabular data, and conducts rigorous experimental testing and analyses to validate the effectiveness of our methodology. The experimental results demonstrate UniTabE's superior performance against several baseline models across a multitude of benchmark datasets. This, therefore, underscores UniTabE's potential to significantly enhance the semantic representation of tabular data, thereby marking a significant stride for tabular data analysis. © 2024 12th International Conference on Learning Representations, ICLR 2024. All rights reserved.\",\n",
       " 'In recent years, we have witnessed the growing interest from academia and industry in applying data science technologies to analyze large amounts of data. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.) are created. However, there has been no systematic attempt to holistically collect and exploit all the knowledge and experiences that are implicitly contained in those artifacts. Instead, data scientists recover information and expertise from colleagues or learn via trial and error. Hence, this paper presents a scalable platform, KGLiDS, that employs machine learning and knowledge graph technologies to abstract and capture the semantics of data science artifacts and their connections. Based on this information, KGLiDS enables various downstream applications, such as data discovery and pipeline automation. Our comprehensive evaluation covers use cases in data discovery, data cleaning, transformation, and AutoML. It shows that KGLiDS is significantly faster with a lower memory footprint than the state-of-the-art systems while achieving comparable or better accuracy. © 2024 IEEE.',\n",
       " \"Integrated factory design (IFD) requires cross-disciplinary competences obtained inter alia from areas of industrial, mechanical, civil engineering and architecture as well as operations, assets and facility management. However, traditional teaching practices in higher education could neither effectively provide solutions to cross-disciplinary competencies for IFD nor supply evolving job market demands (aka skill gaps). Information and Communication Technologies (ICT) are pivotal in this transformation, shaping smart learning environments that integrate collaborative activities and harness freely available online tools and cloud services. However, challenges persist in cross-disciplinary fields like IFD, where data science and visualization technologies like Virtual Reality (VR) are increasingly applied but often not fully integrated into traditional curricula. To address this educational gap, this paper introduces the digiTeachVR platform, which incorporates VR and data science into a collaborative teaching platform designed for IFD. Undergraduate and graduate students from diverse disciplines participate in a parametric design course that leverages industrial data science and VR within an interactive, self-paced, collaborative environment. The digiTeachVR's approach involves self-taught programming lessons followed by collaborative programming on Python. This is to develop parametric designs, subsequently transformed into VR models using Rhino 3D. A pilot study of digiTeachVR at the TU Wien indicates a significant (79%) increase in student enthusiasm, suggesting improved content delivery and enhanced learning outcomes (63%). The paper presents the approach, summarizes the key achievements and identifies the future research pathways for enhancing quality of cross-disciplinary engineering education. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.\",\n",
       " 'Python data science libraries such as Pandas and NumPy have recently gained immense popularity. Although these libraries are feature-rich and easy to use, their scalability limitations require more robust computational resources. In this paper, we present PyTond, an efficient approach to push the processing of data science workloads down into the database engines that are already known for their big data handling capabilities. Compared to the previous work, by introducing TondIR, our approach can capture a more comprehensive set of workloads and data layouts. Moreover, by doing IR-level optimizations, we generate better SQL code that improves the query processing by the underlying database engine. Our evaluation results show promising performance improvement compared to Python and other alternatives for diverse data science workloads. © 2024 IEEE.',\n",
       " 'Data science pipelines commonly utilize dataframe and array operations for tasks such as data preprocessing, analysis, and machine learning. The most popular tools for these tasks are pandas and NumPy. However, these tools are limited to executing on a single node, making them unsuitable for processing large-scale data. Several systems have attempted to distribute data science applications to clusters while maintaining interfaces similar to single-node libraries, enabling data scientists to scale their workloads without significant effort. However, existing systems often struggle with processing large datasets due to Out-of-Memory (OOM) problems caused by poor data partitioning. To overcome these challenges, we develop Xorbits, a high-performance, scalable data science framework specifically designed to distribute data science workloads across clusters while retaining familiar APIs. The key differentiator of Xorbits is its ability to dynamically switch between graph construction and graph execution. Xorbits has been successfully deployed in production environments with up to 5k CPU cores. Its applications span various domains, including user behavior analysis and recommendation systems in the e-commerce sector, as well as credit assessment and risk management in the finance industry. Users can easily scale their data science workloads by simply changing the import line of their pandas and NumPy code. Our experiments demonstrate that Xorbits can effectively process very large datasets without encountering OOM or data-skewing problems. Over the fastest state-of-the-art solutions, Xorbits achieves an impressive 2.66 × speedup on average. In terms of API coverage, Xorbits attains a compatibility rate of 96.7%, surpassing the fastest framework by an impressive margin of 60 percentage points. Xorbits is available at https://github.com/xorbitsai/xorbits. © 2024 IEEE.',\n",
       " 'Due to several advantages like scalability or fast development cycles, microservice architectures could also support the development and deployment of data science workflows. In recent literature, many patterns for the development of microservice architectures have evolved mainly for transaction-oriented applications. This paper investigates suitable design patterns for data science workflows. For this, we will implement a data science workflow using a microservice architecture, implemented with two different patterns, the orchestrator and choreography patterns, and with synchronous and asynchronous communication styles. Experiments are conducted to compare these architecture patterns with workloads of volume and velocity criteria. Orchestrator pattern performs best and could be used for inference, while choreography pattern could be used for training of machine learning models due to asynchronous communication. Our paper can provide practical support to software architects in implementing data science workflows using appropriate microservice design patterns. Also, we have now combined microservices pattern with data science workflows. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " '[No abstract available]',\n",
       " 'The proceedings contain 16 papers. The topics discussed include: enhancing silicon nitride waveguide performance: optimization of sidewall roughness for low-loss applications through resist reflowing via tree-based modeling of relevant processing parameters; revealing surface refractive index dynamics in comb-like plasmonic optical fiber biosensors; spike detection algorithms for Raman spectroscopy: a comparative study; automatic optimization of spectral classifiers’ hyperparameters for pathogen identification through evolutionary techniques; active learning concept for materials process optimization of non-fullerene organic photovoltaic using small datasets; a comprehensive pipeline to integrate preprocessing and machine learning techniques for accurate classification in Raman spectroscopy; and fusion and integration pipelines for optical and chemical imaging data for clinical interpretation in ex vivo diagnosis.',\n",
       " 'The Second International Conference on Applied Data Science and Smart Systems (ADSSS-2023) was held on 15-16 December 2023 at Chitkara University, Punjab, India. This multidisciplinary conference focussed on innovation and progressive practices in science, technology, and management. The conference successfully brought together researchers, academicians, and practitioners across different domains such as artificial intelligence and machine learning, software engineering, automation, data science, business computing, data communication, and computer networks. The presenters shared their most recent research works that are critical to contemporary business and societal landscape and encouraged the participants to devise solutions for real-world challenges. © 2025 selection and editorial matter, Jaiteg Singh, S B Goyal, Rajesh Kumar Kaushal, Naveen Kumar and Sukhjit Singh Sehra; individual chapters, the contributors.',\n",
       " '[No abstract available]',\n",
       " 'The proceedings contain 296 papers. The topics discussed include: referring image segmentation: an attention-based integrative fusion approach; segmentation of brain tumor with deep learning models; deep learning based recognition of sign language; research and design of Chinese oral learning system based on speech recognition; digitization of ECG records using signal extraction techniques; efficiency analysis of CNN through different filters for medical image classification; leveraging DeepLabV3+ for lung region delineation in chest radiographs using convolutional neural network approach; Intelli-change remote sensing - a novel transformer approach; enactment of sales forecasting application using artificial intelligence techniques; and triangular inequality based K-means clustering method for online learning platform for college English on web technology.',\n",
       " 'In an increasingly fast-paced and dynamic world with exponentially more data, more roles are required in agile software development. At the same time, the development team needs to maintain speed and autonomy. The challenges surrounding the organization of cross-functional teams are thus exacerbated. Through a multi-case study with interviews from five organizations, we show how agile companies use different models of organizing data scientists. We find that there are specific challenges related to each of these organizational models and that some challenges are shared among all the organizational models. Challenges include difficulty coordinating development strategies and a lack of resources. In addition, we identify strategies used to overcome the challenges, including coordinating mechanisms for platform teams, communities of practice for data scientists, and the development of shared playbooks. © 2024 IEEE Computer Society. All rights reserved.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'The proceedings contain 17 papers. The special focus in this conference is on Southwest Data Science. The topics include: What are Smart Home Product Users Commenting on? A Case Study of Robotic Vacuums; detecting Political Polarization Using Social Media Data; Comparing Cost and\\xa0Performance of\\xa0Microservices and\\xa0Serverless in\\xa0AWS: EC2 vs Lambda; NBD-Tree: Neural Bounded Deformation Tree for\\xa0Collision Culling of\\xa0Deformable Objects; online Linear Regression Based on\\xa0Weighted Average; dimension Reduction Stacking for\\xa0Deep Solar Wind Clustering; an Experimental Study of the Joint Effects of Class Imbalance and Class Overlap; Evaluation of\\xa0SVM Transformations for\\xa0Multi-label Research Article Classification; a Scene Tibetan Text Detection by Combining Multi-scale and Dual-Channel Features; DGL Version 2 – Random Testing in the Mobile Computing Era; a Comparative Evaluation of\\xa0Image Caption Synthesis Using Deep Neural Network; parameter Estimation in\\xa0Biochemical Models Using Marginal Probabilities; detecting Microservice Anti-patterns Using Interactive Service Call Graphs: Effort Assessment; an Accurate and\\xa0Preservative Quenching Data Stream Simulation Method; a Study of\\xa0a\\xa0Metapopulation Model Using the\\xa0Stochastic Reaction Diffusion Master Equation.',\n",
       " \"Over the last decade, Data Science has emerged as one of the most important subjects that has had a major impact on industry. This is due to the continual development of scientific methods, algorithms, processes, and computational tools that help to extract knowledge from raw data efficiently and cost-effectively, compared with early-generation tools. Professional data scientists create code that processes, analyses and extracts actionable insights from high volumes of data. This process requires a deep understanding of mathematical principles, statistics, business knowledge, and computer science. But most importantly, the data science development chain requires knowledge of a high-level programming tool and its dependencies. This is a major problem in some aspects due to the steep learning curve. In this paper, we describe and present a modularized Data Science curriculum for undergraduate learners that relies on no-code software development tools as programming aids for non-computer science majors. No-code development tools have been added to the traditional teaching pedagogy to improve students' motivation and conceptual understanding of coding despite their limited programming skills. The study aims to assess the impacts of visual programming languages on the performance of non-computer science majors on programming problems. The study's sample consists of 50 fourth-year students from the Faculty of Science and Technology at the Midlands State University. A post-survey questionnaire and assessment items were administered to the control and experimental groups. Results show that the students drawn from the experimental group benefited from the use of a visual programming language. These results offer evidence-based recommendations for incorporating high-performance no-code software development tools in the formal curriculum to aid teaching and learning data science programming for students of diverse academic backgrounds. © 2013 IEEE.\",\n",
       " 'This document is the preface of the 6thInternational Workshop on Modern Data Science Technologies (MoDaST-2024), May, 31 - June, 1, 2024, held in Lviv-Shatsk, Ukraine. The main purpose of the MoDaST Workshop is providing a forum for researchers to discuss models, methods and information technology for data science, data analysis and business analysis, and their real-life applications. © 2024 Copyright for this paper by its authors.',\n",
       " 'The proceedings contain 28 papers. The topics discussed include: optimal multifactorial planning of experiments based on the combinatorial configurations MCDM; ensuring accuracy of personal data processed in blockchain systems; automatic smart subword segmentation for the reverse Ukrainian physical dictionary task; an approach to modeling elections in bipartisan democracies on the base of the ‘state-probability of action’ model; towards algorithmic and software solution for business process model analysis and correction; fuzzy logic-based methodology for building access control systems based on fuzzy logic; expert assessment of educational content in it specialists training process; computer linguistic system architecture for Ukrainian language content processing based on machine learning; and evaluation of the quality and usefulness of information technologies for supporting medical decision-making based on civil law.',\n",
       " 'This paper aims to investigate the scientific integration of data science with product design towards data-driven design (D3). Data science has potential to facilitate design decision-making through insight extraction, predictive analytics, and automatic decisions. A systematic scoping review is conduced to converge various D3 applications in four dimensions: the design dimension about design operations, the data dimension about popular data sources and common data-related challenges, the method dimension about the methodological foundations, and the social/ethical dimension about social/ethical considerations and implications. Based on the state-of-the-art, this paper also highlights potential future research avenues in this dynamic field. © 2024 The Author(s)',\n",
       " 'Deep neural networks are used to study the ambient vibrations of the medieval towers of the San Frediano Cathedral and the Guinigi Palace in the historic centre of Lucca. The towers have been continuously monitored for many months via high-sensitivity seismic stations. The recorded data sets integrated with environmental parameters are employed to train a Temporal Fusion Transformer network and forecast the dynamic behaviour of the monitored structures. The results show that the adopted algorithm can learn the main features of the towers’ dynamic response, predict its evolution over time, and detect anomalies. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'The proceedings contain 38 papers. The topics discussed include: voltage induced fluid mixing video data based volumetric ratio modelling using fractional order time series methods; a decision making framework for the selection of business process modeling languages; Smart Trader: towards an advisory chatbot service for Saudi stock market investment; Alzheimer’s disease detection based on brain signals using computational modeling; image segmentation methods: overview, challenges, and future directions; evaluation of the effectiveness of the structural unit of an industrial enterprise for the production of concrete by the method of factor analysis; the intersection of generative AI and healthcare: addressing challenges to enhance patient care; and study of the activity of the structural department of a gas transportation enterprise using methods of system analysis.',\n",
       " 'Today’s data-driven world requires earth and environmental scientists to have skills at the intersection of domain and data science. These skills are imperative to harness information contained in a growing volume of complex data to solve the world’s most pressing environmental challenges. Despite the importance of these skills, Earth and Environmental Data Science (EDS) training is not equally accessible, contributing to a lack of diversity in the field. This creates a critical need for EDS training opportunities designed specifically for underrepresented groups. In response, we developed the Earth Data Science Corps (EDSC) which couples a paid internship for undergraduate students with faculty training to build capacity to teach and learn EDS using Python at smaller Minority Serving Institutions. EDSC faculty participants are further empowered to teach these skills at their home institutions which scales the program beyond the training lead by our team. Using a Rasch modeling approach, we found that participating in the EDSC program had a significant impact on undergraduate learners’ comfort and confidence with technical and nontechnical data science skills, as well as their science identity and sense of belonging in science, two critical aspects of recruiting and retaining members of underrepresented groups in STEM. Supplementary materials for this article are available online. © 2024 The Author(s). Published with license by Taylor & Francis Group, LLC.',\n",
       " \"This study examined the data science career preference, data science skills, and core competencies of 416 students from fourteen Nigerian universities using a Google Forms-created structured online questionnaire. A convenience sampling technique was adopted to select the participants. Data were analysed using both descriptive and inferential statistics, including Structural Equation Modeling (SEM). Results were presented using tables, figures, and charts. Three hypotheses were tested to guide the discussion. The findings showed that while core competencies had no positive significant effect on career preference, data science skills had positive significant effect on career preference, and a positive significant relationship with core competencies. By providing insightful information about the participants' degree of data science knowledge, abilities, and core competencies, this research adds to the body of previous knowledge. Additionally, it shows patterns in students' preferences for careers in data science, which has consequences for developing curriculum and training data scientists in Nigeria. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.\",\n",
       " 'Systematic analysis of scientific, industrial, and commercial information and the application of data science (DS) are alternatives to learn about trends and advances in value chains such as avocado. The objective of this study was to review current trends in research, technological development, and perception about the avocado subsector under tropical conditions using DS tools. A bibliometric analysis (BA) of scientific articles, technological developments, and patents in different sources of web information was implemented. Additionally, search algorithms and DS were designed to perform an analysis of trends from social media platforms in the avocado value chain. The BA identified patters associated with the avocado chain in Colombia, highlighting the increase in scientific-technical information, the grouping by work networks, the bias associated with departments and commercial varieties such as the high volume of works in Antioquia and cv. Hass, and the few high-value alternatives for the chain such as patents, industrial secrets, etc. Trends in social networks were linked to international markets, added value, and environmental aspects. Likewise, a disconnection was found between the needs of the avocado value chain and the existing offers. This study contributes as a competitive strategy for the local avocado subsector and generates a country-adjusted research agenda. © This is an open access article distributed under the terms of the Creative Commons Attribution License (CC-BY), which permits unrestricted use, distribution, and reproduction in any medium, as long as the original work is properly cited.',\n",
       " 'The proceedings contain 102 papers. The topics discussed include: EMG-based wrong posture detection system using Raspberry Pi and cloud-based visualization; a novel trust-enabled data-gathering technique based on modified Golden Eagle optimization in wireless sensor network; earlier detection of microaneurysms in fundus image using VGG-19 with dual attention mechanism; intelligent load monitoring and control in railway wagons using IOT; a novel approach to carrier guidance system using machine learning and blockchain; the intersection of AI, ethics, and education: a bibliometric analysis; stacked regressor for crop yield prediction; a comprehensive review on underwater object detection techniques; and enhanced privacy in data aggregation with secret sharing techniques.',\n",
       " '[No abstract available]',\n",
       " 'In recent years, there has been an explosion in the growth of undergraduate statistics and data science programs across the US. Simultaneously, there has been clear guidance written on curriculum development for both data science (De Veaux et\\xa0al.) and statistics (Carver et\\xa0al.) programs. While this was occurring, ABET (now simply an acronym, but previously standing for the Accreditation Board for Engineering and Technology), in coordination with organizations such as the American Statistical Association, developed accreditation criteria for Data Science programs. In this article, we discuss our journey through ABET accreditation and discuss how adopting ABET processes for continuous improvement strengthens a program’s assessment process. We share best practices for working across multiple departments to collect data not only on individual courses, but also on the program as a whole. While the framework presented was initially established to support ABET accreditation, we argue that a properly executed program assessment should occur regardless of whether or not an institution is seeking ABET accreditation for their data science program. Throughout this article, we also discuss the extent to which ABET requirements naturally fit within our program’s existing goals, including an assessment of how ABET requirements align with major ideas in the field of data science education. © This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law.',\n",
       " 'Context: Epilepsy is a common neurological disease and is classified into different types based on features such as the kind of seizure, age of onset, part of brain effected, etc. There are nearly 30 approved anti-epileptic drugs (AEDs) for treating different epilepsies and each drug targets proteins exhibiting a specific molecular mechanism of action. There are many genes, proteins, and microRNAs known to be associated with different epileptic disorders. This rich information on epilepsy-associated data is not available at one single location and is scattered across multiple publicly available repositories. There is a need to have a single platform integrated with the data, as well as tools required for epilepsy research. Methods and Material: Text mining approaches are used to extract data from multiple biological sources. The data is curated and populated within an in-house developed epilepsy database. Machine-learning based models are built in-house to know the probability of a protein being druggable based on the significant protein features. A web interface is provided for the access of the epilepsy database as well as the ML-based tool developed in-house. Results: The epilepsy-associated data is made accessible through a web browser. For a protein of interest, the platform provides all the feature values, and the results generated using different machine learning models are displayed as visualization plots. Conclusions: To meet these objectives, we present TREADS, a platform for epilepsy research community, having both database and an ML-based tool for the study of AED targets. To access TREADS: https://treads-aer.cdacb.in © 2024 Neurology India, Neurological Society of India.',\n",
       " \"The research offers insights into the impact of the pandemic on students and ruminates on mitigating the negative consequences by using a virtual assistant (VA). In an experimental setting, the study treats 'stress' as a critical factor that relates to students' well-being. The research is exploratory, where mixed-mode data is captured from students using a questionnaire and integrated virtual assistant simultaneously. The research findings establish the role of a virtual assistant to support preventive, predictive, and personalised health. It illuminates heart rate variability as one of the key indicators of perceived stress. As this research is based on the extensive literature on a method called 'photoplethysmography', it can further be scaled for supporting large groups of students. The outcome of the study can be further extended to industries where stress might be detrimental to well-being. The study contributes to the innovative application of data science to reflect on students' well-being. Copyright © 2024 Inderscience Enterprises Ltd.\",\n",
       " 'As leveraging large-scale data analytics becomes the norm for many applications, platforms used to develop these capabilities have become increasingly important. In this work, we compare the benefits and drawbacks of implementations of two commonly used data science platform paradigms: code-based scripts and GUI-based workflows. We implement tasks in both paradigms that provide examples of phases in the typical life cycle of a data science project, including data wrangling, machine learning (ML) model training, and inference. We examine the relative performance of the implementations under each paradigm in various experimental settings. We discuss the benefits and drawbacks associated with each platform implementation and provide a foundation for future work in comparing data science platform paradigms. © 2024 IEEE.',\n",
       " 'As machine learning and AI systems become more prevalent, understanding how their decisions are made is key to maintaining their trust. To solve this problem, it is widely accepted that fundamental support can be provided by the knowledge of how data are altered in the pre-processing phase, using data provenance to track such changes. This paper focuses on the design and development of a system for collecting and managing data provenance of data preparation pipelines in data science. An investigation of publicly available machine learning pipelines is conducted to identify the most important features required for the tool to achieve impact on a broad selection of preprocessing data manipulation. This reveals that the operations that are used in practice can be implemented by combining a rather limited set of basic operators. We then illustrate and test implementation choices aimed at supporting the provenance capture for those operations efficiently and with minimal effort for data scientists. © 2024 IEEE.',\n",
       " 'The proceedings contain 29 papers. The special focus in this conference is on Machine Learning, Image Processing, Network Security and Data Sciences. The topics include: The Potential of\\xa01D-CNN for\\xa0EEG Mental Attention State Detection; marker-Based Augmented Reality Application in\\xa0Education Domain; detection and\\xa0Classification of\\xa0Waste Materials Using Deep Learning Techniques; Deep Learning Based EV’s Charging Network Management; internet of Medical Things: Empowering Mobility and Health Monitoring with a Smart Walking Stick; SynText - Data Augmentation Algorithm in NLP to Improve Performance of Emotion Classifiers; preface; Phishing Detection Using 1D-CNN and\\xa0FF-CNN Models Based on\\xa0URL of\\xa0the\\xa0Website; A Comparative Analysis of\\xa0ML Based Approaches for\\xa0Identifying AQI Level; a Review of Authentication Schemes in Internet of Things; advancements in\\xa0Facial Expression Recognition: A Comprehensive Analysis of\\xa0Techniques; a Deep Learning Method for\\xa0Obfuscated Android Malware Detection; COVID-19 Detection from\\xa0Chest X-Ray Images Using GBM with\\xa0Comparative Analysis; hate Speech Detection Using Machine Learning and\\xa0Deep Learning Techniques; effectiveness of Influencer Marketing on Gen Z Consumers; compressors Using Modified Sorting and Parallel Counting; violence Detection in\\xa0Indoor Domestic Environment Using Multimodal Information; Code-Mixed Language Understanding Using BiLSTM-BERT Multi-attention Fusion Mechanism; diabetes Prediction Using Machine Learning Classifiers; crop Yield Prediction Using Machine Learning Approaches; Comparative Analysis of Economy-Based Multivariate Oil Price Prediction Using LSTM; MRI Based Spatio-Temporal Model for\\xa0Alzheimer’s Disease Prediction; bridging the\\xa0Gap: Condensing Knowledge Graphs for\\xa0Metaphor Processing by\\xa0Visualizing Relationships in\\xa0Figurative and\\xa0Literal Expressions; a Novel Unsupervised Learning Approach for\\xa0False Data Injection Attack Detection in\\xa0Smart Grid; a Multi-stage Encryption Technique Using Asymmetric and Various Symmetric Ciphers; speed-Invariant Gait Recognition Using Correlation Factor Lists for\\xa0Classroom Attendance Systems.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " \"Social media triggered intellectual issues like social network mental disorders (SNMD) also are on the upward push cyber dependence, statistics overload and network crush have emerge as not unusual I noticed currently. The signs of these mental disorders at the moment are normally seen inside the passive this may put off clinical intervention. In this text we discuss social media mining this affords a sensible opportunity for early detection of SNMD at an early level. It's far since intellectual factors are taken into consideration stable, SNMD is less difficult to diagnose. Diagnostic criteria (questionnaires) for anti-social conduct cannot be located on-line. Our method, new and modern, does not depend on the usage of SNMD detection self-diagnosis of those intellectual troubles through questionnaires. We would really like to present a vehicle regulatory frame, particularly social network Mental Disorders Detection (SNMDD), which using capabilities extracted from social media facts to correctly pick out ability incidents SNMD. We practice multi-supply getting to know in SNMDD and suggest a just out of the plastic new SNMD-based totally tensor variation (STM) for higher in general execution. Our machine assessed by means of a person survey of 3,126 social media users. We are on submit analysis, and the use of SNMDD for big-scale facts analysis and evaluation three types of SNMD. The outcomes display that SNMDD is promising for online character. Interpersonal organization clients with viable SNMD © 2024 IEEE.\",\n",
       " 'Foundations of Data Science with Python introduces readers to the fundamentals of data science, including data manipulation and visualization, probability, statistics, and dimensionality reduction. This book is targeted toward engineers and scientists, but it should be readily understandable to anyone who knows basic calculus and the essentials of computer programming. It uses a computational-first approach to data science: the reader will learn how to use Python and the associated data-science libraries to visualize, transform, and model data, as well as how to conduct statistical tests using real data sets. Rather than relying on obscure formulas that only apply to very specific statistical tests, this book teaches readers how to perform statistical tests via resampling; this is a simple and general approach to conducting statistical tests using simulations that draw samples from the data being analyzed. The statistical techniques and tools are explained and demonstrated using a diverse collection of data sets to conduct statistical tests related to contemporary topics, from the effects of socioeconomic factors on the spread of the COVID-19 virus to the impact of state laws on firearms mortality. This book can be used as an undergraduate textbook for an Introduction to Data Science course or to provide a more contemporary approach in courses like Engineering Statistics. However, it is also intended to be accessible to practicing engineers and scientists who need to gain foundational knowledge of data science. Key Features: Applies a modern, computational approach to working with data, Uses real data sets to conduct statistical tests that address a diverse set of contemporary issues, Teaches the fundamentals of some of the most important tools in the Python data-science stack, Provides a basic, but rigorous, introduction to Probability and its application to Statistics, Offers an accompanying website that provides a unique set of online, interactive tools to help the reader learn the material. © 2024 John Mark Shea.',\n",
       " 'This paper describes the intentions, setup, and live performance of a musical experiment that explores the complex intersection of human-technology interactions, music, and data collection. It brings art and data science together through a novel experimental music installation. The interdisciplinary project “The (Un)Answered Question: A Data Science Powered Music Experiment” explored integrating data science and biomedical imaging techniques with theatrical and compositional ideas. This combination leads to the creation of interactive music. Gestural interfaces and sensory input devices translate physiological behavior into music through digital signal processing. Ralph Waldo Emerson’s poem “The Sphynx” and Charles Ives’ composition “The Unanswered Question” serve as foundational elements to create a live remix of the original music using biometric data from performers and an audience of 180 people. The audience became a powerful instrument of musical expression. Each live performance was experiential and unique, depending on the different people involved. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " '[No abstract available]',\n",
       " 'By its nature, data science uses ideas and methodologies from computer science and statistics, along with field-specific knowledge, to describe, learn and predict. Recently, storytelling has been highlighted as an important extension of more traditional data science skills such as coding and modeling. Three courses in our new Master in Data Science and Analytic Storytelling program were designed to include interdisciplinary modules, mainly taught by faculty in storytelling-related disciplines, such as Communication and Art & Design. These courses were PDAT 622: Narrative, Argument, and Persuasion in Data Science; PDAT 624: Principles of Design in Data Visualization; and PDAT 625: Big Data Ethics and Security. Our first cohort serves as a natural case study, allowing us to reflectively analyze our materials and an informal student survey to explore the effects of interdisciplinarity in these novel courses. Results of the student survey show that students generally found value in these interdisciplinary course components, especially in course “signature assignments,” which allow students to actively engage with course content while reinforcing technical skills from previous courses. Examples of these signature assignments are presented in this paper’s supplementary materials. © 2024 The Author(s).',\n",
       " '[No abstract available]',\n",
       " 'Access to large amounts of data, provided by continuous technological progress, becomes increasingly important for organisations. The access to data becomes even more useful when managers consult structured information in real time. This requires the adoption of data analysis techniques that, at a strategic level, allow the definition of business goals according to pre-defined objectives, and which help companies in their functional activities. With Data Science, Data Mining and Machine Learning platforms allow the development of decision support systems, capable of establishing patterns between data and making truly useful predictions for decision support. The methodology used in this work aims to combine Data Science methods, such as Data Mining and Machine Learning, in order to demonstrate the importance of transforming data into organised and oriented information to support decision making. © 2024 The Authors. Published by ELSEVIER B.V.',\n",
       " 'In this ambitious agricultural project, cutting-edge technologies such as Machine Learning (ML) and Deep Learning (DL) are harnessed to revolutionize farming practices in India. The primary focus lies in automating vital tasks like crop disease detection, crop recommendation, and fertilizer suggestion, which are traditionally laborious and time-consuming for farmers. Leveraging recent advancements in ML algorithms, a comprehensive solution is envisaged. This innovative approach involves the integration of sophisticated models into a user-friendly website, creating a seamless user experience. Through this platform, farmers gain access to automated tools that significantly enhance their crop management strategies. The platform provides a comprehensive solution by combining deep learning-based crop disease detection mechanisms, intelligent crop recommendation systems, and effective fertilizer recommendations derived from machine learning algorithms. This combination of technologies helps farmers maximize their overall crop yields and minimize losses caused by diseases in addition to streamlining farming procedures. The synergy of ML, and DL on a unified web interface stands as a testament to the transformative power of technology in agriculture, marking a pivotal step toward sustainable and efficient farming practices in India. © 2024 IEEE.',\n",
       " 'This book is the second in a two-volume series that introduces the field of spatial data science. It moves beyond pure data exploration to the organization of observations into meaningful groups, i.e., spatial clustering. This constitutes an important component of so-called unsupervised learning, a major aspect of modern machine learning. The distinctive aspects of the book are both to explore ways to spatialize classic clustering methods through linked maps and graphs, as well as the explicit introduction of spatial contiguity constraints into clustering algorithms. Leveraging a large number of real-world empirical illustrations, readers will gain an understanding of the main concepts and techniques and their relative advantages and disadvantages. The book also constitutes the definitive user’s guide for these methods as implemented in the GeoDa open source software for spatial analysis. It is organized into three major parts, dealing with dimension reduction (principal components, multidimensional scaling, stochastic network embedding), classic clustering methods (hierarchical clustering, k-means, k-medians, k-medoids and spectral clustering), and spatially constrained clustering methods (both hierarchical and partitioning). It closes with an assessment of spatial and non-spatial cluster properties. The book is intended for readers interested in going beyond simple mapping of geographical data to gain insight into interesting patterns as expressed in spatial clusters of observations. Familiarity with the material in Volume 1 is assumed, especially the analysis of local spatial autocorrelation and the full range of visualization methods. © 2024 Luc Anselin.',\n",
       " \"In this era of information explosion and digitized connectivity, the whole dimension of political participation is going through a radical transformation. This research paper deeply explores the influence of data science utilizing Python as a programming language and machine learning models up to the deep intricacies of public opinion, political campaigns, and political behavior. As such, this study aims to shed light on the contours and dynamics within these arenas - providing crucial insights into the complex interplay between advances in information technology and developments in democratic practices. With the unrivaled availability of data, our study taps into a wide range of datasets that cover election results, voters' attributes, social media interactions, campaigns' funding, and public sentiment surveys. We make use of Python tools like Pandas, NumPy, Scikit-learn, and Matplotlib for the task of conducting exploratory data analysis (EDA), feature engineering, as well as machine learning techniques. The findings of our investigation help us understand the comprehensive various ways that focused extensive campaigning by data-driven insights impact public perceptions. The research bravely shows associations between the emotion that was expressed on social media and election results, leaving no doubt as to the degree to which the digital platform has become influential in shaping current political discussions. Further, the changing approaches in campaigns have been proven to predict effectively through the models of machine learning, the changes in public mood. In essence, this study attempts to make a valuable contribution toward the scholarly understanding of the dynamic political setting and, simultaneously, offer practical implications for political professionals, policymakers, and scholars. The findings highlight the ability of Python programming and machine learning to decipher the nuances that typify contemporary political campaigns hence increasing effectiveness in serving an entire gamut of voter constituencies. However, it also uncovers the ethical issues of data protection and responsible use of algorithms in the context of politics. © 2024 IEEE.\",\n",
       " 'In contemporary warfare, data science is crucial for the military in achieving information superiority. To gain an overview of the topic, 158 peer-reviewed articles were analysed through a semi-systematic literature review. The proportion of social science literature with a focus on risks of data science implies that policymakers are disproportionally influenced by a pessimistic view on military data science. The perceived risks of data science, however, are hardly addressed in formal science literature. Additionally, when levels of war are taken into account, relatively low attention for the operational level is observed. Literature reflects an emphasis on the tactical level when it comes to opportunities for military data science. On the contrary, studies examining the risks of military data science mostly consider the strategic level. Consequently, domain-specific requirements for military strategic data science may not be expressed. Lacking such applications ultimately leads to suboptimal strategic decisions in today’s warfare. © 2024 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'Efficient factory planning is becoming increasingly important in an ever more dynamic competitive environment. The automated generation of planning information from production data provides an approach to avoid errors in factory planning, accelerate the factory planning process and thus increase the efficiency of factory planning. The challenges involved in implementing such approaches are analyzed below and suitable solutions are derived. © 2024, VDI Fachmedien GmBH & Co. KG. All rights reserved.',\n",
       " 'This study proposes a two-phase data science framework for the friction force and parameter estimation of the hysteresis effect segment in the servo-control systems of precision machines. The first phase uses an exponential-based friction force model to identify the model parameters by an autoregressive model and Z-transform. The second phase uses symbolic regression for the residual analysis to enhance the friction force estimation. An empirical study of three types of CNC machines under different working conditions is conducted to validate the two-phase data science framework and identify the change of the machining displacement considered to be a critical factor affecting friction force. The exponential-based model successfully eliminates the circular spike error caused by the friction force in the tapping center machine. The results indicate that the proposed two-phase framework improves mean absolute error by 5.6% on average in the tapping center, 6.5% in the milling machine and 7.6% in the turning and milling center, respectively. © 2024 Informa UK Limited, trading as Taylor & Francis Group.',\n",
       " 'Optical imaging of the brain has expanded dramatically in the past two decades. New optics, indicators, and experimental paradigms are now enabling in-vivo imaging from the synaptic to the cortex-wide scales. To match the resulting flood of data across scales, computational methods are continuously being developed to meet the need of extracting biologically relevant information. In this pursuit challenges arise in some domains (e.g., SNR and resolution limits in micron-scale data) that require specialized algorithms. These algorithms can, for example, make use of state-of-the-art machine learning to maximally learn the details of a given scale to optimize the processing pipeline. In contrast, other methods, however, such as graph signal processing, seek to abstract away from some of the details that are scale-specific to provide solution to specific sub-problems common across scales of neuroimaging. Here we discuss limitations and tradeoffs in algorithmic design with the goal of identifying how data quality and variability can hamper algorithm use and dissemination. © 2024 SPIE.',\n",
       " 'Violence against women has increased. Throughout time, various strategies have been implemented to eradicate this public health problem. Researchers with diverse professional profiles have joined forces to propose solutions in the area of computer sciences. In this article, a systematic revision on the technologies implemented to eradicate violence against women was carried out, applying the PRISMA methodology. The country with the most research in the area of Artificial Intelligence is the United States of America. Research is focused on the predictive analysis of violence in different contexts, and studies related to Neural Networks and the treatment of medical reports data are the ones that are most similar to the analysis of partner violence. However, no evidence was found that predictions were made to prevent feminicide. This is why implementing data science in reports of violent incidents against women is imperative to restructure the protocol of action in governmental institutions and, with it, prevent the risk of feminicide. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'The paper presents a resource search and visualization approach for learning data science fundamentals. We implement a web prototype that uses a knowledge graph built from the topics of the domain and a metadata repository of resources related to these topics. The application allows the graphical and interactive visualization of the graph and its semantic relationships, as well as the exploration, search, and visualization of concepts and resources. The results of the preliminary validation show its potential to improve the understanding of data science topics and promote free access to educational resources such as datasets, notebooks, and multimedia material. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " 'Artificial Intelligence (AI) and Data Science stand as pivotal innovations that revolutionize methods and processes across a multitude of industries. In unison, they facilitate the management, storage, transmission, and analysis of vast data volumes. A significant portion of these challenges are articulated as optimization problems. The potency of AI and Data Science is deeply rooted in the successful resolution of these optimization problems, which are prevalent in areas such as machine learning model fine-tuning, operational research, and logistics. However, it is crucial to acknowledge that solutions to these optimization problems do not always come with guarantees. This does not necessarily imply that research is lacking in this direction. Instead, it is often a manifestation of the constraints imposed by the nature of the digital hardware used for calculations. Digital hardware is bound by physical limitations, including constraints on processing power, storage capacity, and time. A key limitation, however, is its inherent binary nature, which can handle only discrete values precisely and may struggle with mathematical functions on real variables. This chapter aims to summarize key results from the field of computability and highlight critical, yet lesser-known issues in optimization theory. © 2024 Elsevier B.V.',\n",
       " \"The paper is designed as a review and analysis of the current situation in the field of information technology (IT). The relevance of the article arises from the need to popularize IT among young people, which is reflected in the documents of the Ministry of Digital Development, Communications and Mass Communications of the Russian Federation, as well as from the high demand for experts in the field of digital technologies and the need to increase the digitalization of Russia. According to GeekBrains (the first programming school in Russia, which appeared in 2010, and the leader in the field of training for digital professions, which operates in 85 regions of the Russian Federation and 21 countries of the world), the number of IT job vacancies in this country will grow up to 2 million by 2027. All large organizations are looking for specialists in different IT branches, which are described in this and the following research paper. In this article, which is the first part of the study, the capabilities and problems of the following advanced technologies are considered: artificial intelligence, Data Science and quantum computing. The concepts of technologies are analyzed from the viewpoints of their everyday use and professional application. The topics are closely related and complement each other in one way or another. The article follows the logic and chronology of events in the global IT sphere. The conclusion contains a description of the author's thought experiment about bringing machine intelligence to life, it accumulates the mentioned technologies while taking into account the laws of physics. The calculations for this experiment are given as well. The materials from one of the GeekBrains's educational shows “The Way to IT” were used, including the GIF animation, which is reproduced only on the website of the “Scientific Visualization” journal. © 2024 National Research Nuclear University. All rights reserved.\",\n",
       " 'The proceedings contain 70 papers. The special focus in this conference is on Data Science and Artificial Intelligence. The topics include: Internet Employment Detection Scam of Fake Jobs via Random Forest Classifier; sentiment Analysis and Age Factor Influences on Brand Usage of Personal Care Products: A Study with Reference to Visakhapatnam City, India; construction of Pythagorean and Reciprocal Pythagorean n-tuples; statistical Approaches for Forecasting Air pollution: A Review; computer-Assisted Statistical Analysis of L-Methionine Protonation Equilibria in The Anionic, Cationic, and Neutral Micellar Systems; a Study on Benefits of Continuous Integration and Continuous Delivery in Software Engineering; an Overview: Progressive Report on Magic Labelling; multi-keyword Ranked Search with Privacy Protection on Encrypted Cloud Data; BERT-Based Similarity Measures Oriented Approach for Style Change Detection; computer Vision–Based Malpractice Detection System; the Evolution of Influence Maximization Studies: A Scientometric Analysis; a Survey on Evolving Optimal Encryption Methods in Cloud Computing Data Forensics; DR-HIPI: Performance Evaluation of Retinal Images for DR Lesion Segmentation Using the HIPI Architecture; adaptability of Robotic Process Automation and Capabilities of Human Automation; A Brief Review on the Impact of Digital Trading on Conventional Marketing Strategies After COVID-19’s Effect on Ecommerce; fall Support Assistant Application; A Systematic VANET Traffic Congestion by Eliminating Recursion Using Intervention Linear Minimum Spanning Tree (ILMST) for Traffic Management System; Real-Time Object Cloning in Augmented Reality Using BASNet and Screenpoint Algorithms; Maximization of Energy Efficiency for Optimal Spectral Efficiency in Massive MIMO System; Optimization of Spectral Efficiency Using Precoding Techniques in Multi-cell Massive MIMO.',\n",
       " 'The breadth of problems that can be solved with data science is astonishing, and this book provides the required tools and skills for a broad audience. The reader takes a journey into the forms, uses, and abuses of data and models, and learns how to critically examine each step. Python coding and data analysis skills are built from the ground up, with no prior coding experience assumed. The necessary background in computer science, mathematics, and statistics is provided in an approachable manner. Each step of the machine learning lifecycle is discussed, from business objective planning to monitoring a model in production. This end-to-end approach supplies the broad view necessary to sidestep many of the pitfalls that can sink a data science project. Detailed examples are provided from a wide range of applications and fields, from fraud detection in banking to breast cancer classification in healthcare. The reader will learn the techniques to accomplish tasks that include predicting outcomes, explaining observations, and detecting patterns. Improper use of data and models can introduce unwanted effects and dangers to society. A chapter on model risk provides a framework for comprehensively challenging a model and mitigating weaknesses. When data is collected, stored, and used, it may misrepresent reality and introduce bias. Strategies for addressing bias are discussed. From Concepts to Code: Introduction to Data Science leverages content developed by the author for a full-year data science course suitable for advanced high school or early undergraduate students. This course is freely available and it includes weekly lesson plans. © 2024 Adam P. Tashman.',\n",
       " 'This commentary discusses opportunities for advancing the field of developmental psychopathology through the integration of data science and neuroscience approaches. We first review elements of our research program investigating how early life adversity shapes neurodevelopment and may convey risk for psychopathology. We then illustrate three ways that data science techniques (e.g., machine learning) can support developmental psychopathology research, such as by distinguishing between common and diverse developmental outcomes after stress exposure. Finally, we discuss logistical and conceptual refinements that may aid the field moving forward. Throughout the piece, we underscore the profound impact of Dr Dante Cicchetti, reflecting on how his work influenced our own, and gave rise to the field of developmental psychopathology. © The Author(s), 2024. Published by Cambridge University Press.',\n",
       " 'In the era of Big Data, the successful completion of Data Science (DS) projects is crucial. However, DS project management is quite challenging due to its interdisciplinary nature. Existing DS process models, such as CRISP-DM, have limitations, resulting in low success rates for these undertakings. To address this issue, a novel methodology for the construction of patterns in DS project management has been proposed, using the Design Science Research methodology. The design draws inspiration from existing pattern concepts to address common problems in DS project execution. The methodology is demonstrated through the creation of patterns for best practices in DS project management, synthesized from scientific literature. The goal of this approach is to provide a platform for exchanging and standardizing best practices in DS project management. While initial demonstrations show the general applicability of the methodology, further evaluations and case studies are necessary to assess its effectiveness and areas for improvement. The study identifies potential ambiguities in certain activities within the process, suggesting opportunities for refinement. Overall, this research contributes to the field of DS project management by offering a structured method to encapsulate and disseminate effective practices, supporting the successful execution of data projects in organizations. Copyright © 2024 by SCITEPRESS – Science and Technology Publications, Lda.',\n",
       " 'This research paper explores how advancements in Artificial Intelligence, particularly Natural Language Processing (NLP), are impacting healthcare. NLP, which can analyse and understand human language, is being integrated into clinical research. The importance of integrating Natural Language Processing (NLP) methods into clinical informatics research has been increasingly acknowledged in recent years, leading to significant advancements. NLP systems are designed to process medical documents, sentences, and individual words to extract valuable information. Clinical text classification with filtering tools, techniques and domain knowledge improves accuracy on imbalanced and noisy data. This review analyses existing studies on NLP in healthcare, exploring different approaches and technical aspects. The paper also highlights the use of NLP in various healthcare applications and identifies areas for future research to further develop NLP-driven smart healthcare. © 2024 IEEE.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'The ethos surrounding data science as a sociotechnical phenomenon is multifaceted. The phenomenon embodies both advantageous and detrimental discourses. On the one hand, data science systems in healthcare offer novel technologies to help private and public institutions aid in better decision-making. On the other hand, facial recognition software often jeopardizes fundamental human rights with invasive and discriminatory algorithms. While making data science systems, practitioners are typically encouraged to execute project management methodology CRISP-DM (Cross Industry Standard Process for Data Mining) to complete projects successfully. Created for data mining projects, CRISP-DM guides the management of data science projects with six phases: business understanding, data understanding, data preparation, modeling, evaluation, and deployment. This work-in-progress conceptual paper uses an intersectional feminist framework to critically analyze CRISP-DM for data science projects. The reimagined intersectional CRISP-DM or InCRISP-DM methodology embraces iterative intersectional feminist interrogation to clarify six standard CRISP-DM workflow phases with four provocations: Learning & Praxis, Harm Reduction, Transformation and Accountability & Transparency. Future work appeals to bringing awareness of transnational risks that can emerge when applying western project management methodologies to countries of the Global South. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " \"Data Science techniques hold the potential of analyzing data from multiple sources, which is crucial in knowledge discovery and decision making. It is a promising career with solid demand growth. Therefore, students are pursuing efforts to understand and learn about this topic. Spanish speaker students, however, have difficulties on obtaining hands-on training about this topic in their native language, then, the production of video lectures in Spanish are desirable. This paper shows the perception of 177 students who took the 'Introduction to Data Science' course produced by [1] and translated by the authors. In addition, the optional use of the Arduino APMonitor temperature control lab (TCLab) as a tool to enhance learning is studied. Multiple correspondence analysis studied the answers of the survey filled before and after the course. According to students' opinion, there was improvement in their abilities to program, use Python and Arduinos after finishing the course. © 2024 IEEE.\",\n",
       " 'The evolution of industrial revolutions has been marked by the increasing use of data and information to improve productivity and efficiency. Industry 3.0 introduced automation and digitalization, which generated a lot of data from various sources and processes. This data was mainly used for monitoring and controlling the industrial activities, such as production, quality, and maintenance. Industry 4.0 leveraged this data to generate insights and intelligence, using technologies such as cloud computing, big data analytics, and the Internet of Things (IoT). These technologies enabled the integration and communication of data across different levels and domains of the industrial system, such as machines, products, processes, and services. Industry 4.0 also introduced the concept of smart factories, which are self-organizing, adaptive, and learning systems that can optimize their performance and efficiency. Industry 5.0 aims to enable human-robot collaboration and artificial intelligence [1], creating a more personalized and sustainable industrial system. Industry 5.0 focuses on enhancing the human capabilities and creativity, rather than replacing them with machines. It also emphasizes the social and environmental aspects of industrial development, such as customer satisfaction, worker well-being, and resource conservation. Industry 5.0 envisions a human-centric and eco-friendly industrial paradigm, where humans and machines work together in harmony and synergy. One of the sectors that can benefit from the convergence of business intelligence (BI) and artificial intelligence (AI) is the energy industry, which faces challenges such as increasing demand, environmental regulations, and market volatility. By combining BI and AI, energy companies can unlock value from their data and optimize their operations, such as production, distribution, and consumption. BI helps energy companies to collect, store, analyze, and visualize data from various sources, such as sensors, meters, devices, and systems. BI enables energy companies to monitor and manage their assets, processes, and performance, as well as to identify and solve problems, improve efficiency, and reduce costs. AI helps energy companies to augment and automate their decision making, using techniques such as machine learning, natural language processing, computer vision, and deep learning. AI enables energy companies to generate predictions, recommendations, and insights from their data, as well as to optimize their operations, such as scheduling, dispatching, pricing, and trading. AI also helps energy companies to create new products and services, such as smart grids, smart meters, smart homes, and smart cities. By combining BI and AI, energy companies can create a data-driven and intelligent energy system, which can respond to the changing needs and preferences of customers, stakeholders, and regulators, as well as to the dynamic and uncertain market conditions. This paper discusses the approach of complimenting the established business intelligence (BI) process with Artificial Intelligence (AI) in order to optimize gas production in an oil field in the south of Sultanate of Oman, it details the facts, observations, and insights the multidisciplinary authors have captured throughout the progress of this work, as well as general industry insights and BI process description. Copyright © 2024, Society of Petroleum Engineers.',\n",
       " 'Data Science Fundamentals with R, Python, and Open Data Introduction to essential concepts and techniques of the fundamentals of R and Python needed to start data science projects Organized with a strong focus on open data, Data Science Fundamentals with R, Python, and Open Data discusses concepts, techniques, tools, and first steps to carry out data science projects, with a focus on Python and RStudio, reflecting a clear industry trend emerging towards the integration of the two. The text examines intricacies and inconsistencies often found in real data, explaining how to recognize them and guiding readers through possible solutions, and enables readers to handle real data confidently and apply transformations to reorganize, indexing, aggregate, and elaborate. This book is full of reader interactivity, with a companion website hosting supplementary material including datasets used in the examples and complete running code (R scripts and Jupyter notebooks) of all examples. Exam-style questions are implemented and multiple choice questions to support the readers’ active learning. Each chapter presents one or more case studies. Written by a highly qualified academic, Data Science Fundamentals with R, Python, and Open Data discuss sample topics such as: • Data organization and operations on data frames, covering reading CSV dataset and common errors, and slicing, creating, and deleting columns in R • Logical conditions and row selection, covering selection of rows with logical condition and operations on dates, strings, and missing values • Pivoting operations and wide form-long form transformations, indexing by groups with multiple variables, and indexing by group and aggregations • Conditional statements and iterations, multicolumn functions and operations, data frame joins, and handling data in list/dictionary format Data Science Fundamentals with R, Python, and Open Data is a highly accessible learning resource for students from heterogeneous disciplines where Data Science and quantitative, computational methods are gaining popularity, along with hard sciences not closely related to computer science, and medical fields using stochastic and quantitative models. © 2024 by John Wiley & Sons, Inc. All rights reserved.',\n",
       " 'The COVID-19 pandemic has significantly disrupted the world economy and has put a strain on healthcare systems around the world. To be able to reduce the virus’s spread, it is essential to develop an effective detection and prevention system. This system should be able to detect the presence of the virus in a population and provide early warning to the public health authorities. Data science, machine learning, and Internet of Things (IoT) technologies can be used to develop such a system. Data science can be used to analyze large datasets to find trends and patterns in the virus’s transmission. Machine learning algorithms can be used to identify potential hot spots and foresee how the infection will propagate. IoT technologies can be used to monitor the health of individuals and provide real-time data to public health authorities. The system uses IoT devices to monitor the environment and detect any changes in temperature, humidity, and air quality that may indicate the presence of the virus. This system can be used to provide early warning to the public health authorities and help them take necessary preventive measures to contain viral infection and spread. © 2024 selection and editorial matter, Arun Kumar Rana, Vishnu Sharma, Ajay Rana, Maksud Alam and Suman Lata Tripathi.',\n",
       " 'Data technology has experienced an explosion in popularity and scope over the past few years, inflicting several companies to try to capitalize on the sizeable possibilities the technology has to provide. Agencies are utilizing AI to develop records science studies, allowing them to go beyond conventional strategies and discover new insights and innovations within the discipline. With AI, statistics scientists can identify styles in massive datasets, optimize algorithms to clear up complicated issues and create predictive fashions to discover developments and correlations. In addition, AI may be used to automate specific components of fact processing, making data technological know-how simpler and quicker. To make the most of this generation, organizations should have personnel knowledgeable in the basics of facts science and AI and the knowledge to recognize the complexities of the algorithms and applications they use. With the right group and the proper equipment, AI may make data technological know-how studies extra effective and more beneficial than ever. © 2024 IEEE.',\n",
       " \"This paper is a documentation for datawindow, a Python package which has the goal of simplifying the data processing and analysis techniques. Datawindow provides eight methods which allows users to quickly and easily handle activities in the data science process such as model building, data cleaning and summarization using user-friendly interfaces. The datawindow keeps user-friendliness and modularity as the main focus in the library's design. Datawindow provides the user-friendly experience by utilizing Tkinter for graphical user interfaces. Datawindow users Pandas for data processing and Tkinter for the interfaces to help provide clarity to the users. This ensures that the library satisfies user needs and aims to provide the user with a streamlined data science experience. Datawindow seeks to refocus the focus of the data analysis and data science process from coding and programming to allowing data workers to focus more on the creative processes and analytical thinking part of the analysis using interfaces. © 2024 IEEE.\",\n",
       " 'Agriculture constitutes a sector with a considerable environmental impact, a concern that is poised to increase with the projected growth in population, thereby amplifying implications for public health. Effectively mitigating and managing this impact demands the implementation of intelligent technologies and data-driven methodologies collectively called precision agriculture. While certain methodologies enjoy widespread acknowledgement, others, despite their lesser prominence, contribute meaningfully. This mini-review report discusses the prevalent AI technologies within precision agriculture over the preceding five years, with a specific emphasis on crop yield prediction and disease detection domains extensively studied within the current literature. The primary objective is to give a comprehensive overview of AI applications in agriculture, spanning machine learning, deep learning, and statistical methods. This approach aims to address a notable gap wherein existing reviews predominantly focus on singular aspects rather than presenting a unified and inclusive perspective. Copyright © 2024 Valleggi and Stefanini.',\n",
       " 'Abstract section not found',\n",
       " \"This article explores the integration of innovative data-driven technologies into digital governance at the local level, with a focus on open data, smart cities, and public sector data analytics and processing. Governments globally strive for digital transformation with emerging technologies, and data play a crucial role in improving service delivery and decision-making processes at the local level. However, there needs to be a proper debate about data analytics and data science in the public sector and the crucial aspects of smart cities’ interoperability and open data governance. This research aims to fill this gap, proposing a systematic approach to connect and link these innovations, promoting interoperability and data governance at the local level. Based on a multi-method literature analysis, including Portugal's remarkable digitalization journey, this study sheds light on the importance of comprehensive data analytics in the public sector. The findings indicate that the existing debate on data analytics in the public sector needs more depth and synergy from the point of view of data analytics techniques. By presenting propositions on the challenges for interoperability and open data governance of smart cities, this article provides valuable information for policymakers, decision-makers, and implementers looking for solutions to governance challenges at the local level. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.\",\n",
       " 'This article provides an overview of the use of data science and machine learning in technology and energy management. As data availability continues to increase and computing power improves, organizations and businesses are using this technology to uncover valuable insights, improve processes, and improve treatment decisions in the context of technology and energy management. This article outlines the various applications, challenges, and future directions of data science and machine learning in these fields. © 2024 IEEE.',\n",
       " 'Abstract section not found',\n",
       " 'Studiul explorează competențele academice și abilitățile profesionale cheie în domeniul științei datelor în contextul dezvoltării inteligenței artificiale, evidențiind importanța acestora în mediul de afaceri. Utilizând setul de date,, 2022 Stack Overflow Annual Developer Survey” și metode de învățare automată precum analiza în componente principale, clusterizarea K-means și regresia logistică, se analizează abilitățile profesionale în știința datelor. Obiectivele cercetării vizează distribuția locurilor de muncă în domeniu, nivelul de experiență, limbajele și programele de analiză utilizate, suportul oferit de companii și dinamica echipelor de știință a datelor, cât și impactul pe care inteligența artificială îl are asupra domeniului. Cu ajutorul acestora, se oferă o înțelegere cuprinzătoare a impactului pregătirii academice asupra oportunităților de carieră în domeniul științei datelor, contribuind la dezvoltarea profilului specialistului calificat în acest domeniu. Cercetarea oferă, de asemenea, indicații și recomandări relevante pentru îmbunătățirea abilităților necesare în știința datelor, cu scopul de a contura un profil calificat și pentru a răspunde cerințelor mediului de afaceri într-o lume dominată de analiza datelor și inteligență artificială. Prin includerea competențelor academice în procesul de formare a specialiștilor în știința datelor, cercetarea aduce inovație și evidențiază abilitățile necesare a fi formate în domeniul academic pentru a facilita angajarea absolvenților în domenii specifice științei datelor. Acest aspect este semnificativ deoarece, în practică, s-a observat că majoritatea specialiștilor care lucrează în știința datelor se bazează pe învățarea independentă mai degrabă decât pe competențele dobândite în cadrul academic. © 2024 Toate drepturile aparțin autorilor. All Rights Reserved.',\n",
       " 'The proceedings contain 158 papers. The topics discussed include: duct inspection and monitoring robot; deep learning-based approaches for preventing and predicting wild animals disappearance: a review; classification and tracking of items on a moving conveyor belt using convolutional networks and image processing; critical analysis of the 220/110/20 kV Sardanesti power substation from Romania in the context of identification elements of instability and insecurity; machine learning based collaborative prediction of SSD failures in the cloud; the impact of explainable ai on low-accuracy models: a practical approach with movie genre prediction; utilizing transfer learning-based algorithms for breast ultrasound data in multi-instance classification; predictive maintenance model-based on multi-stage neural network systems for wind turbines; and using teaching learning-based optimization with convolutional neural network to detect pneumonia based on chest X-Ray images.',\n",
       " 'Abstract section not found',\n",
       " 'Mental health issues, such as depression, anxiety, and stress, are prevalent among adolescents and Persons with Disabilities (PwDs), yet access to mental health professionals is limited, especially in rural areas. A screening application for mental health issues in adolescents and PwDs is developed to address this challenge, utilizing Flutter, Dart, and a fine-tuned NLP model of GPT-4. The application consists of a questionnaire module that analyzes user responses, a Results module providing mental health assessments, and a Support and resources module offering over 100+\\u2009healthcare resources. Data on mental health issues faced by adolescents is collected from Kilpauk Medical Hospital (KMC) and Velammal Medical College to inform the application’s development. The GPT-4 model is fine-tuned for phobia screening, resulting in an 80% accuracy rate in identifying phobia types and levels. The application effectively diagnoses various mental health issues, including anxiety disorders, bipolar disorder, depression, eating disorders, obsessive–compulsive disorder, post-traumatic stress disorder, schizophrenia, and various phobias. The application holds promise in enabling early identification and intervention for mental health issues, particularly in underserved areas. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.',\n",
       " 'Data science is a multidisciplinary area that gathers several branches, such as statistics, databases, and computer science and whose importance has become more substantial over the last few years. Using several techniques and algorithms from machine learning allows us to understand how certain variables are related, as well as to visualize data and make predictions. This paper aims to use data science as a strategic instrument for the hospitality industry by proposing a model that can help to predict which characteristics will be more valued by guests. By better understanding which features guests value most when evaluating a stay at a hotel, it will be easier for hotel managers to make informed decisions about which service operations management strategies should be used. It can also be helpful in terms of investment decisions, as it can indicate which aspects will be most important to value in a hotel. In this research, it was possible to conclude that guests’ ratings are related primarily to the commodities available at the hotels, followed by cleanliness, staff, location, price-quality relation, and comfort. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.',\n",
       " '[No abstract available]',\n",
       " \"The proceedings contain 15 papers. The special focus in this conference is on Big Data, Cloud Computing, and Data Science Engineering. The topics include: A Study on the Integration of Endpoint Security Service Operations Management: Focusing on Cloud Services; a Study on the Factors Affecting the Korean Financial Institution’s Switching Intention to Open Source Software: Focused on System Software; A Study on the Intention to Utilize Overseas Developers Through Offshoring—Using the Value-Acceptance Model (VAM); the Role of Co-Creation Experience and Switching Cost in the Relationship Between Service Recovery and Customer Loyalty; A Comprehensive Analysis of Security Measures for MyData in South Korea Based on AHP; a Study on the Factors Influencing the Performance of Korea Venture Capital Funds; a Research on Factors Influencing the Survival of Small Businesses: Focusing on Franchise Convenience Stores; a Study on Purchase Intention for Innovative Products: Focusing on Oxygen-Generating Air Purifiers; The Impact of Company's ESG Activities on Corporate Reputation; The Impact of ESG Activities in Midsize Manufacturing Companies on Purchase Intentions: Focusing on the Mediating Roles of Corporate Reputation, Brand Image, and Perceived Quality; A Study on the Intention to Utilize Overseas Developers Through Offshoring Approach and Strategy for Economic Cooperation Between South and North Korea Using the Value-Acceptance Model (VAM)—Based on the Case Study of the Kaesong Industrial Complex; a Study on the Intention to Use Home Network Services; A Study on the Impact of Green Patent Data on ESG Environment Indicators.\",\n",
       " 'In today’s internet-driven world, fake news is a problem that is only becoming worse. Given the ease of exchanging information online, separating false information from reliable information is a crucial endeavor. Using bag-of-words and consecutive mining approaches, we provide a data mining solution in this work to categorize articles as genuine or fake. We also compare the accuracy of the solution for identifying fake news across different datasets. Our method first purifies the input information by normalizing words and eliminating “filler” words. The cleansed data is then vectorized using\\xa0sequential mining techniques. After that, it uses vectorized data to train the classification models and categorizes unknown news as authentic or fake. Assessment of our technology to mine and categorize bogus news using actual data demonstrates its viability. The classification algorithms are then trained using vectorized data to categorize unreported news as real or bogus. The effectiveness of our technology in identifying and categorize bogus news has been evaluated using real-world data. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.',\n",
       " 'The technologies and innovations accompanying the Fourth Industrial Revolution are reshaping industries worldwide; the Architecture, Engineering, and Construction (AEC) industry is no different. This study specifically investigates the application of data science in the US commercial construction industry, which has been relatively limited and understudied. This study assessed the current state of data science utilization, identified potential benefits and challenges, and explored future directions. Semi-structured interviews were carried out with representatives from eight general contracting firms listed in the ENR top 100 contractors. Thematic analysis of the interview transcripts revealed several prominent themes. The findings indicate that while technical support for data storage and management exists, challenges persist in terms of data consistency and leveraging data for advanced analyses. Encouragingly, top management is recognizing the importance of data science in their daily operations, leading to a growing trend of hiring individuals with specialized data skills in construction companies. The establishment of dedicated data science departments and fostering, mitigating the current challenges of data consistency and interoperability with a mind-set of continuous improvement are viewed as strategic approaches to harness data for a competitive edge in the commercial construction industry. © 2024 Associated Schools of Construction.',\n",
       " 'The article “Revisiting Alwyn H. Gentry’s forest transect data: latitudinal beta diversity patterns are revealed using a statistical sampling-model-based approach”, written by Anne Chao · Chun-Huo Chiu · Kai-Hsiang Hu · David Zelený was originally published Online First without Open Access. After publication in volume 6, issue 2, page 861–884 the author decided to opt for Open Choice and to make the article an Open Access publication. Therefore, the copyright of the article has been changed to © The Author(s) 2024 and the article is forthwith distributed under the terms of the Creative Commons Attribution 4.0 International License (https://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The original article has been corrected. © The Author(s) under exclusive licence to Japanese Federation of Statistical Science Associations 2024.',\n",
       " \"Delivering a data science project encompasses several hurdles that need to be addressed. As the project matures, the business requirements may change over time. In addition, uncertainties associated with data integrity, quantity, and quality can have an impact on the success of the project. The effectiveness of advanced analytics and algorithms that is changing depending on the complexity of the project and ambiguities in project values potentially will lead to adverse effects on deliverables and task prioritization. Agile-scrum framework enables projects with time-boxed iterations (sprints). It also introduces delivery through increments (MVPs) that assist in reaching the overall aim or vision of the product. However, backlog prioritization and sequence of tasks is not bounded by any criteria and depends fully on product owner's understanding of product goal and value. On the other hand, CRISP-DM is a solid place to start for advising developers on the steps and tasks needed to build a data science product. It enables exploratory and discovery work through iterations to satisfy the requirements of the data science project. However, the lack of time element within the process might cause infinite iterative cycles and delay delivery to customers. At Petronas, we have integrated a hybrid strategy that envelops the CRISP-DM process within defined time-limited sprints. The process flow from CRISP-DM can help to plan which tasks to be assigned in which sprint. Properly assigned scrum team roles will ensure proper establishment of scrum. Furthermore, conducting scrum events will enable effective and productive customers engagement. Periodic inspection of scrum artifacts will also ensure alignment with product goals. This hybrid approach demonstrates how the change in requirements can be strategically addressed by utilizing the Agile-Scrum CRISP-DM methodology while ensuring that the product goal is achieved. It also highlights how Agile-Scrum ensures successful delivery of product, maximizing product value, and customer satisfaction, while CRISP-DM can guide us in planning for data science project sprints. Copyright 2024, Society of Petroleum Engineers.\",\n",
       " 'Precision Medicine and Precision Public Health are approaches to improve population health. Achieving these goals requires innovation in health informatics. The Centre for Health Informatics (CHI) within the Cumming School of Medicine (CSM) at the University of Calgary (UC), Canada, was created to respond to this need by fostering multidisciplinary collaborations, building capacity by recruiting and training outstanding faculty and students, and harnessing Alberta’s rich health data to advance health informatics. To establish CHI as a health informatics leader, CHI has struck partnerships with stakeholders, including Alberta Health Services (AHS), Alberta Health (AH), and the Alberta Strategy for Patient-Oriented Research Unit (AbSPORU) among others. Through these close relationships, the CHI intake team facilitates access to Alberta’s rich health data sources and educates researchers on the available health data in Alberta. The concept of a “One Stop Shop” for CSM and UC researchers encourages multidisciplinary collaboration, helps investigators access a wide range of datasets, and receive analytical support. Population-based data sets enable the development of methods to turn raw data into health information, improve health data collection, linkage, analysis, and quality, and applied studies creating clinical decision-support tools, prognostic tools, improved health surveillance methods, and health system performance indicators. CHI’s ecosystem of diverse research expertise, cutting-edge technology, and embedded AHS analysts to support data access via a wide-ranging network of partnerships allows our provincial researchers, national and international collaborators tremendous opportunities for empirical research. It paves the way for implementing Precision Medicine in the real world. 2024 © The Authors.',\n",
       " 'Data Science and data-driven Artificial Intelligence are here to stay and they are expected to further transform the current global economy. From a technical point of view, there is an overall agreement that disciplines based on data require to combine data engineering and data analysis skills, but the fact is that data engineering is nowadays trailing and catching up with the rapid changes in the data analysis landscape. To unleash the real power of data, data-centric systems must be professionalized, i.e., operationalized and systematized, so that repetitive, time-consuming and error-prone tasks are automated. To such end, we propose our vision on next generation data governance for data-centric systems based on knowledge graphs. We claim that without the knowledge embedded in the data governance layer, Data Science will not unleash its potential. © 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).',\n",
       " 'Many-Objective Feature Selection (MOFS) approaches use four or more objectives to determine the relevance of a subset of features in a supervised learning task. As a consequence, MOFS typically returns a large set of non-dominated solutions, which have to be assessed by the data scientist in order to proceed with the final choice. Given the multi-variate nature of the assessment, which may include objectives (e.g., fairness) unrelated to predictive accuracy, this step is often not straightforward and suffers from the lack of existing tools. For instance, it is common to make use of a tabular presentation of the solutions, which provides little information about the trade-offs and the relationships between objectives over the set of solutions. Adopting a GA-based MOFS with six objectives (number of selected features, balanced accuracy, F1-Score, variance inflation factor, statistical parity, and equalised odds) for two feature selection tasks, this paper illustrates the complex challenge of assessing MOFS results and the need for a methodology to aid and justify the final choice of a solution. © 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).',\n",
       " 'Abstract section not found',\n",
       " '[No abstract available]',\n",
       " 'Even though working with data is as important as coding for understanding and dealing with complex problems across multiple fields, it has received very little attention in the context of Computational Thinking. This paper discusses an approach for bridging the gap between Computational Thinking with Data Science by employing and studying classification as a higher-order thinking process that connects the two. To achieve that, we designed and developed an online constructionist gaming tool called SorBET which integrates coding and database design enabling students to interpret, organize, and analyze data through game play and game design. The paper presents and discusses the results of a pilot study that aimed to investigate the data practices secondary students develop through playing and modifying SorBET games, and to determine the impact of game modding on student critical engagement with CT. According to the results, students developed and used certain data practices such as data interpretation and data model design to become better players or to design an interesting classification game. Moreover, game modding process motivated students to question the original games’ content, leading them to develop a critical stance towards the game data model and representations. © 2024 Vilnius University',\n",
       " 'Abstract section not found',\n",
       " 'The effect of weld geometry on fatigue properties of flame-resistant magnesium alloy welded joints was evaluated using computational and data science methods. First, a workflow for fatigue life prediction of flame-resistant magnesium alloy welded joints was proposed. The workflow consists of thermo-elastic-plastic analysis simulating welding, macroscopic stress field analysis to identify stress concentration zones, crack initiation analysis using crystal plasticity analysis and Tanaka-Mura model, and crack propagation analysis using finite element method and Paris’ law. The fatigue life calculations were repeatedly performed by using the proposed workflow for different shapes of weld toe, excess weld metal and undercut, and the results were compiled into a database. A surrogate model was developed from the database by machine learning to predict the fatigue indicator parameter (FIP), which is an index of crack initiation life obtained by the Tanaka-Mura model. The effect of weld geometry on fatigue life was evaluated by exploring the surrogate model using a Markov chain Monte Carlo method. The results obtained are consistent with the previous findings that toe radius and undercut depth have a strong influence on fatigue. © 2024 Japan Institute of Light Metals. All rights reserved.',\n",
       " 'The proceedings contain 34 papers. The special focus in this conference is on Machine Learning, Optimization, and Data Science. The topics include: Alternating Mixed-Integer Programming and Neural Network Training for Approximating Stochastic Two-Stage Problems; heaviest and Densest Subgraph Computation for Binary Classification. A Case Study; SMBOX: A Scalable and Efficient Method for Sequential Model-Based Parameter Optimization; accelerated Graph Integration with Approximation of Combining Parameters; improving Reinforcement Learning Efficiency with Auxiliary Tasks in Non-visual Environments: A Comparison; a Hybrid Steady-State Genetic Algorithm for the Minimum Conflict Spanning Tree Problem; reinforcement Learning for Multi-Neighborhood Local Search in Combinatorial Optimization; evaluation of Selected Autoencoders in the Context of End-User Experience Management; application of Multi-agent Reinforcement Learning to the Dynamic Scheduling Problem in Manufacturing Systems; solving Mixed Influence Diagrams by Reinforcement Learning; exploring Image Transformations with Diffusion Models: A Survey of Applications and Implementation Code; multi-scale Heat Kernel Graph Network for Graph Classification; PROS-C: Accelerating Random Orthogonal Search for Global Optimization Using Crossover; a Multiclass Robust Twin Parametric Margin Support Vector Machine with an Application to Vehicles Emissions; LSTM Noise Robustness: A Case Study for Heavy Vehicles; ensemble Clustering for Boundary Detection in High-Dimensional Data; learning Graph Configuration Spaces with Graph Embedding in Engineering Domains; towards an Interpretable Functional Image-Based Classifier: Dimensionality Reduction of High-Density Diffuse Optical Tomography Data; on Ensemble Learning for Mental Workload Classification; decision-Making over Compact Preference Structures; user-Like Bots for Cognitive Automation: A Survey; geolocation Risk Scores for Credit Scoring Models; On Channel Selection for EEG-Based Mental Workload Classification.',\n",
       " 'In this chapter, we propose a non-traditional RCR training in data science that is grounded in a virtue theory framework. First, we delineate the approach in more theoretical detail by discussing how the goal of RCR training is to foster the cultivation of certain moral abilities. We specify the nature of these ‘abilities’: while the ideal is the cultivation of virtues, the limited space allowed by RCR modules can only facilitate the cultivation of superficial abilities or proto-virtues, which help students to familiarize themselves with moral and political issues in the data science environment. Third, we operationalize our approach by stressing that (proto-)virtue acquisition (like skill acquisition) occurs through the technical and social tasks of daily data science activities, where these repetitive tasks provide the opportunities to develop (proto-)virtue capacity and to support the development of ethically robust data systems. Finally, we discuss a concrete example of implementing this approach. In particular, we describe how this method is applied to teach data ethics to students participating in the CODATA-RDA Data Science Summer Schools. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " '[No abstract available]',\n",
       " 'The spread of the novel coronavirus disease, SARS-CoV-19 (COVID-19), has affected human activities everywhere, resulting in fear and panic among all age groups. Hence, this study implements a novel data science process to empirically model the daily reported cases and Google search queries in 14 countries. We observed a strong positive association (0.79-0.96) among reported cases of COVID-19 in the 14 countries. Furthermore, there is an inverse correlation of -0.18 to -0.62 between information diffusion on the virus and reported cases (new cases and deaths). Our outcome shows that contagious diseases are highly predictable using historical records from other countries and information spread on the disease. © 2024 Inderscience Enterprises Ltd.',\n",
       " 'The analysis of large volumes of information has become necessary for decision-making in companies, creating new roles and capacities that must be covered. To meet this demand, institutes and universities have created curricular programs that satisfy this need; however, the coverage of these programs is nonstandard and leaves important topics uncovered. This paper compares a set of academic curricula inside and outside of Costa Rica based on the report Computing Competencies for Undergraduate Data Science Curricula-ACM Data Science Task Force. Likewise, the results of a survey with professionals from different industries to identify the topics related to data science that they learned in formal education, work or that they would need to know, providing an overview of the issues that should strengthen or add within the academic curricula analyzed. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " \"Large-scale medical data sets are vital for hands-on education in health data science but are often inaccessible due to privacy concerns. Addressing this gap, we developed the Health Gym project, a free and open-source platform designed to generate synthetic health data sets applicable to various areas of data science education, including machine learning, data visualization, and traditional statistical models. Initially, we generated 3 synthetic data sets for sepsis, acute hypotension, and antiretroviral therapy for HIV infection. This paper discusses the educational applications of Health Gym's synthetic data sets. We illustrate this through their use in postgraduate health data science courses delivered by the University of New South Wales, Australia, and a Datathon event, involving academics, students, clinicians, and local health district professionals. We also include adaptable worked examples using our synthetic data sets, designed to enrich hands-on tutorial and workshop experiences. Although we highlight the potential of these data sets in advancing data science education and health care artificial intelligence, we also emphasize the need for continued research into the inherent limitations of synthetic data. © 2024 JMIR Publications Inc.. All rights reserved.\",\n",
       " \"This study aims to explore the intricacies of entrepreneurial development within the realm of data science, shedding light on both internal and external factors that play pivotal roles in shaping the entrepreneurial landscape. Through the lens of the CCIP-PF model and employing rigorous statistical analysis, this research endeavors to provide insights crucial for fostering entrepreneurial growth in this dynamic field. The objectives of this study are: 1）To develop the CCIP-PF model and establish an assessment index system for mental health literacy training in junior high schools; and 2）To apply the evaluation index system to junior high school mental health literacy training, thereby promoting the enhancement of educational quality. The sample group consisted of 17 experts who participated in discussions and generated 162 viewpoints on the constituent elements of evaluation for junior high school mental health literacy training. The methodology employed the Delphi method; the instrument utilized a qualitative assessment approach, employing questionnaires to ensure anonymity and provide reliable feedback. A questionnaire survey was conducted among 422 participants in Sichuan Province's relevant educational administrative authorities, middle school mental health education teachers, university lecturers and professors in mental health education, and psychological counselors. The response rate reached 96.2%. The study analyzed the data using mathematical statistics and SPSSAU22.0, focusing on the reliability of the entire questionnaire and its dimensions. The findings of this study are as follows:1）primary indicators at mean 4.794, SD = 0.473, IQR = 0.125; secondary indicators at mean 4.823, SD = 0.379, IQR = 0.25; tertiary indicators at mean 4.790, SD = 0.424, IQR = 0.302. A factor contribution rate of 74.175% demonstrates efficacy. 2）Empirical research was conducted in various districts of Zigong City, yielding outcomes that align with reality and meet the anticipated objectives. © Authors retain all.\",\n",
       " 'The Data Mine at Purdue University is a pioneering experiential learning community for undergraduate and graduate students of any background to learn data science. The first data-intensive experience embedded in a large learning community, The Data Mine had nearly 1300 students in academic year (AY) 2022–2023 and nearly 1700 students for AY 2023–2024. The Data Mine embodies data-infused education, research, and collaboration. Students learn Python, R, SQL, and shell-scripting, while working on weekly projects within a high-performance computing (HPC) cluster. In the Corporate Partners cohort, students work on teams of 5–15 students, led by a paid student team leader. Each cohort follows an Agile approach, working on data-intensive projects provided by industry partners and mentored by company employees. Students develop professional and data skills throughout the academic year, from August through April. Many students return in subsequent years to the program, increasing their tenure with a Corporate Partner. Student teams are inherently interdisciplinary; students from 133 different majors are involved in the program, ranging from new incoming students through PhD level students. These interdisciplinary teams of students bring new perspectives to challenging problems in which data science is a key part of the solution. The interdisciplinary teams foster an environment of synthesis with ideas and solutions. Students come together with different life experiences, different levels of technical skill, but also varying ways they navigate paths to solutions because of the variety of majors represented, resulting in a more creative and robust solution than a traditional data science program. This article is categorized under: Applications of Computational Statistics > Education in Computational Statistics. © 2024 The Authors. WIREs Computational Statistics published by Wiley Periodicals LLC.',\n",
       " 'The proceedings contain 50 papers. The special focus in this conference is on Data Science, Machine Learning and Blockchain Technology. The topics include: An unsupervised approach to creating a restaurant recommendation system; Classification of alzheimer’s disease using D-DEMNET framework; comparison of machine learning and deep learning methods for detection of liver abnormality; soil micronutrient detection using machine learning; a review of tracking concept drift detection in machine learning; wearable electrogastrogram perspective for healthcare applications; computer vision based home automation; Early autism detection using ML on behavioural pattern; implementation of application prototypes for human-to-computer interactions; recommendation system for anime using machine learning algorithms; predicting bitcoin price fluctuation by Twitter sentiment analysis; microarchitecture design and verification of co-processor for floating point operation; blockchain based higher education ecosystem; document verification using blockchain; blockchain-based traceability system for readymade food products; a cloud based interactive framework for emergency medical data sharing; the analysis and interpretation of higher education teachers based on student and teachers feedback; Implementing AI on microcontrollers in fog and edge architectures; abnormality detection in chest radiograph using deep learning models; a comprehensive review on hate speech recognition utilizing natural language processing and machine learning; prevalence of migraine among collegiate students in greater Noida; Indoor navigation using BLE beacons; strategic health planner and exercise suggester; perspective of deep learning strategies for analysis of 1D biomedical signals; revolution in agriculture sector using blockchain technology; customer churn prediction using ensemble learning with neural networks; Securing crime case summary and E-FIR using blockchain concept.',\n",
       " 'This paper initially exposed the advantages of the Oracle Accelerated Data Science (ADS) SDK in the larger context of OCI features. The focus was on explaining the configuration steps for a machine learning life cycle. The most important steps related to environment setup and clarifying OCI and data science concepts were highlighted. To test the features of ADS SDK, a public dataset on higher education students’ performance evaluation with 33 attributes and 145 rows was used. I have described and explained the most important steps for a machine learning pipeline with ADS SDK, completing with the features selected, based on machine learning explainability computing. The advantage of ADS SDK, Conda environments, can be further exploited in my next machine learning experiments. To cross-validate the results provided by ADS SDK, I created an automated machine learning pipeline in Microsoft Automated ML, presented the results, and commented on the comparison between both technologies. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.',\n",
       " 'Electron backscatter diffractionElectron Backscatter Diffraction (EBSD) method is widely adopted in metal fields. However, despite the abundant data sources, sufficient analysis covering all features is often absent. Especially with the emerging in-situ techniques, data processingData processing is time-consuming, where access to every bit of data is imperative. In this work, a toolkit is developed with the aim of processing EBSDElectron Backscatter Diffraction (EBSD) data automatically and efficiently. Two parts of toolkits are developed with Matlab and Mtex. One is used to correlate two maps, with simple implementation, results will generate within few minutes, indicating the grains correlation between two maps. The other correlates a series of in-situ datasets, making each individual grain become trackable. With the assistance of the toolkits, a large dataset containing pixels, digital information, and grains properties through an in-situ process can be created. Thus, the microfeatures and grain behaviors are studied using novel data science methods, especially machine learning and deep learning. © The Minerals, Metals & Materials Society 2024.',\n",
       " 'This article builds on discussions on peer review in science and the role of ethics in the governance of technologies to achieve a two-fold goal. First, it shows the process of co-production of the European Commission’s (EC’s) standardised ‘ethics appraisal process’ with the development of the EC’s Framework Programmes (FPs), by looking at the standardisation of the process and the mutual shaping of data protection law and risk-based discourses around Artificial Intelligence. Second, it investigates the political and epistemic implications arising from this process for the EU governance of scientific research regarding (1) sponsorship of science and technology by the EU and (2) how many limitations or constraints researchers have in practice when carrying out their research. After sketching the origins of the ethics appraisal process and its co-production with data protection law and risk-based jargon, the article outlines the implications for the governance of scientific research and draws recommendations. © 2024 Informa UK Limited, trading as Taylor & Francis Group.',\n",
       " \"Capital adequacy ratio is an important indicator when evaluating banks' safety and risk management activities. Therefore, commercial banking system is increasingly moving towards international standards is a prerequisite to ensure the reduction of systemic risks in banking. Besides, commercial banks and the financial industry face considerable hurdles in light of the fourth Covid-19 outbreak. Commercial banks continuously put capital adequacy measures in place to fulfill Basel regulations. One of the main ways they do this is by issuing bonds, which boost tier 2 capital sources. This helps mobilize capital and assure capital safety for the market's borrowing requirements in the long run. As a result, considering both external and internal variables, this research seeks to investigate what influences the capital adequacy ratio of Vietnam's joint-stock commercial banks. Between 2011 and 2022, the authors combed through data from 25 different Vietnamese joint-stock commercial banks. The authors employed the system generalized method of moments model and other conventional techniques for panel data analysis. The study's findings had fourteen components affecting the capital adequacy ratio, with a significance of 0.01. Therefore, it is evident that the equity capital of Vietnamese commercial banks has successfully met the required safety standards for assets with credit risk as per legislation. As a result, this assists Vietnamese commercial banks in managing potential losses from credit activities, thus assuring the security of banking operations and protecting depositors. However, the issue suggests policy implications for enhancing Vietnamese commercial banks' future capital adequacy ratio coefficient. © Authors retain all copyrights.\",\n",
       " 'Technological advances such as Learning Management System (LMS) are changing the teaching-learning conditions, organization of school activities and functions of educators. In particular, the use of LMS in the educational field is necessary due to the appearance of the SARS-CoV-2 virus. This quantitative research analyzes the teachers’ perception about the use of LMS during the COVID-19 pandemic considering data science. The sample is 115 teachers from the National Autonomous University of Mexico. These teachers took the “Classroom of the Future 2020” Diploma in order to create new educational spaces. The results of the machine learning technique indicate that the performance of the school activities in LMS positively influences the learning process, motivation and participation of the students during the COVID-19 pandemic. Also, the decision tree technique identifies three predictive models about the use of this technological tool in the educational field considering the academic level and sex of the teachers. In conclusion, educators can improve the learning conditions, organize creative activities inside and outside the classroom, achieve the innovation in the educational context and build virtual spaces through LMS. © 2024, Anadolu Universitesi. All rights reserved.',\n",
       " 'The digitization of learning resources has led to an increase in specialized collaboration platforms across various fields, including the need for manufacturing companies to develop and maintain expertise in Industrial Data Science (IDS). This paper presents an approach to integrating collaborative and competency-based needs specific to industrial data analytics into a functional collaboration platform. We define the unique requirements of IDS projects and translate them into platform features. These features are then implemented and tested in an online platform within a research project, validating their effectiveness in a dynamic value network setting. The platform’s primary innovation lies in its tailored design for IDS project practitioners from diverse domains, ensuring sustainable integration of data analytics in industrial settings. The initial version of this collaborative platform is currently accessible online and undergoing validation. © 2024 Universidad Politecnica de Valencia. All Rights Reserved.',\n",
       " \"This chapter proposes an overarching norm for polycentric digital data governance. It argues that questions should also be given more consideration as a device for modern data responsibility. Traditionally, questions have typically been seen solely as a device for inquiry. We suggest, however, that designing a process for asking the right questions can play an important role in ensuring that data is used responsibly and with maximum positive social impact. More generally, we argue that, combined with other methods and approaches, questions can help achieve a variety of key data responsibility goals, including data minimization and proportionality, increasing participation and enhancing accountability. In addition to 'data science,' we need to invest in creating a new kind of 'question science’ that can contribute to data responsibility. These observations are supplemented by the author’s own experience, as founder and lead of the '100 Questions Initiative,' an effort to help determine the most important questions across a variety of fields that could be answered if data were made more readily available to trusted parties. © 2024 selection and editorial matter, Carolina Aguerre, Malcolm Campbell-Verduyn and Jan Aart Scholte; individual chapters, the contributors.\",\n",
       " 'The proceedings contain 42 papers. The special focus in this conference is on Data Science and Applications. The topics include: Analyzing Blockchain Data to Detect Bitcoin Addresses Involved in Illicit Activities Using Anomaly Detection; Comparing Spring Boot and ReactJS with Other Web Development Frameworks: A Study; Performance Analysis of InceptionV3, VGG16, and Resnet50 Models for Crevices Recognition on Surfaces; a Machine Learning Approach for Moderating Toxic Hinglish Comments of YouTube Videos; instant Accident Detection and Emergency Alert System; A Novel Approach to Video Summarization Using AI-GPT and Speech Recognition; classification of Underwater Fish Species Using Custom-Built Deep Learning Architectures; deep Learning-Based Approach for Plant Disease Classification; prediction of Liver Disease Using Machine Learning Algorithms; a Real-Time Cataract Detection and Diagnosis Through Web-Based Imaging Analysis; Analysis of Detection of Glioma by Segmentation of Brain Tumor MRI Images Using Deep Learning; CloneAI: A Deep Learning-Based Approach for\\xa0Cloned Voice Detection; Analyzing the Performance and Wireless Network Capacity of NOMA: Study of the Impact of OMA and NOMA on 5G Network; malaria Parasite Detection Using Deep Neural Networks; ioT-Based Agriculture: Identification and Classification of Apple Quality Using Deep Learning; plant Disease Detection on\\xa0Edge Devices; analyze the Quality of Wine Based on Machine Learning Approach; safeguarding Financial Transaction with Cryptocurrency; analysis and Control of Dual Active Bridge Converter for Vehicle to Load Application in Electric Vehicle; efficient Plant Leaf Disease Detection Using a\\xa0Customized Convolutional Neural Network; a Survey of\\xa0Decentralized Digital Voting System Using Blockchain Technology; An Explainable AI (XAI)-Based Framework for\\xa0Detecting Diseases in\\xa0Paddy Crops.',\n",
       " 'As data continues to grow exponentially, knowledge of data science and machine learning has become more crucial than ever. Machine learning has grown exponentially; however, the abundance of resources can be overwhelming, making it challenging for new learners. This book aims to address this disparity and cater to learners from various non-technical fields, enabling them to utilize machine learning effectively. Adopting a hands-on approach, readers are guided through practical implementations using real datasets and SAS Enterprise Miner, a user-friendly data mining software that requires no programming. Throughout the chapters, two large datasets are used consistently, allowing readers to practice all stages of the data mining process within a cohesive project framework. This book also provides specific guidelines and examples on presenting data mining results and reports, enhancing effective communication with stakeholders. Designed as a guiding companion for both beginners and experienced practitioners, this book targets a wide audience, including students, lecturers, researchers, and industry professionals from various backgrounds. © 2024 Dothang Truong.',\n",
       " 'Nowadays, information technology (IT) has been holding a significant role in daily life worldwide. The trajectory of data science and bioinformatics promises pioneering personalized therapies, reshaping medical landscapes and patient care. For RNA therapy to reach more patients, a comprehensive understanding of the application of data science and bioinformatics to this therapy is essential. Thus, this chapter has summarized the application of data science and bioinformatics in RNA therapeutics. Data science applications in RNA therapy, such as data integration and analytics, machine learning, and drug development, have been discussed. In addition, aspects of bioinformatics such as RNA design and evaluation, drug delivery system simulation, and databases for personalized medicine have also been covered in this chapter. These insights have shed light on existing evidence and opened potential future directions. From there, scientists can elevate RNA-based therapeutics into an era of tailored treatments and revolutionary healthcare. © 2024',\n",
       " 'Random forest (RF) is one of the most popular statistical learning methods in both data science education and applications. Feature selection, enabled by RF, is often among the very first tasks in a data science project, such as the college capstone project, industry consulting projects. The goal of this paper is to provide a comprehensive review of 12 RF-based feature selection methods for classification problems. The review provides necessary description of each method and the software packages. We show that different methods typically do not provide consistent feature selection results, and the model performance also varies when different RF-based feature selection approaches are employed. This observation suggests that caution must be taken when performing feature selection tasks using RF. Feature selection cannot be blindly done without a sound understanding of the methods adopted, which is not always the case in industry and many senior capstone projects that we have observed. The paper serves as a one-stop reference where students, data science consultants, engineers, and data scientists can access the basic ideas behind these methods, the advantages and limitations of different approaches, as well as the software packages to implement these methods. © 2024, The Author(s), under exclusive licence to Springer Nature Switzerland AG.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " \"Request failed: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\",\n",
       " \"The Internet of Things (IoT) is a rapidly expanding field of research and application that focuses on creating networks of physical objects embedded with sensors, software, and connectivity. The number of IoT devices continues to grow, there is a corresponding surge in the amount of data they generate. This data presents opportunities for gaining valuable insights and making better decisions across various sectors, such as healthcare, agriculture, business, logistics, and manufacturing. However, the growing volume, speed, and diversity of IoT data present significant challenges for processing and analysis using traditional methods. This is where the role of data science methods becomes crucial. In this chapter, we examine existing frameworks and overarching methodologies widely employed in the field of data science and data analytics. Drawing inspiration from these, we introduce the Data Science Life Cycle for IoT Applications (DSLC-IoT), which serves as a fundamental guideline for addressing data-intensive and data-driven challenges in the realm of IoT. Subsequently, we delve into an exploration of various data science methods utilized to enhance data processing within the context of IoT. The review centers on the application areas of healthcare, agriculture, business, logistics, and manufacturing. These areas have significant social and economic importance and are closely intertwined with people's lives. © 2024 Elsevier Inc.\",\n",
       " 'Abstract section not found',\n",
       " 'Data science is assuming a pivotal role in guiding reaction optimization and streamlining experimental workloads in the evolving landscape of synthetic chemistry. A discipline-wide goal is the development of workflows that integrate computational chemistry and data science tools with high-throughput experimentation as it provides experimentalists the ability to maximize success in expensive synthetic campaigns. Here, we report an end-to-end data-driven process to effectively predict how structural features of coupling partners and ligands affect Cu-catalyzed C–N coupling reactions. The established workflow underscores the limitations posed by substrates and ligands while also providing a systematic ligand prediction tool that uses probability to assess when a ligand will be successful. This platform is strategically designed to confront the intrinsic unpredictability frequently encountered in synthetic reaction deployment. © 2024 American Association for the Advancement of Science. All rights reserved.',\n",
       " 'Understanding, modeling, and predicting student performance in higher education poses significant challenges concerning the design of accurate and robust diagnostic models. While numerous studies attempted to develop intelligent classifiers for anticipating student achievement, they overlooked the importance of identifying the key factors that lead to the achieved performance. Such identification is essential to empower program leaders to recognize the strengths and weaknesses of their academic programs and thereby take the necessary corrective interventions to ameliorate student achievements. To this end, our paper contributes, to begin with, a hybrid approach of factor analysis that combines various data science approaches. The prediction of student performance is produced by combining baseline models, cross-validation, and factor analysis. We empirically investigate and demonstrate the effectiveness of our entire approach on four datasets. The experimental results show considerable improvements compared to single baseline models, demonstrating the practicality of the proposed approach in pinpointing multiple factors impacting student performance. The result proves that the hybrid algorithm combining cross-validation and factor analysis approaches yields results that are far superior in terms of achieving accuracy in prediction of academic performance of the students. The model may be successfully extended to other programs to predict the performance of the students. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.',\n",
       " 'Data science in healthcare has made great progress using data analysis and machine learning methods that have the potential to detect and help solve healthcare problems. After mortality and during morbidity, relevant data about a health problem have been gathered. This massive amount of data in various forms needs to be handled for any healthcare issues are significant. With the growth of big data in healthcare communities, accurate analysis of medical data has the benefits of early disease detection, improved patient care, and effective community services. Because of its significance, there is a need to develop efficient and better-performing data analytics techniques and tools to analyze medical big data from the gene level to the clinical level. The purpose of data analytics in healthcare is to find new insights in data, at least partially automate tasks such as diagnosing, and to facilitate clinical decision-making. Also, healthcare analytics has the potential to reduce costs of treatment, predict outbreaks of disease, avoid preventable diseases, and improve the quality of life in general. The average human lifespan is increasing across the world population and the application of big data analytics in healthcare are increasing day by day. Different format of health data are used for different types of analyses. For example, IoT gadgets are used by certain patients and clinicians as wearable monitors to track heartbeat and temperature. This signal generated data should be carefully analyzed over time. Also, scans such as X-rays, magnetic resonance images (MRIs), and computed tomography (CAT) scans can be studied with different data analysis techniques and machine learning algorithms to visualize the insides of the body in depth. In addition, data on individual cases of disease are analyzed; data received as text must be sorted, categorized, and coded for statistical analysis; and data from surveys might need to be weighted to produce valid estimates for sampled populations. The number of resources healthcare professionals can obtain from their patients continues to increase. Since these data are normally in different formats and sizes, it can be difficult for analysis. However, the current focus is no longer on how big the data is, but how intelligently it is managed by data analysis techniques and machine learning algorithms. This study examines analytical techniques for different forms of health-related data to generate comprehensive healthcare reports and transform them into relevant critical insights that can then be used to provide better care. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.',\n",
       " \"This study introduces an innovative technique for the prediction of financial market movements by leveraging chaos theory principles. Employing time-delay embedding alongside attractor reconstruction, the research discerns critical structures within financial market time series data. The identification of these patterns facilitates the creation of a predictive model aimed at forecasting forthcoming market behaviours. The findings of the research acknowledge the persistent challenge posed by the unpredictable nature of financial markets; however, the application of a chaos theory framework offers valuable perspective into the intricate mechanisms governing these sophisticated systems. This paper's approach highlights the potential of chaos theory as a tool in deciphering and anticipating the fluctuations of financial markets, thereby contributing to the fields of economic forecasting and financial analyse. © 2024, Ismail Saritas. All rights reserved.\",\n",
       " 'This paper arises from a research-based teaching project (“Die Biographie eines Heiligtums. Computergestützte Visualisierung und Analyse mit georeferenzierten Datenmodellen”), held during the summer term 2019 at the Ludwig-Maximilians-Universität München. Based on a case study focussing on the area of Sounion and the Laureotike, with the authors, aided by a group of students, developed a data structure concept for the critical analysis of ancient sanctuaries that takes into account their landscape setting, cults and worshipped deities, as well as ancient worshippers. In order to facilitate the evaluation of large amounts of data sets, a preliminary GIS-based graph database was established and first data sets were entered. The paper presented here outlines the data structure as well as the configuration of the provisional tool and aims to illustrate how our understanding Greek religion is improved by large and highly diverse data sets. © 2024 The Author(s).',\n",
       " 'Data science consulting and collaboration units (DSUs) are core infrastructure for research at universities. Activities span data management, study design, data analysis, data visualization, predictive modelling, preparing reports, manuscript writing and advising on statistical methods and may include an experiential or teaching component. Partnerships are needed for a thriving DSU as an active part of the larger university network. Guidance for identifying, developing and managing successful partnerships for DSUs can be summarized in six rules: (1) align with institutional strategic plans, (2) cultivate partnerships that fit your mission, (3) ensure sustainability and prepare for growth, (4) define clear expectations in a partnership agreement, (5) communicate and (6) expect the unexpected. While these rules are not exhaustive, they are derived from experiences in a diverse set of DSUs, which vary by administrative home, mission, staffing and funding model. As examples in this paper illustrate, these rules can be adapted to different organizational models for DSUs. Clear expectations in partnership agreements are essential for high quality and consistent collaborations and address core activities, duration, staffing, cost and evaluation. A DSU is an organizational asset that should involve thoughtful investment if the institution is to gain real value. © 2024 The Authors. Stat published by John Wiley & Sons Ltd.',\n",
       " 'Applying agile practices in data science requires adaptations. This paper describes challenges and lessons learned in two applied machine learning projects developed in the XP Lab course at University of São Paulo in Brazil. It compiles six suggestions for educators and practitioners who want to bring agility to their data science initiatives. © 2024, The Author(s).',\n",
       " '[No abstract available]',\n",
       " \"Open innovation in data science generally takes the form of public competitions where teams exchange messages and solutions by competing and collaborating simultaneously. Team behaviours are widely heterogeneous in terms of the performance of their solutions and the participation in knowledge creation. We present a novel research framework for open innovation by integrating system dynamics and structural topic modelling to extract open factors and adopting a machine learning-based difference-in-differences estimator to understand the impact of team behaviour on their performance using data from Kaggle's competition. Our results identify four team behaviour categories—active, learner, lurker, and passive— in data science open innovation competitions which depend on the performance of their solutions and actions related to posting and reading messages in the forum. Furthermore, the activities of model evaluation, community support, and business understanding are the top three most positive and significant factors affecting team performance. Our research contributes to the literature by highlighting the value of forum feedback and exploring the data science activities in the forum discussion, in relation to innovation performance, to enrich the empirical understanding of open innovation. Research implications for researchers and practitioners participating in, organising, and supporting data science open innovation activities are provided. © 2023 The Authors\",\n",
       " 'The proceedings contain 39 papers. The special focus in this conference is on Data Science and Network Engineering. The topics include: Smart Surveillance System and Prediction of Abnormal Activity in ATM Using Deep Learning; a Framework for Extractive Text Summarization of Single Text Document in Tamil Language Using Frequency Based Feature Extraction Technique; an Approach to Mizo Language News Classification Using Machine Learning; BASiP: A Novel Architecture for Abstractive Text Summarization; a Hybrid Approach for Leaf Disease Classification Using Machine Learning and Deep Learning; enhancing Agricultural Decision-Making Through Machine Learning-Based Crop Yield Predictions; Pest Detection Using YOLO V7 Model; random Forest Classifier-Based Acute Lymphoblastic Leukemia Detection from Microscopic Blood Smear Images; FedCNNAvg: Federated Learning for Preserving-Privacy of Multi-clients Decentralized Medical Image Classification; training Algorithms for Mixtures of Normalizing Flows; acute Lymphoblastic Leukemia Detection Using DenseNet Model from Microscopic Blood Smear Images; a Disease Prediction Framework Based on Predictive Modelling; a Data-Driven Diabetes Predictive Model Using a Novel Optimized Weighted Ensemble Approach; analysis of Image Caechniques Using Encoder–Decoder; security and Energy Efficiency Enhancement for the Internet of Things: Challenges, Architecture and Future Research; spot Pricing in Cloud Computing: A Comprehensive Survey of Mechanisms, Strategies, and Future Directions; A Comparative Analysis of Propagation Models Suitable for Non-Line-of-Sight 5G Communication at 26 GHz; validating δ-Currency Using Model Checking; a Batch-Service Queueing Assisted Blockchain System for Supply Chain Management; biFrost: A Blockchain-Based Decentralized Messaging Application; priority Based Load Balancing for Intercloud Computing Environments; machine Learning Approach to the Internet of Things Threat Detection.',\n",
       " 'Data science is the driving force behind the significant technological and operational changes that cyber security is going through in the computer world. Identifying trends or insights regarding security occurrences in cyber security data and creating a data-driven model that correlates with them are essential elements in creating an automated and intelligent safety system. This means gathering data from pertinent cyber security sources and applying analytics to improve the most recent trends based on data. The article also highlights important variables that affect the design choices made for the control, communication, redundancy, and reliability of ICS, as these aspects are crucial in figuring out the security requirements of the system. Network segmentation, access control, patches management, and security monitoring are just a few of the security countermeasures that are currently in place. Additionally, the paper investigates how machine learning methods might be integrated to improve ICS cyber security. Subsequently, we list the pros and cons of the available security solutions, talk about how to secure industrial control systems (ICSs) and implement additional security measures (like risk assessment methodologies), point out unresolved security research issues related to ICSs, and make recommendations for future directions in ICS security research. © 2024, Ismail Saritas. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " \"Profiling data by plotting distributions and analyzing summary statistics is a critical step throughout data analysis. Currently, this process is manual and tedious since analysts must write extra code to examine their data after every transformation. This inefficiency may lead to data scientists profiling their data infrequently, rather than after each transformation, making it easy for them to miss important errors or insights. We propose continuous data profiling as a process that allows analysts to immediately see interactive visual summaries of their data throughout their data analysis to facilitate fast and thorough analysis. Our system, AutoProfiler, presents three ways to support continuous data profiling: (1) it automatically displays data distributions and summary statistics to facilitate data comprehension; (2) it is live, so visualizations are always accessible and update automatically as the data updates; (3) it supports follow up analysis and documentation by authoring code for the user in the notebook. In a user study with 16 participants, we evaluate two versions of our system that integrate different levels of automation: both automatically show data profiles and facilitate code authoring, however, one version updates reactively ('live') and the other updates only on demand ('dead'). We find that both tools, dead or alive, facilitate insight discovery with 91% of user-generated insights originating from the tools rather than manual profiling code written by users. Participants found live updates intuitive and felt it helped them verify their transformations while those with on-demand profiles liked the ability to look at past visualizations. We also present a longitudinal case study on how AutoProfiler helped domain scientists find serendipitous insights about their data through automatic, live data profiles. Our results have implications for the design of future tools that offer automated data analysis support. © 1995-2012 IEEE.\",\n",
       " 'Despite the elevated importance of Data Science in Statistics, there exists limited research investigating how students learn the computing concepts and skills necessary for carrying out data science tasks. Computer Science educators have investigated how students debug their own code and how students reason through foreign code. While these studies illuminate different aspects of students’ programming behavior or conceptual understanding, a method has yet to be employed that can shed light on students’ learning processes. This type of inquiry necessitates qualitative methods, which allow for a holistic description of the skills a student uses throughout the computing code they produce, the organization of these descriptions into themes, and a comparison of the emergent themes across students or across time. In this article we share how to conceptualize and carry out the qualitative coding process with students’ computing code. Drawing on the Block Model to frame our analysis, we explore two types of research questions which could be posed about students’ learning. Supplementary materials for this article are available online. © 2023 The Author(s). Published with license by Taylor & Francis Group, LLC.',\n",
       " 'Social media platforms generate large amounts of data on user activities, including likes, shares, comments, and follows. In recent years, the importance of data analytics as a sub-field of machine learning has grown significantly as more organizations have recognized the potential benefits of using data to improve their operations and strategies. One of the primary ways in which graph data science can be applied to Twitter data analytics is through network analysis. A network is a set of nodes (users) that are connected by edges (interactions), and network analysis involves examining the structure and properties of these networks. This chapter is an attempt to perform data analytics on the Twitter data set using Neo4j-based network analysis. Neo4j is a graph database that is designed to handle complex and interconnected data. With its powerful graph-based data model and query language, Neo4j is an excellent tool for data analytics, allowing users to easily explore and analyze relationships between data points. Analyzing social media activities using graph data science can provide valuable insights into user behavior, preferences, and trends. The research outcome of this chapter is to analyze social media activities using graph data science. © 2024 selection and editorial matter, Ankur Beohar, Ribu Mathew, Abhishek Kumar Upadhyay, and Santosh Kumar Vishvakarma -individual chapters, the contributors. All rights reserved.',\n",
       " 'There are several Data Science methodologies that entities and organizations have daily contact with however real-time decision support is seen as a decisive factor for success in making a decision. Due to the complexity, quantity, and diversity of data currently existing, a set of Data Science methodologies has emerged that help in the implementation of solutions. This article arises, fundamentally, with the purpose of answering the following question: What is the most complete and comprehensive data science methodology for any Data Science project? In carrying out this article, twenty-four methodologies were found and analyzed in detail. This study was based on a comparative benchmarking of methodologies, consisting of three phases of analysis, a first that evaluates and compares the phases of all the methodologies collected, a second that analyzes, compares and evaluates the cost, usability, maintenance, scalability, precision, speed, flexibility, reliability, explainability, interpretability, cyclicity and the support of OLAP technology by each methodology, and a third phase where the previous evaluations are compiled and the methodologies with the best results are returned. Quotes. After the three analyses, the methodologies that stood out the most were AgileData.io and IBM – Base Methodology for Data Science, however both obtained a quotation of 63.03%, which demonstrates a low percentage compared to the requirements. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.',\n",
       " 'The proceedings contain 20 papers. The special focus in this conference is on Data Science and Machine Learning. The topics include: Mitigating the\\xa0Adverse Effects of\\xa0Long-Tailed Data on\\xa0Deep Learning Models; shapley Value Based Feature Selection to\\xa0Improve Generalization of\\xa0Genetic Programming for\\xa0High-Dimensional Symbolic Regression; hybrid Models for\\xa0Predicting Cryptocurrency Price Using Financial and\\xa0Non-Financial Indicators; multi-dimensional Data Visualization for Analyzing Materials; law in\\xa0Order: An Open Legal Citation Network for\\xa0New Zealand; Enhancing Resource Allocation in\\xa0IT Projects: The Potentials of\\xa0Deep Learning-Based Recommendation Systems and\\xa0Data-Driven Approaches; a Comparison of\\xa0One-Class Versus Two-Class Machine Learning Models for\\xa0Wildfire Prediction in\\xa0California; skin Cancer Detection with\\xa0Multimodal Data: A Feature Selection Approach Using Genetic Programming; comparison of\\xa0Interpolation Techniques for\\xa0Prolonged Exposure Estimation: A Case Study on\\xa0Seven Years of\\xa0Daily Nitrogen Oxide in\\xa0Greater Sydney; unsupervised Fraud Detection on\\xa0Sparse Rating Networks; detecting Asthma Presentations from Emergency Department Notes: An Active Learning Approach; semi-supervised Model-Based Clustering for\\xa0Ordinal Data; Damage GAN: A Generative Model for\\xa0Imbalanced Data; text-Conditioned Graph Generation Using Discrete Graph Variational Autoencoders; Boosting QA Performance Through SA-Net and\\xa0AA-Net with\\xa0the\\xa0Read+Verify Framework; anomaly Detection Algorithms: Comparative Analysis and\\xa0Explainability Perspectives; towards Fairness and\\xa0Privacy: A Novel Data Pre-processing Optimization Framework for\\xa0Non-binary Protected Attributes; MStoCast: Multimodal Deep Network for\\xa0Stock Market Forecast.',\n",
       " 'This book covers unsupervised learning, supervised learning, clustering approaches, feature engineering, explainable AI and multioutput regression models for subsurface engineering problems. Processing voluminous and complex data sets are the primary focus of the field of machine learning (ML). ML aims to develop data-driven methods and computational algorithms that can learn to identify complex and non-linear patterns to understand and predict the relationships between variables by analysing extensive data. Although ML models provide the final output for predictions, several steps need to be performed to achieve accurate predictions. These steps, data pre-processing, feature selection, feature engineering and outlier removal, are all contained in this book. New models are also developed using existing ML architecture and learning theories to improve the performance of traditional ML models and handle small and big data without manual adjustments. This research-oriented book will help subsurface engineers, geophysicists, and geoscientists become familiar with data science and ML advances relevant to subsurface engineering. Additionally, it demonstrates the use of data-driven approaches for salt identification, seismic interpretation, estimating enhanced oil recovery factor, predicting pore fluid types, petrophysical property prediction, estimating pressure drop in pipelines, bubble point pressure prediction, enhancing drilling mud loss, smart well completion and synthetic well log predictions. © 2024 Daniel Asante Otchere.',\n",
       " 'What is data science? The simplest definition to come up with is, it is the study of data. The real-world data is raw and data science uses tools and techniques to extract meaningful information from the raw data. It incorporates different fields such as statistics, mathematics, computer engineering, machine learning, data miming, and artificial intelligence to analyze large amount of data. Nowadays there are various applications available to automatically capture and store this large amount of data such as online system and payment portals. Organizations are overwhelmed with this huge data and want to make inferences as to enhance business and productivity also to give users a better experience. Data science is helping to reveal gaps and uncovering new patterns take it from health, medicine, finance to e-commerce. Data science is a broader term, which consider multiple challenges, such as capturing, cleaning and transforming data to finally make inferences from it. Whereas data mining is mainly about extracting knowledge and unknown patterns from the huge amount of data hence it is also called “knowledge discovery process”. On the other hand, machine learning is an automated technique which uses complex algorithms for data processing and providing trained model output or we can say that it is a technique to train model on the given data and make predictions. Artificial Intelligence goes one step ahead and uses machine learning algorithm to make intelligent systems which can work on their own. These techniques have made data processing faster and much more efficient. It is because of different expertise required in this field, data science is showing strong growth. © 2024 by Apple Academic Press, Inc.',\n",
       " 'Educational curricula in data analysis are increasingly fundamental to statistics, data science, and a wide range of disciplines. The educational literature comparing coding syntaxes for instruction in data analysis recommends utilizing a simple syntax for introductory coursework. However, there is limited prior work to assess the pedagogical elements of coding syntaxes. The study investigates the paradigms of the dplyr, data.table, and DTwrappers packages for R programming from a pedagogical perspective. We enumerate the pedagogical elements of computer programming that are inherent to utilizing each package, including the functions, operators, general knowledge, and specialized knowledge. The merits of each package are also considered in concert with other pedagogical goals, such as computational efficiency and extensions to future coursework. The pedagogical considerations of this study can help instructors make informed choices about their curriculum and how best to teach their selected methods. © 2023 Teaching Statistics Trust.',\n",
       " 'The concepts associated with business analytics, such as business intelligence and data science, are generally murky. However, this misconception has a harmful impact on both academics and practitioners. This uncertainty may cause universities to develop misleading or incoherent curricula. This lack of clarity may also cause enterprises to choose an inappropriate analytical solution to a business problem, resulting in project failure and wasted resources. Despite its significance; it appears that only practitioners and major consulting firms are exerting significant effort to address this matter. Hence, this study aims to fill this void and uses the Delphi method to indicate that business intelligence and data science may be classified using eight dimensions which are: types of analytics, analytics process, skill set, data sources, business value, the scope of analytics, methods & techniques, and finally, technological platforms & tools. Significant implications for theory and practice are offered. © 2023 The Operational Research Society.',\n",
       " 'Abstract section not found',\n",
       " 'The proceedings contain 48 papers. The special focus in this conference is on Intelligent Systems and Data Science. The topics include: Blockchain-Based Platform for\\xa0IoT Sensor Data Management; bangla News Classification Employing Deep Learning; bangla Social Media Cyberbullying Detection Using Deep Learning; Bangladeshi Native Vehicle Classification Employing YOLOv8; Monitoring Attendance and\\xa0Checking School Uniforms Using YOLOv8; topic Classification Based on\\xa0Scientific Article Structure: A Case Study at\\xa0Can Tho University Journal of\\xa0Science; fake News Detection Using Knowledge Graph and Graph Convolutional Network; PETSAI: Physical Education Teaching Support with\\xa0Artificial Intelligence; A Graph-Based Approach for\\xa0Representing Wastewater Networks from\\xa0GIS Data: Ensuring Connectivity and\\xa0Consistency; SDCANet: Enhancing Symptoms-Driven Disease Prediction with\\xa0CNN-Attention Networks; image Recommendation Based on\\xa0Pre-trained Deep Learning and\\xa0Similarity Matching; a Practical Approach to\\xa0Leverage Knowledge Graphs for\\xa0Legal Query; a Study of\\xa0the\\xa0Impact of\\xa0Attention Mechanisms on\\xa0Feature Correlation Learning for\\xa0Building Extraction Models; Study and Implementation of AQI Predictive Recommendation System Based on Artificial Intelligence; development of a New Acoustic System for Nondestructive Internal Quality Assessment of Fruits; MR-Unet: Modified Recurrent Unet for Medical Image Segmentation; retrospective Analysis of a Large-Scale Archive of Ultrasonic Movies for Ischemic Diseases of Neonatal Brain; exam Cheating Detection Based on\\xa0Action Recognition Using Vision Transformer; building a\\xa0Health Monitoring System; fall Detection Using Intelligent Walking-Aids and\\xa0Machine Learning Methods; a Cloud-Based Intelligent Virtual Assistant for\\xa0Adolescents.',\n",
       " 'What has become evident over time in higher education is the low performance of students, especially in the first cycles, and higher education is of vital importance for our society today. That is why the GIETAES Group and ASU AyudantĀas Estudiantiles of the Universidad Politécnica Salesiana (UPS) offers tutoring to students. These tutorials are provided from students to students; however, the challenge is to detect the students most prone to fail the subject, in order to help them at an early stage. Therefore, in the present work, we propose an innovative analysis method of machine learning built in phases with the aim of predicting whether a student will lose or not the subject; firstly, we perform data preparation in which a preprocessing of variables, variable analysis, secondly, we perform a predictive analysis for this we have experimented with some techniques including support vector machines, Random Forest (RF) algorithm, KNN algorithm, and finally in the third phase performs the evaluation and interpretation of results. To demonstrate the effectiveness of our method, we have used a UPS own dataset and evaluated it with several quality metrics such as accuracy, precision, recall, and F1-Score. This research is a base point to experiment with various parameters based on the low performance of students not only in higher education but in any educational entity. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.',\n",
       " 'An investigative project can engage the learner in all aspects of a statistical investigation, including developing a question or issue of interest, gathering needed information, exploring the data, and communicating the results. This article summarizes the available literature regarding the implementation of investigative projects, including the potential benefits of projects, how projects have been used in a variety of settings, advice for those looking to implement projects, and future research avenues. © 2023 The Author(s). Published with license by Taylor & Francis Group, LLC.',\n",
       " 'Membrane-based separation can offer significant energy savings over conventional separation methods. Given their highly customizable and porous structures, metal–organic frameworks- (MOFs) are considered as next-generation membrane materials that can bring about high separation performance and energy efficiency in various separation applications. Yet, the enormously large number of possible MOF structures necessitates the development and implementation of efficient modeling approaches to expedite the design, discovery, and selection of optimal MOF-based membranes via directing the experimental efforts, time, and resources to the potentially useful membrane materials. With the recent developments in the field of atomic simulations and artificial intelligence methods, a new era of membrane modeling has started. This review focuses on the recent advances made and key strategies used in the modeling of MOF-based membranes and highlight the huge potential of combining atomistic modeling of MOFs with machine learning to explore very large number of MOF membranes and MOF/polymer composite membranes for gas separation. Opportunities and challenges related to the implementation of data-driven approaches to extract useful structure–property relations of MOF-based membranes and to produce design principles for the high-performing MOF-based membranes are discussed. © 2023 The Authors. Macromolecular Materials and Engineering published by Wiley-VCH GmbH.',\n",
       " 'Keeping abreast of current trends, technologies, and best practices in visualization and data analysis is becoming increasingly difficult, especially for fledgling data scientists. In this paper, we propose lodestar, an interactive computational notebook that allows users to quickly explore and construct new data science workflows by selecting from a list of automated analysis recommendations. We derive our recommendations from directed graphs of known analysis states, with two input sources: one manually curated from online data science tutorials, and another extracted through semi-automatic analysis of a corpus of over 6000 Jupyter notebooks. We validated Lodestar through three separate user studies: first a formative evaluation involving novices learning data science using the tool. We used the feedback from this study to improve the tool. This was followed by a summative study involving both new and returning participants from the formative evaluation to test the efficacy of our improvements. We also engaged professional data scientists in an expert review assessing the utility of the different recommendations. Overall, our results suggest that both novice and professional users find Lodestar useful for rapidly creating data science workflows. © The Author(s) 2023.',\n",
       " 'Probabilistic models such as logistic regression, Bayesian classification, neural networks, and models for natural language processing, are increasingly more present in both undergraduate and graduate statistics and data science curricula due to their wide range of applications. In this article, we present a one-week course module for students in advanced undergraduate and applied graduate courses on variational inference, a popular optimization-based approach for approximate inference with probabilistic models. Our proposed module is guided by active learning principles: In addition to lecture materials on variational inference, we provide an accompanying class activity, an R shiny app, and guided labs based on real data applications of logistic regression and clustering documents using Latent Dirichlet Allocation with R code. The main goal of our module is to expose students to a method that facilitates statistical modeling and inference with large datasets. Using our proposed module as a foundation, instructors can adopt and adapt it to introduce more realistic case studies and applications in data science, Bayesian statistics, multivariate analysis, and statistical machine learning courses. © 2023 American Statistical Association.',\n",
       " 'Abstract section not found',\n",
       " 'In this article, we consider how to make data more meaningful to students through the choice of data and the activities we use them in drawing upon students lived experiences more in the teaching of statistics and data science courses. In translating scholarship around culturally relevant pedagogy from the fields of education and mathematics education we develop the idea of culturally relevant data. We see this development as a key ingredient to implementing culturally relevant pedagogy in teaching data-intensive courses leveraging the centrality of context through data in both statistics and data science to engage students particularly from historically marginalized groups in STEM. We provide suggestions as to ways of finding or creating culturally relevant data and using them in implementing culturally relevant pedagogy to support the learning and flourishing of students in statistics and data science courses. We also present findings from pilot work we have done in implementing these data in statistics courses. Finally, we discuss lingering questions and possible next steps for research in this area. Supplementary materials for this article are available online. © 2023 The Author(s). Published with license by Taylor & Francis Group, LLC.',\n",
       " 'The importance of data literacies and the shortage of research surrounding data science in elementary schools motivated this research-practice partnership (RPP) between researchers and teachers from a STEM elementary school. We used a narrative case study methodology to describe the instructional practices of one music teacher who co-designed a data science curricular unit during a summer professional development program and implemented it in her 5th-grade music classroom. Data collected for this study include in-person and video observations, reflective journals, artifacts, and interviews. Findings suggest that this teacher integrated data science literacies into her classroom by supporting multiple avenues for data storytelling and relying on learners’ everyday discourse and experiences. Our study details a practical example of implementing data science with non-STEM domains in elementary classrooms. © 2023 Taylor & Francis Group, LLC.',\n",
       " 'Data science is inherently collaborative as individuals across fields and sectors use quantitative data to answer relevant questions. As a result, there is a growing body of research regarding how to teach interdisciplinary collaboration skills. However, much of the work evaluating methods of teaching statistics and data science collaboration relies primarily on self-reflection data. Additionally, prior research lacks detailed methods for assessing the quality of collaboration skills. In this case study, we present a method for teaching statistics and data science collaboration, a framework for identifying elements of effective collaboration, and a comparative case study to evaluate the collaboration skills of both a team of students and an experienced collaborator on two components of effective data science collaboration: structuring a collaboration meeting and communicating with a domain expert. Results show that the students could facilitate meetings and communicate comparably well to the experienced collaborator, but that the experienced collaborator was better able to facilitate meetings and communicate to develop strong relationships, an important element for high-quality and long-term collaboration. Further work is needed to generalize these findings to a larger population, but these results begin to inform the field regarding effective ways to teach specific data science collaboration skills. © 2023 The Author(s). Published with license by Taylor and Francis Group, LLC.',\n",
       " 'Teaching data literacy topics, such as machine learning, to security studies students is difficult because there are limited security-related teaching materials (e.g. datasets, user friendly software) for instructors. To address this challenge, we conducted an exploratory study to evaluate an asynchronous training module and software prototype with 15 college students. A key finding from this study is the importance of a simple teaching software tool and security case studies. The module boosted knowledge of key concepts and awareness of ‘big data’ accountability issues. We also found that teaching data-science concepts–even at an elementary level–requires that students have basic proficiencies working with datasets and spreadsheets, which suggests the need to integrate these skills throughout security studies curricula. This research also highlights the importance of building partnerships with data-science instructors to integrate data-science literacy in security studies and intelligence studies. © 2023 Department of Security Studies and Criminology.',\n",
       " 'In order to determine a suitable automobile insurance policy premium, one needs to take into account three factors: the risk associated with the drivers and cars on the policy, the operational costs associated with management of the policy and the desired profit margin. The premium should then be some function of these three values. We focus on risk assessment using a data science approach. Instead of using the traditional frequency and severity metrics, we instead predict the total claims that will be made by a new customer using historical data of current and past policies. Given multiple features of the policy (age and gender of drivers, value of car, previous accidents, etc.), one can potentially try to provide personalized insurance policies based specifically on these features as follows. We can compute the average claims made per year of all past and current policies with identical features and then take an average over these claim rates. Unfortunately there may not be sufficient samples to obtain a robust average. We can instead try to include policies that are “similar” to obtain sufficient samples for a robust average. We therefore face a trade-off between personalization (only using closely similar policies) and robustness (extending the domain far enough to capture sufficient samples). This is known as the bias–variance trade-off. We model this problem and determine the optimal trade-off between the two (i.e., the balance that provides the highest prediction accuracy) and apply it to the claim rate prediction problem. We demonstrate our approach using real data. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.',\n",
       " 'Abstract section not found',\n",
       " 'Research developments in the recommendation system and electronic commerce literature present more accurate and comprehensive recommendation system solutions. However, while these developments add new features to the recommendation systems, the question of whether a novel solution would excel in practice remains. Open innovation and crowdsourcing platforms are becoming an arena for designers to test their solutions in business competitions. We show how structural topical modeling identifies topical themes that improve contestant performance using forum message data during the competition period. Our topic modeling analysis identifies technological and business issues that emerge in recommendation system development. An econometric framework further investigates the link between topic distribution and performance. The multiperiod difference-in-differences estimator reports no significant statistical relation when linking all message communications to the performance. However, topic-dominant and topic-dispersed messages are both found to positively and significantly impact performance. Our result shows that structural topical modeling has an essential role to critically examine the most valuable message links to boost performance. Stakeholders may prioritize the messages with specific topics and/or a mixture of topics. We provide research and practical implications for researchers, business analysts, developers, and managers to improve their experiences when engaging in recommendation system design on platforms. © 1988-2012 IEEE.',\n",
       " 'A scatterplot is often the graph of choice for displaying the relationship between two variables. Scatterplots are useful for exploratory analysis, but can do much more than just identifying correlations. As data sets get larger and more complex, relying solely on “eye power” alone may cause us to miss interesting associations, or worse, make wrong interpretations. We show that by combining scatterplots with statistical and logical reasoning (the sliding window and two-axis median bisection), we may identify interesting associations in a case study of Graduate Record Examination admission versus graduation outcomes, and whether low detectability of proteins in a biological sample are truly associated with low abundance. Due to subjective visual interpretability, we recommend graphing the data using a multitude of visual variables and graph types before concluding the absence of an association. Finally, even if associations are demonstrable, developing causal models that could explain the observed fuzziness and lack of apparent correlations in the scatterplot are helpful for better decision-making and interpretation. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.',\n",
       " 'This study aims to the impact of the use of cyber security and data science in analyzing big medical data. The study sample comprised 120 participants, including hospital chief information officers, chief information security officers, and healthcare cyber security professionals, who were selected from all 33 government hospitals in Jordan connected to the Ministry of Health as the research sample. The primary independent variable, cyber security, was evaluated using information security, network security, operational security, and end-user education. massive amounts of medical data were used as the dependent variable. The study used SPSS to determine the impact of cybersecurity on the analysis of big medical data in Jordanian hospitals. The results found that 75% of participants confirmed that analyzing big data in the medical field will have a high impact on the evaluation of medical diagnoses and 63.3% of the participants agreed that analyzing big data in the medical field, it will have a high impact in predicting the incidence of diseases, the results also found the role of cybersecurity in protecting the storage of a large amount of data in hospital information systems (HIS), ranking first with an arithmetic mean of (3.73), The study recommends that future research should explore the benefits to medical organizations of analyzing structured and unstructured data in clinical and administrative fields, such as the limitations they face in these areas. Additionally, it is suggested that further research should also include medical institutions from outside Jordan borders to enable international comparative analyses. © 2023 Little Lion Scientific.',\n",
       " \"Artificial intelligence (AI) and data science have a massive contribution by transforming the present world into a revolutionary step through their application in various fields to solve the most common problems. It gives the machines to exhibit human-like characteristics such as learning and problem-solving. Data science is the field of study of large amounts of data to discover previously unknown patterns and extract meaningful knowledge by combining domain expertise, using modern tools and techniques, and knowledge of mathematics and statistics. AI and data science are becoming much more important for the healthcare industry. It is helpful in getting better and faster diagnoses than humans. AI and data science applications in the domain of finance are huge. Time-series analysis and forecasting are useful tools for making quick and quality decisions that are for solving challenging real-time financial problems like stock market predictions. In today's world chatbots are used to make effective communication with its user. The use of chatbots is huge as they are universally used on various websites for providing information and guidance by interacting and explaining to human users how the company or product works with a quick and meaningful response. A virtual assistant is a digital assistant that recognizes simple voice commands and completes tasks for the user. This is also an application of AI and data science that enables virtual assistants to listen to the user's command, interpret it, and perform the task. It gives the users the power to set an alarm, play music from Spotify, make calls, send messages, make a shopping list, or provide information such as weather, facts from Wikipedia, search meaning of a word from the dictionary, etc., by browsing the web with just a simple natural language voice command. There are many more real-life applications, such as e-mail spam filtering, Recommendation systems, autocomplete, face recognition, etc. © 2024 River Publishers. All rights reserved.\",\n",
       " 'This chapter comprises various parts. The first part consists of the introduction of artificial intelligence and its types and its history. The second part concerns achieving artificial intelligence and its tools and working. The third part is based on the emergence of data science, and the concluding part consists of the correlation between artificial intelligence and its applications in various domains. This part consists of various fields and real-life examples where artificial intelligence can be implemented and used. © 2024 River Publishers. All rights reserved.',\n",
       " 'Objective: In this synopsis, the editors of the Clinical Information Systems (CIS) section of the IMIA Yearbook of Medical Informatics overview recent research and propose a selection of best papers published in 2022 in the CIS field. Methods: The editors follow a systematic approach to gather relevant articles and select the best papers for the section. This year, they updated the query to incorporate the topic of telemedicine and removed search terms related to geographic information systems. The revised query resulted in a larger number of identified papers, necessitating the appointment of a third section editor to handle the increased workload. The editors narrowed the initial pool of articles to 15 candidate papers through a multi-stage selection process. At least seven independent reviews were collected for each candidate paper, and a selection meeting with the IMIA Yearbook editorial board led to the final selection of the best papers for the CIS section. Results: The query was carried out in mid-January 2023 and retrieved a deduplicated result set of 5,206 articles from 1,500 journals. This year, 15 papers were nominated as candidates, and four were finally selected as the best papers in the CIS section. Including telemedicine in the query resulted in a substantial increase in the number of papers found. The analysis highlights the growing convergence between clinical information systems and telemedicine, with mobile health (mHealth) technologies and data science applications gaining prominence. The selected candidate papers emphasize the practical impact of research efforts, focusing on patient-centric outcomes and benefits, including intelligent mobile health monitoring systems and AI-assisted decision-making in healthcare. Conclusions: Looking ahead, the field of CIS is expected to continue evolving, driven by advances in telemedicine, mHealth technologies, data science, and AI integration, leading to more efficient, patient-oriented, and intelligent healthcare systems and overall improvement of global healthcare outcomes. © The Author(s) 2023.',\n",
       " 'Abstract section not found',\n",
       " \"In this article, we presented our findings regarding an online project-based learning course, delivered to 64 students from the Federal University of Sao Paulo, Brazil, during the COVID-19 pandemic, in the second semester of 2021. The course had the goal of teaching Project Management by means of a competition (the Data Science Olympics). Our goal was to investigate the systemic impacts of the competition on learning. Data was collected by means of a questionnaire and from comments posted on the teams’ websites. We followed a convergent parallel mixed methods approach. We analyzed the data using a causal loop diagram to connect the insights gained with quantitative and qualitative results. Our findings were as follows: 1)The use of competition in a project-based learning centered course helped the students to develop project management and data science skills, and fostered metacognition and knowledge sharing opportunities. 2)The Data Science Olympics increased the students’ intrinsic motivation to learn. 3)The project-based teaching practices (scaffolding the students’ learning, giving meaningful feedback to the students, and managing the activities) facilitated the students' learning. 4)The problems the students faced throughout the Project (dropouts, communication problems, lack of commitment, difficulty scheduling online team meetings) impacted negatively on the students' motivation. © 2023, Aalborg University press. All rights reserved.\",\n",
       " 'This paper analyses the future prospects of statistics as a profession and how data science will change it. Indeed, according to Hadley Wickham, Chief Scientist at Rstudio, “a data scientist is a useful statistician”, establishing a strong connection between data science and applied statistics. In this direction, the aim is to look to the future by proposing a structural approach to future scenarios. Some possible definitions of data science are then discussed, considering the relationship with statistics as a scientific discipline. The focus then turns to an assessment of the skills required by the labor market for data scientists and the specific characteristics of this profession. Finally, the phases of a data science project are considered, outlining how these can be exploited by a statistician. © 2023 - IOS Press. All rights reserved.',\n",
       " 'Synthesis of α-branched aryl amines is a continuing challenge in synthetic and medicinal chemistry. Although transition-metal-promoted C-N cross-coupling is a desirable approach to the synthesis of α-branched aryl amines, there are limited examples of methods that demonstrate broad generality and applicability using structurally complex substrates. Herein, we report a method to cross-couple α-branched amines and aryl halides promoted by Cu using an oxamate ligand system. The method is compatible with many druglike aryl halides and amines and can be executed using miniaturized high-throughput experimentation (HTE) techniques that enable the rapid production of high-quality, consistent data sets. In addition to exploring the substrate scope and executing late-stage library synthesis of an active pharmaceutical, we used miniaturized HTE to interrogate the performance of a library of ligands across multiple substrate combinations in parallel at multiple time points. The data were used to build statistical models that provided insight as to what ligand features are important for function, which further enabled the design of ligands with improved reaction performance. © 2023 American Chemical Society.',\n",
       " 'The growing demand for data science and coding skills has led to an increase in the number of educational programs in these areas. However, the effectiveness of these programs in terms of career development outcomes remains unclear. This systematic review aims to evaluate the available literature on the impact of coding education on data science career development. The study will follow a rigorous and systematic approach to identify and analyze relevant research on data science and coding education. A comprehensive search of relevant databases will be conducted using keywords and MeSH terms related to data science, coding education, career development, and program evaluation. The purpose of this systematic review is to evaluate the available literature on the effectiveness of coding education for Data science career development. The increasing demand for professionals with data science and coding skills has led to a growth in the number of educational programs in these areas. However, the quality and effectiveness of these programs are not well understood. This systematic review aims to address this gap by synthesizing the existing research on the impact of coding education on data science career development. © 2024 Nova Science Publishers, Inc. All rights reserved.',\n",
       " 'The potential wide applications of big data analytics have created a high demand for data analysts in various industries, including business, healthcare, bioinformatics, politics, and management. As a result, higher education institutions are capitalizing on this opportunity by offering different data science programs to attract students and cater to industry needs. Over the past decade, there has been a rapid emergence of data science programs both nationally and globally. This chapter will begin by reviewing the impact of big data analytics on different industries. It will then proceed to describe various data science programs, including their curriculum design, course offerings, and target industry sectors for employment. Additionally, the chapter will address the weaknesses of some curricula and propose new teaching areas that are relevant to improve the learning outcomes of students. The aim of the suggestions is to better prepare data science students for the ever-evolving demands of big data analytics in the industry. © 2024, IGI Global.',\n",
       " 'The proceedings contain 107 papers. The topics discussed include: multi-lingual text classification models to detect hate and offensive text; effect of different concentrations of silicon dioxide nanoparticles on electro-optical and morphological properties of homeotropic aligned liquid crystal; a systematic literature review on sustainable development: emergence of sustainable mobility for global ecology; comparative analysis of existing security techniques in intelligent transportation system for smart cities; performance analysis of priority queue network consisting biserial and parallel channel; effect of physio-chemical parameters for testing the quality of wastewater in STPs; a cross-sectional study of hearing-impaired students on instructional practices in searching words; brief overview of biochar as carbon sequestration, its synthetic techniques, kinetic and equilibrium studies; and data preprocessing for development of customer churn prediction models in e-commerce.',\n",
       " '[No abstract available]',\n",
       " 'This study makes a speciality of a way to use the Statistical Analysis for Data Science approach to a real-international commercial enterprise scenario. Through this paper, the prominence of the Statistical Learning is known. Statistical Learning helps us, locating out the answer for a statistical inferential trouble through which it is easy to discover a predictive characteristic primarily based on the information we have. Statistical learning performs a distinguished position in the various fields like Computer Vision, Speech Recognition, Emotion and Gender identification of Data Science. © 2023 American Institute of Physics Inc.. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " 'Abstract section not found',\n",
       " 'This work intends to describe the humanitarian response to the cholera epidemic carried out by the international health non-governmental organisation Doctors with Africa CUAMM in the provinces of Zambezia, Sofala and Tete in Mozambique. The knowledge and practices of applied medical anthropology have proved to be fundamental in the management of prevention and monitoring activities in homes where cases of cholera have been identified and confirmed. The collection and management of data relating to the epidemiological curve has represented a challenge in the field and pose considerable theoretical problems with respect to the representation, public discourse and policies of local government and large international donors in response to health emergencies caused by the ecological crisis-climate, especially in Africa and Mozambique. © The Author(s).',\n",
       " 'The purpose of this article is to understand how educators may support students from different backgrounds (both relatively privileged and marginalized) to participate equitably and meaningfully in ethical data science discussions. To do this, we draw on the literature regarding STEM identity formation, use Cobb and Yackel’s (1996) framework for analyzing social norms for discourse in inquiry-based classrooms, and draw on Hodge and Cobb’s Cultural Participation Orientation towards developing an inclusive classroom environment. Finally, we describe the course elements (task structures, participation structures, and discursive moves) from a designed Ethical Data Science course that supported students’ equitable participation in ethical data science discussions (Sandoval, 2004). Copyright: © 2023 Register and Stephan.',\n",
       " 'Abstract section not found',\n",
       " 'The high volume of information produced by project management and its quality have become a challenge for organizations. Due to this, emerging technologies such as big data, data science and artificial intelligence (ETs) have become an alternative in the project life cycle. This article aims to present a systematic review of the literature on the use of these technologies in the architecture, engineering, and construction industry. A methodology of collection, purification, evaluation, bibliometric, and categorical analysis was used. A total of 224 articles were found, which, using the PRISMA method, finally generated 57 articles. The categorical analysis focused on determining the technologies used, the most common methodologies, the most-discussed project management areas, and the contributions to the AEC industry. The review found that there is international leadership by China, the United States, and the United Kingdom. The type of research most used is quantitative. The areas of knowledge where ETs are most used are Cost, Quality, Time, and Scope. Finally, among the most outstanding contributions are as follows: prediction in the development of projects, the identification of critical factors, the detailed identification of risks, the optimization of planning, the automation of tasks, and the increase in efficiency; all of these to facilitate management decision making. © 2023 by the authors.',\n",
       " 'The capabilities of data science can help us see hidden patterns, customize services, and advance biomedicine and science. As data science permeates industry and academia, a question that often arises is: How can we use these capabilities to genuinely help the world? Considering this question brings to mind terms such as “responsible data science” and “data for good.” Inherent in these terms is the desire that data science improve the human condition-in other words, the desire to undertake humane data science. How can we do this? What follows are 10 simple rules to contemplate. (Note that we are referring to data science in a holistic way. Along with machine learning and AI methods, we include all aspects of the data acquisition, analysis, dissemination, and application pipeline, and their accompanying human and socioeconomic factors). Copyright: © 2023 Masum, Bourne. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.',\n",
       " '[No abstract available]',\n",
       " 'Applications of graph theory exist in many science disciplines including chemistry, physics, medicine, and engineering. Since computational methods as well as computer-based approaches are less expensive and efficient, these techniques are very helpful to analyze chemical compounds. Chemical graph theory is a field that includes such kinds of analyses of chemical structures. Graph descriptors, also referred as topological indices, are graph invariants that help to study different structural properties of chemical substances. Such descriptors also aid to understand different activities related to chemical compounds. The main objective of this study is to find the best appropriate index to estimate the heat of formation of SF6. Initially, we compute degree-based topological indices, co-indices, and reverse indices for Sulfur hexafluoride. A similarity measure is used for feature selection. A network of the indices is constructed based on a similarity measure which is defined using Euclidean distance and Pearson correlation. Next, twenty-one subnetworks of the network consisting of highly similar indices, referred as modules, are captured. One module is containing 13 indices, three are containing 2 indices, and all the remaining modules are comprised of only one vertex. Hierarchical clustering is used to verify the detected modules. From each module one index, called master regulatory index (MRI), is selected for further study. Afterward, the thermodynamical measure heat of formation (HOF) is computed. A correlation analysis is done between each master regulatory index and heat of formation to capture any uncorrelated feature, if exists. Finally, mathematical formulations between each MRI and HOF are estimated. The best estimate is selected based on root mean squared error. © 2023, The Author(s), under exclusive licence to Società Italiana di Fisica and Springer-Verlag GmbH Germany, part of Springer Nature.',\n",
       " 'This groundbreaking book transcends traditional machine learning approaches by introducing information measurement methodologies that revolutionize the field. Stemming from a UC Berkeley seminar on experimental design for machine learning tasks, these techniques aim to overcome the \\'black box\\' approach of machine learning by reducing conjectures such as magic numbers (hyper-parameters) or model-type bias. Information-based machine learning enables data quality measurements, a priori task complexity estimations, and reproducible design of data science experiments. The benefits include significant size reduction, increased explainability, and enhanced resilience of models, all contributing to advancing the discipline\\'s robustness and credibility. While bridging the gap between machine learning and disciplines such as physics, information theory, and computer engineering, this textbook maintains an accessible and comprehensive style, making complex topics digestible fora broad readership. Information-Driven Machine Learning explores the synergistic harmony among these disciplines to enhance our understanding of data science modeling. Instead of solely focusing on the \"how,\" this text provides answers to the \"why\" questions that permeate the field, shedding light on the underlying principles of machine learning processes and their practical implications. By advocating for systematic methodologies grounded in fundamental principles, this book challenges industry practices that have often evolved from ideologic or profit-driven motivations. It addresses a range of topics, including deep learning, data drift, and MLOps, using fundamental principles such as entropy, capacity, and high dimensionality. Ideal for both academia and industry professionals, this textbook serves as a valuable tool for those seeking to deepen their understanding of data science as an engineering discipline. Its thought-provoking content stimulates intellectual curiosity and caters to readers who desire more than just code or ready-made formulas. The text invites readers to explore beyond conventional viewpoints, offering an alternative perspective that promotes a big-picture view for integrating theory with practice. Suitable for upper undergraduate or graduate-level courses, this book can also benefit practicing engineers and scientists in various disciplines by enhancing their understanding of modeling and improving data measurement effectively. © The Editor(s)(if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2024. All rights reserved.',\n",
       " 'The need for renewable energy sources has challenged most countries to comply with environmental protection actions and to handle climate change. Solar energy figures as a natural option, despite its intermittence. Brazil has a green energy matrix with significant expansion of solar form in recent years. To preserve the Amazon basin, the use of solar energy can help communities and cities improve their living standards without new hydroelectric units or even to burn biomass, avoiding harsh environmental consequences. The novelty of this work is using data science with machine-learning tools to predict the solar incidence (W.h/m ) in four cities in Amazonas state (north-west Brazil), using data from NASA satellites within the period of 2013-22. Decision-tree-based models and vector autoregressive (time-series) models were used with three time aggregations: day, week and month. The predictor model can aid in the economic assessment of solar energy in the Amazon basin and the use of satellite data was encouraged by the lack of data from ground stations. The mean absolute error was selected as the output indicator, with the lowest values obtained close to 0.20, from the adaptive boosting and light gradient boosting algorithms, in the same order of magnitude of similar references. © The Author(s) 2023.',\n",
       " 'As social expectations of data science and Artificial Intelligence (AI) in-crease and their application to various industrial fields advances, greater emphasis is being placed on data science and AI related education. In this study, we ascertained the impact of data science and AI education on learners’ motivation and career development by analyzing a lecture-style course offered as part of mathematical and data science education at Tokyo City University, Japan. The course analyzed was Data Science Literacy 1 (DS1). The analysis period spanned three academic years, from 2020 to 2022. The analyzed items were three motivational factors derived from expectancy-value theory, namely intrinsic value, attainment utility value, and expectations for success. A fourth factor, career development, was also analyzed. We collected data pertaining to the four factors, that is, the three above mentioned motivational factors and career development, through questionnaires administered to DS1 learners. Regarding data analysis, learners were clas-sified according to the values (high vs. low) they reported for each of the four factors of interest at the beginning of the course. These were compared to the values reported at the end of the course. Results showed increases in all the motivation factors. The increasing trend was particularly pronounced among learners who initially reported low values. Although trends differed from year to year, the results suggest that data science and AI education can positively impact motivational factors and related career development. © 2023, ICIC International. All rights reserved.',\n",
       " 'Recent advances in data science are opening up new research fields and broadening the range of applications of stochastic dynamical systems. Considering the complexities in real-world systems (e.g., noisy data sets and high dimensionality) and challenges in mathematical foundation of machine learning, this review presents two perspectives in the interaction between stochastic dynamical systems and data science. On the one hand, deep learning helps to improve first principle-based methods for stochastic dynamical systems. AI for science, combining machine learning methods with available scientific understanding, is becoming a valuable approach to study stochastic dynamical systems with the help of observation data. On the other hand, a challenge is the theoretical explanations for deep learning. It is crucial to build explainable deep learning structures with the help of stochastic dynamical systems theory in order to demonstrate how and why deep learning works. In this review, we seek better understanding of the mathematical foundation of the state-of-the-art techniques in data science, with the help of stochastic dynamical systems, and we further apply machine learning tools for studying stochastic dynamical systems. This is achieved through stochastic analysis, algorithm development, and computational implementation. Topics involved with this review include Stochastic Analysis, Dynamical Systems, Inverse Problems, Data Assimilation, Numerical Analysis, Optimization, Nonparametric Statistics, Uncertainty Quantification, Deep Learning, and Deep Reinforcement Learning. Moreover, we emphasize available analytical tools for non-Gaussian fluctuations in scientific and engineering modeling. © 2023 World Scientific Publishing Company.',\n",
       " \"This work describes the scientific and academic contributions of Professor Enric Brillas through the analysis of Social Network Analysis and data science. The study examines the research collaborations and co-authorship networks of Professor Brillas, indicating his active engagement and up-to-date collaborations with key co-authors, including Ignasi Sirés and Pere.L. Cabot. The analysis also reveals Professor Brillas' significant research focus on water treatment and related concepts such as oxidation-reduction, Fenton reactions, photoelectro-Fenton, and electrocatalysis. Furthermore, the most cited and recent articles by Professor Brillas are identified and discusses. Overall, the research demonstrates Professor Brillas' notable contributions to the field of electrochemical water treatment and highlights his ongoing research and collaborations in this area. © 2023 Elsevier Ltd\",\n",
       " 'Data science health research promises tremendous benefits for African populations, but its implementation is fraught with substantial ethical governance risks that could thwart the delivery of these anticipated benefits. We discuss emerging efforts to build ethical governance frameworks for data science health research in Africa and the opportunities to advance these through investments by African governments and institutions, international funding organizations and collaborations for research and capacity development. © 2023, Springer Nature Limited.',\n",
       " 'EPJ Data Science (2023) 12:6 After publication of this article [1], it was noted that the grant number in the Funding information section was incorrectly given as ‘CCF-1907591’ and should have read ‘CCF-2106578’. In addition, in the Conclusion section, the authors added several key references [48–53]. The original article [1] has been corrected. © 2023, Springer-Verlag GmbH, DE.',\n",
       " '[No abstract available]',\n",
       " \"Despite optimistic forecasts, industry innovations in data science have extraordinarily high rates of failure. It is essential to minimise the failure of data science projects, for both businesses and data professionals. Human systems are critical to the success of data science innovations. However, the human aspects of innovation management are often neglected or omitted in most guidelines and frameworks for data science. This provides limited guidance about the necessary human conditions for successful data science innovations. In this article we address this concern by developing a systematic framework for understanding human systems that support data science innovations. We first reviewed the elements of human system at different levels of analysis and how they contribute to innovation. Substantial research and theory indicate successful innovation requires supports at different levels of organisations, which combine to create the organisation's innovation capability. The review provided an initial framework for integrating human systems with data science innovations. Then, we drew on a series of interviews with key innovators engaged in developing current data science innovations. The interviews generated a more complete picture of human systems in practice. The findings support a range of practices to energise and facilitate innovation as an integral part of strategic planning and business processes. This study contributes to the advancement of innovation management theories and calls attention to guiding and engaging individuals through providing support and removing barriers to data science at different organisational levels. More practically, innovation managers could use this as a guide to optimise work systems and inform pathways to improve organisation data science efforts. © 2023 The Authors\",\n",
       " 'Letters of Recommendation (LORs) are widely utilized for admission to both undergraduate and graduate programs, and are becoming even more important with the decreasing role that standardized tests play in the admissions process. However, LORs are highly subjective and thus can inject recommender bias into the process, leading to an inequitable evaluation of the candidates’ competitiveness and competence. Our study utilizes natural language processing methods and manually determined ratings to investigate gender and cultural differences and biases in LORs written for STEM Master’s program applicants. We generate features to measure important characteristics of the LORs and then compare these characteristics across groups based on recommender gender, applicant gender, and applicant country of origin. One set of features, which measure the underlying sentiment, tone, and emotions associated with each LOR, is automatically generated using IBM Watson’s Natural Language Understanding (NLU) service. The second set of features is measured manually by our research team and quantifies the relevance, specificity, and positivity of each LOR. We identify and discuss features that exhibit statistically significant differences across gender and culture study groups. Our analysis is based on approximately 4000 applications for the MS in Data Science and MS in Computer Science programs at Fordham University. To our knowledge, no similar study has been performed on these graduate programs. © 2023, Springer Nature Limited.',\n",
       " 'Modern technology offers new treatment options for patients and novel avenues of research. However, there is limited available information in easily understandable language for the public explaining how technology relates to them and could influence their healthcare. The researchers, healthcare professionals and public members worked together collaboratively to address this problem by creating new materials for the public. Our paper explores that project through creative methods. Firstly, everyone involved was offered an opportunity to attend training sessions. Then, people took photos and described them to illustrate to others what is their experience of working together. Finally, we all met to use included photos as building blocks to present a shared experience in the project. Afterwards, the professional artist included it as one circular illustration with six interlinked layers. These layers present everyone’s experiences (from inside) (1) is about the opportunity to build confidence in a new topic, (2) relationships with others, (3) working remotely during the pandemic, (4) motivation that influenced people to become involved in this particular piece of work, (5) expectation that the project needs be inclusive and accessible, (6) ethical principles that researchers using new technology should follow. We showed that it is possible for researchers, healthcare professionals and members of the public to feel joint ownership of the project and that working together can be meaningful to everyone. © 2023, BioMed Central Ltd., part of Springer Nature. Background: The growth of data science and artificial intelligence offers novel healthcare applications and research possibilities. Patients should be able to make informed choices about using healthcare. Therefore, they must be provided with lay information about new technology. A team consisting of academic researchers, health professionals, and public contributors collaboratively co-designed and co-developed the new resource offering that information. In this paper, we evaluate this novel approach to co-production. Methods: We used participatory evaluation to understand the co-production process. This consisted of creative approaches and reflexivity over three stages. Firstly, everyone had an opportunity to participate in three online training sessions. The first one focused on the aims of evaluation, the second on photovoice (that included practical training on using photos as metaphors), and the third on being reflective (recognising one’s biases and perspectives during analysis). During the second stage, using photovoice, everyone took photos that symbolised their experiences of being involved in the project. This included a session with a professional photographer. At the last stage, we met in person and, using data collected from photovoice, built the mandala as a representation of a joint experience of the project. This stage was supported by professional artists who summarised the mandala in the illustration. Results: The mandala is the artistic presentation of the findings from the evaluation. It is a shared journey between everyone involved. We divided it into six related layers. Starting from inside layers present the following experiences (1) public contributors had space to build confidence in a new topic, (2) relationships between individuals and within the project, (3) working remotely during the COVID-19 pandemic, (4) motivation that influenced people to become involved in this particular piece of work, (5) requirements that co-production needs to be inclusive and accessible to everyone, (6) expectations towards data science and artificial intelligence that researchers should follow to establish public support. Conclusions: The participatory evaluation suggests that co-production around data science and artificial intelligence can be a meaningful process that is co-owned by everyone involved. © 2023, BioMed Central Ltd., part of Springer Nature.',\n",
       " 'Background: We present FHIR-PYrate, a Python package to handle the full clinical data collection and extraction process. The software is to be plugged into a modern hospital domain, where electronic patient records are used to handle the entire patient’s history. Most research institutes follow the same procedures to build study cohorts, but mainly in a non-standardized and repetitive way. As a result, researchers spend time writing boilerplate code, which could be used for more challenging tasks. Methods: The package can improve and simplify existing processes in the clinical research environment. It collects all needed functionalities into a straightforward interface that can be used to query a FHIR server, download imaging studies and filter clinical documents. The full capacity of the search mechanism of the FHIR REST API is available to the user, leading to a uniform querying process for all resources, thus simplifying the customization of each use case. Additionally, valuable features like parallelization and filtering are included to make it more performant. Results: As an exemplary practical application, the package can be used to analyze the prognostic significance of routine CT imaging and clinical data in breast cancer with tumor metastases in the lungs. In this example, the initial patient cohort is first collected using ICD-10 codes. For these patients, the survival information is also gathered. Some additional clinical data is retrieved, and CT scans of the thorax are downloaded. Finally, the survival analysis can be computed using a deep learning model with the CT scans, the TNM staging and positivity of relevant markers as input. This process may vary depending on the FHIR server and available clinical data, and can be customized to cover even more use cases. Conclusions: FHIR-PYrate opens up the possibility to quickly and easily retrieve FHIR data, download image data, and search medical documents for keywords within a Python package. With the demonstrated functionality, FHIR-PYrate opens an easy way to assemble research collectives automatically. © 2023, The Author(s).',\n",
       " 'Correction to: npj Parkinson’s Disease, published online 04 March 2023 In this article the affiliation details for Alastair J Noyce, Jonggeol Jeff Kim, Isabelle Francesca Foote, Sumit Dey were incorrectly given as ‘Department of Genetics and Genomic Sciences and Mindich Child Health and Development Institute, Icahn School of Medicine at Mount, Hess Center for Science and Medicine, New York, NY 10029, USA,’ but should have been ‘Preventive Neurology Unit, Wolfson Institute of Population Health, Queen Mary University of London, London, UK’. The affiliation details for Prabhjyot Saini were incorrectly given as ‘Preventive Neurology Unit, Wolfson Institute of Population Health, Queen Mary University of London, London, UK’ but should have been ‘The Neuro (Montreal Neurological Institute-Hospital), McGill University, Montreal, QC, Canada’. The original article has been corrected. © 2023, The Author(s).',\n",
       " 'Personalised approaches to cancer therapeutics primarily involve identification of patient sub-populations most likely to benefit from targeted drugs. Such a stratification has led to plethora of designs of clinical trials that are often too complex due to the need for incorporating biomarkers and tissue types. Many statistical methods have been developed to address these issues; however, by the time such methodology is available research in cancer has moved on to new challenges and therefore in order to avoid playing catch-up it is necessary to develop new analytic tools alongside. One of the challenges facing cancer therapy is to effectively and appropriately target multiple therapies for sensitive patient population based on a panel of biomarkers across multiple cancer types, and matched future trial designs. We present novel geometric methods (mathematical theory of hypersurfaces) to visualise complex cancer therapeutics data as multidimensional, as well as geometric representation of oncology trial design space in higher dimensions. The hypersurfaces are used to describe master protocols, with application to a specific example of a basket trial design for melanoma, and thus setup a framework for further incorporating multi-omics data as multidimensional therapeutics. © 2023, The Author(s).',\n",
       " 'Coherent Raman scattering (CRS) microscopy is a chemical imaging modality that provides contrast based on intrinsic biomolecular vibrations. To date, endeavors on instrumentation have advanced CRS into a powerful analytical tool for studies of cell functions and in situ clinical diagnosis. Nevertheless, the small cross-section of Raman scattering sets up a physical boundary for the design space of a CRS system, which trades off speed, signal fidelity and spectral bandwidth. The synergistic combination of instrumentation and computational approaches offers a way to break the trade-off. In this review, we first introduce coherent Raman scattering and recent instrumentation developments, then discuss current computational CRS imaging methods, including compressive micro-spectroscopy, computational volumetric imaging, as well as machine learning algorithms that improve system performance and decipher chemical information. We foresee a constant permeation of computational concepts and algorithms to push the capability boundary of CRS microscopy. © 2023, The Author(s).',\n",
       " '[No abstract available]',\n",
       " 'The Internet of Things, privacy, and technical constraints increase the demand for edge-based data-driven services, which is one of the major goals of Industry 4.0 and Society 5.0. Big data analysis (BDA) is the preferred approach to unleash hidden knowledge. However, BDA consumes excessive resources and time. These limitations hamper the meaningful adoption of BDA, especially the time and situation critical edge use cases, and hinder the goals of Industry 4.0 and Society 5.0. Automating the BDA process at the edge is a cognitive approach to address the aforementioned concerns. Data science workflow is an indispensable challenge for successful automation. Therefore, we conducted a systematic literature survey on data science workflow platforms as the first contribution. Moreover, we learned that the BDA workflow depends on diversified constraints and undergoes rigorous data-mining stages. These caused an increase in the solution space, dynamic constraints, complexity issues, and NP-hardness of BDA workflow. Graphplan is a heuristic AI-planning technique that can address concerns associated with BDA workflow. Therefore, as the second contribution, we adopted the graphplan to generate a workflow for edge-based BDA automation. Experiments demonstrate that the proposed method achieved our objectives. © 2023 IEEE.',\n",
       " 'The exchange of large and complex slide microscopy imaging data in biomedical research and pathology practice is impeded by a lack of data standardization and interoperability, which is detrimental to the reproducibility of scientific findings and clinical integration of technological innovations. We introduce Slim, an open-source, web-based slide microscopy viewer that implements the internationally accepted Digital Imaging and Communications in Medicine (DICOM) standard to achieve interoperability with a multitude of existing medical imaging systems. We showcase the capabilities of Slim as the slide microscopy viewer of the NCI Imaging Data Commons and demonstrate how the viewer enables interactive visualization of traditional brightfield microscopy and highly-multiplexed immunofluorescence microscopy images from The Cancer Genome Atlas and Human Tissue Atlas Network, respectively, using standard DICOMweb services. We further show how Slim enables the collection of standardized image annotations for the development or validation of machine learning models and the visual interpretation of model inference results in the form of segmentation masks, spatial heat maps, or image-derived measurements. © 2023, The Author(s).',\n",
       " 'Open science and collaboration are necessary to facilitate the advancement of Parkinson’s disease (PD) research. Hackathons are collaborative events that bring together people with different skill sets and backgrounds to generate resources and creative solutions to problems. These events can be used as training and networking opportunities, thus we coordinated a virtual 3-day hackathon event, during which 49 early-career scientists from 12 countries built tools and pipelines with a focus on PD. Resources were created with the goal of helping scientists accelerate their own research by having access to the necessary code and tools. Each team was allocated one of nine different projects, each with a different goal. These included developing post-genome-wide association studies (GWAS) analysis pipelines, downstream analysis of genetic variation pipelines, and various visualization tools. Hackathons are a valuable approach to inspire creative thinking, supplement training in data science, and foster collaborative scientific relationships, which are foundational practices for early-career researchers. The resources generated can be used to accelerate research on the genetics of PD. © 2023, The Author(s).',\n",
       " 'Failed to fetch page (status 524)',\n",
       " 'Laparoscopy is an imaging technique that enables minimally-invasive procedures in various medical disciplines including abdominal surgery, gynaecology and urology. To date, publicly available laparoscopic image datasets are mostly limited to general classifications of data, semantic segmentations of surgical instruments and low-volume weak annotations of specific abdominal organs. The Dresden Surgical Anatomy Dataset provides semantic segmentations of eight abdominal organs (colon, liver, pancreas, small intestine, spleen, stomach, ureter, vesicular glands), the abdominal wall and two vessel structures (inferior mesenteric artery, intestinal veins) in laparoscopic view. In total, this dataset comprises 13195 laparoscopic images. For each anatomical structure, we provide over a thousand images with pixel-wise segmentations. Annotations comprise semantic segmentations of single organs and one multi-organ-segmentation dataset including segments for all eleven anatomical structures. Moreover, we provide weak annotations of organ presence for every single image. This dataset markedly expands the horizon for surgical data science applications of computer vision in laparoscopic surgery and could thereby contribute to a reduction of risks and faster translation of Artificial Intelligence into surgical practice. © 2023, The Author(s).',\n",
       " 'Biomedical data science education faces the challenge of preparing students for conducting rigorous research with increasingly complex and large datasets. At the same time, philosophers of science face the challenge of making their expertise accessible for scientists in such a way that it can improve everyday research practice. Here, we investigate the possibility of approaching these challenges together. In current and proposed approaches to biomedical data science education, we identify a dominant focus on only one aspect of conducting scientific research: understanding and using data, research methods, and statistical methods. We argue that this approach cannot solve biomedical data science’s challenge and we propose to shift the focus to four other aspects of conducting research: making and justifying decisions in research design and implementation, explaining their epistemic and non-epistemic effects, balancing varying responsibilities, and reporting scientific research. Attending to these aspects requires learning on different dimensions than solely learning to apply techniques (first dimension). It also requires learning to make choices (second dimension) and to understand the rationale behind choices (third dimension). This could be fostered by integrating philosophical training in biomedical data science education. Furthermore, philosophical training fosters a fourth dimension of learning, namely, understanding the nature of science. In this article, we explain how we identified the five aspects of conducting research and the four dimensions of learning, and why attending to the fourth dimension is essential. We discuss educational approaches to attend to all aspects and dimensions, and present initial design principles to implement these approaches. © 2022, The Author(s).',\n",
       " 'A Jyotirlinga or Jyotirlingam is a devotional representation of a Hindu Deity. The word is made up of Sanskrit \\'jyotis\\' which means \\'radiance\\' and linga, also spelled lingam, means \\'sign\\' or \"distinguishing symbol\". Hinduism defines Jyotirlingam as the radiant sign of the Almighty. In our data scientific view, nonetheless, Jyotirlinga represents the embodiment of time series. It exhibits stationarity and multi-model patterns of naturally occurring time series, a classical data science pattern, that has a causative relationship with historical events, world macroeconomics, agriculture, and other worldly events. © 2024 by Nova Science Publishers, Inc. All rights reserved.',\n",
       " 'The drug discovery and design process has been significantly transformed by the integration of data science, artificial intelligence (AI), green chemistry principles, and affordable medicine. AI techniques enable rapid analysis of vast datasets, predicting molecular interactions, optimizing drug candidates, and identifying potential therapeutics. Green chemistry practices promote sustainability and efficiency, resulting in environmentally friendly and cost-effective production processes. The goal is to develop affordable medicines that are not only efficacious but also accessible to a wider population. This chapter explores case studies and emerging trends to highlight the transformation of the pharmaceutical industry and innovation in drug discovery. © 2024, IGI Global. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " 'The book systematically introduces the basic contents of data science, including data preprocessing and basic methods of data analysis, handling special problems (e.g. text analysis), deep learning, and distributed systems. In addition to systematically introducing the basic content of data science from a theoretical point of view, the book also provides a large number of data analysis practice cases. © 2024 by Higher Education Press Limited Company and World Scientific Publishing Co. Pte. Ltd. All rights reserved.',\n",
       " 'Failed to fetch page (status 404)',\n",
       " '[No abstract available]',\n",
       " 'An analysis of information system technology and its connections to the economy and society is the focus of this research study. The improvements in digital technology have increased the amount of data that can provide insights critical to the transformation of both enterprises and society. Knowledge workers or researchers are responsible for producing and evaluating essential data gained employment prospects as corporations reshape their operations. A necessary part of this book asserts that data science has had a profound impact on the lives of individuals and corporations alike. As a result of digitalized data science, this study intends to propose a significant research issue to investigate the apparent modifications in society and business models. © 2023 American Institute of Physics Inc.. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " 'Credit risk is the critical problem faced by banking and financial sectors when the borrower fails to complete their commitments to pay back. The factors that could increase credit risk are non-performing assets and frauds which are improved by continuous monitoring of payments and other assessment patterns. In past years, few statistical and manual auditing methods were investigated which were not much suitable for tremendous amount of data. Thus, the growth of Artificial Intelligence (AI) with efficient access to big data is focused. However, the effective Deep Learning (DL) and Machine Learning (ML) techniques are introduced to improve the performance and issues in banking and finance sectors by concentrating the business process and customer interaction. In this review, it mainly focusses on the different learning methods-based research articles available in recent years. This review also considers 93 recent research articles that were available in the last 5 years related to the topic of credit risk with different learning methods to tackle traditional challenges. Thus, these advances can make the banking process as smart and fast while preserving themselves from credit defaulters. © 2023 - IOS Press. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " 'This paper presents an adaptive data delivery method for data science pipelines. While this method is feasible for processes communicating over any network, in this work we focus on edge-fog-cloud infrastructures. In a diagnostic phase, a model based on the Bernoulli principle is used to create a representation of bottlenecks in a pipeline. In a supervision phase, a watchman/sentinel cooperative system monitors the throughput of the pipeline stages to create a bottleneck-stage scheme. In a rectification phase, this system produces replicas of bottlenecks stages, mitigating the workload congestion using implicit parallelism and load balancing algorithms. This method is automatically and transparently invoked to produce a steady continuum dataflow. To test our proposal, we conducted a case study about the processing of medical and satellite data. The evaluation revealed that this method creates continuum dataflows, without neither characterising workloads nor knowing infrastructure details, which yields a competitive performance with state-of-the-art solutions. © 2023 ACM.',\n",
       " \"Data Science (DS) is emerging in major software development projects and often needs to follow software development practices. Therefore, DS processes will likely continue to attract Software Engineering (SE) practices and vice-versa. This case study aims to map and describe a software development process for Machine Learning(ML)-enabled applications and associated practices used in a real DS project at the Recod.ai laboratory in collaboration with an industrial partner. The focus was to analyze the process and identify the strengths and primary challenges, considering their expertise in robust ML practices and how they can contribute to general software quality. To achieve this, we conducted semi-structured interviews and analyzed them using procedures from the Straussian Grounded Theory. The results showed that the DS development process is iterative, with feedback between activities, which differs from the processes in the literature. Additionally, this process presents a greater involvement of domain experts. Besides, the team prioritizes software quality characteristics (attributes) in these DS projects to ensure some aspects of the final product's quality, i.e., functional correctness and robustness. To achieve those, they use regular accuracy metrics and include explainability and data leakage as quality metrics during training. Finally, the software engineer's role and its responsibilities differ from those of a traditional industry software engineer, as s/he is involved in most of the process steps. These characteristics can contribute to high-quality models achieving the partner needs and, consequently, relevant contributions to the intersection between SE and DS. © 2023 ACM.\",\n",
       " 'There have been consistent calls for more research on managing teams and embedding processes in data science innovations. Widely used frameworks (e.g., the cross-industry standard process for data mining) provide a standardized approach to data science but are limited in features such as role clarity, skills, and cross-team collaboration that are essential for developing organizational capabilities in data science. In this study, we introduce a data workflow method (DWM) as a new approach to break organizational silos and create a multi-disciplinary team to develop, implement and embed data science. Different from current data science process workflows, the DWM is managed at the system level that shapes business operating model for continuous improvement, rather than as a function of a particular project, one single business unit, or isolated individuals. To further operationalize the DWM approach, we investigated an embedded data workflow at a mining operation that has been using geological data in a machine-learning model to stabilize daily mill production for the last 2years. Based on the findings in this study, we propose that the DWM approach derives its capability from three aspects: (a) a systemic data workflow; (b) multi-disciplinary networks of collaboration and responsibility; and (c) clearly identified data roles and the associated skills and expertise. This study suggests a whole-of-organization approach and pathway to develop data science capability. © The Author(s), 2023. Published by Cambridge University Press.',\n",
       " 'Development of non-aqueous redox flow batteries as a viable energy storage solution relies upon the identification of soluble charge carriers capable of storing large amounts of energy over extended time periods. A combination of metrics including number of electrons stored per molecule, redox potential, stability, and solubility of the charge carrier impact performance. In this context, we recently reported a 2,2′-bipyrimidine charge carrier that stores two electrons per molecule with reduction near −2.0 V vs. Fc/Fc+and high stability. However, these first-generation derivatives showed a modest solubility of 0.17 M (0.34 M e−). Seeking to improve solubility without sacrificing stability, we harnessed the synthetic modularity of this scaffold to design a library of sixteen candidates. Using computed molecular descriptors and a single node decision tree, we found that minimization of the solvent accessible surface area (SASA) can be used to predict derivatives with enhanced solubility. This parameter was used in combination with a heatmap describing stability to de-risk a virtual screen that ultimately identified a 2,2′-bipyrimidine with significantly increased solubility and good stability metrics in the reduced states. This molecule was paired with a cyclopropenium catholyte in a prototype all-organic redox flow battery, achieving a cell potential up to 3 V. © 2023 The Royal Society of Chemistry.',\n",
       " 'Purpose: This chapter conceptualises a link between Industrial Revolution 4.0 (IR 4.0), big data, data science and sustainable tourism. Design/Methodology/Approach: The author adopts a grounded theory and conceptual approach to endeavour in this exploratory research. Findings: The outcome shows a significant rise of big data in the tourism sector under three major dimensions, i.e. business, governance and research. And, some exemplary evidence of institutions promoting the use of big data and data science for sustainable tourism has been discussed. Originality/Value: The conceptualised interlinkage of concepts like IR 4.0, big data, data science and sustainable development provides a valuable knowledge resource to policy-makers, researchers, businesses and students. © 2024 by Emerald Publishing Limited.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'The URLs in the Data Availability statement for this paper are incorrect. The correct URLs are: • DNase-Seq has the unique identifier minid:b9dt2t and is available from https://identifiers.org/minid:b9dt2t • Aligned BAM files have an identifier: minid:b9vx04 and are available from https://identifiers.org/minid:b9vx04 • The collection of BED files of footprints have an identifier: minid:b9496p and are available from https://identifiers.org/minid:b9496p • The non-redundant Motifs database has an identifier: minid:b97957 and is available from https://identifiers.org/minid:b97957 • The motif intersected hits have an identifier: minid:b9p09p and are available from https://identifiers.org/minid:b9p09p • The Transcription Factor Binding Sites generated from the study have an identifier: minid: b9v398 and are available from https://identifiers.org/minid:b9v398 © 2023 Madduri et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.',\n",
       " 'Early de-risking of drug targets and chemistry is essential to provide drug projects with the best chance of success. Target safety assessments (TSAs) use target biology, gene and protein expression data, genetic information from humans and animals, and competitor compound intelligence to understand the potential safety risks associated with modulating a drug target. However, there is a vast amount of information, updated daily that must be considered for each TSA. We have developed a data science–based approach that allows acquisition of relevant evidence for an optimal TSA. This is built on expert-led conventional and artificial intelligence–based mining of literature and other bioinformatics databases. Potential safety risks are identified according to an evidence framework, adjusted to the degree of target novelty. Expert knowledge is necessary to interpret the evidence and to take account of the nuances of drug safety, the modality, and the intended patient population for each TSA within each project. Overall, TSAs take full advantage of the most recent developments in data science and can be used within drug projects to identify and mitigate risks, helping with informed decision-making and resource management. These approaches should be used in the earliest stages of a drug project to guide decisions such as target selection, discovery chemistry options, in vitro assay choice, and end points for investigative in vivo studies. © 2023 by the Society for Experimental Biology and Medicine.',\n",
       " 'The application of data science (DS) techniques has become increasingly essential in various fields, including epidemiology and climatology in agricultural production systems. In this sector, traditionally large amounts of data are acquired, but not well-managed and -analyzed as a basis for evidence-based decision-making processes. Here, we present a comprehensive step-by-step guide that explores the use of DS in managing epidemiological and climatological data within rice production systems under tropical conditions. Our work focuses on using the multi-temporal dataset associated with the monitoring of diseases and climate variables in rice in Colombia during eight years (2012–2019). The study comprises four main phases: (I) data cleaning and organization to ensure the integrity and consistency of the dataset; (II) data management involving web-scraping techniques to acquire climate information from free databases, like WordClim and Chelsa, validation against in situ weather stations, and bias removal to enrich the dataset; (III) data visualization techniques to effectively represent the gathered information, and (IV) a basic analysis related to the clustering and climatic characterization of rice-producing areas in Colombia. In our work, a process of evaluation and the validation of climate data are conducted based on errors (r, R2,MAE, RSME) and bias evaluation metrics. In addition, in phase II, climate clustering was conducted based on a PCA and K-means algorithm. Understanding the association of climatic and epidemiological data is pivotal in predicting and mitigating disease outbreaks in rice production areas. Our research underscores the significance of DS in managing epidemiological and climatological data for rice production systems. By applying a protocol responsible for DS tools, our study provides a solid foundation for further research into disease dynamics and climate interactions in rice-producing regions and other crops, ultimately contributing to more informed decision-making processes in agriculture. © 2023 by the authors.',\n",
       " 'In the last decade, the signal processing (SP) community has witnessed a paradigm shift from model-based to data-driven methods. Machine learning (ML) - more specifically, deep learning - methodologies are nowadays widely used in all SP fields, e.g., audio, speech, image, video, multimedia, and multimodal/multisensor processing, to name a few. Many data-driven methods also incorporate domain knowledge to improve problem modeling, especially when computational burden, training data scarceness, and memory size are important constraints. © 1991-2012 IEEE.',\n",
       " 'This article studies the rise of academic data science in Germany, Austria and Switzerland. By focusing on the boundary work that accompanies this development, we try to understand current transformations in knowledge production within digital academia and beyond. Drawing on qualitative interviews with data science scholars, we identify five lines of demarcation in claiming universal epistemic authority. This boundary work is characterized by multiple tensions and varies depending upon context and counterpart, making it inherently relational. © 2023 Bianca Prietl et al.',\n",
       " '(1) Background: Surgical phases form the basic building blocks for surgical skill assessment, feedback, and teaching. The phase duration itself and its correlation with clinical parameters at diagnosis have not yet been investigated. Novel commercial platforms provide phase indications but have not been assessed for accuracy yet. (2) Methods: We assessed 100 robot-assisted partial nephrectomy videos for phase durations based on previously defined proficiency metrics. We developed an annotation framework and subsequently compared our annotations to an existing commercial solution (Touch Surgery, Medtronic™). We subsequently explored clinical correlations between phase durations and parameters derived from diagnosis and treatment. (3) Results: An objective and uniform phase assessment requires precise definitions derived from an iterative revision process. A comparison to a commercial solution shows large differences in definitions across phases. BMI and the duration of renal tumor identification are positively correlated, as are tumor complexity and both tumor excision and renorrhaphy duration. (4) Conclusions: The surgical phase duration can be correlated with certain clinical outcomes. Further research should investigate whether the retrieved correlations are also clinically meaningful. This requires an increase in dataset sizes and facilitation through intelligent computer vision algorithms. Commercial platforms can facilitate this dataset expansion and help unlock the full potential, provided that the phase annotation details are disclosed. © 2023 by the authors.',\n",
       " '[No abstract available]',\n",
       " 'Firms have been proactively holding data science competitions via online contest platforms to look for innovative solutions from the crowd. When firms are designing such competitions, a key question is “What should be a better contest design to motivate contestants to exert more effort?” We model two commonly observed contest structures (one stage and two stage) and two widely adopted prize structures (high spread and low spread). We employ economic experiments to examine how contest design affects contestants’ effort level. The results reject the base model with rationality assumption. We find that contestants exert significantly more effort in both the first stage and the second stage of the two-stage contest. Moreover, it is better to assign most prizes to the winner in the two-stage contest while it does not matter in one stage. To explain the empirical regularities, we develop a behavioral economics model that captures contestants’ psychological aversion to falling behind and continuous exertion of effort. Our findings demonstrate that it is important for contest organizers to account for the nonpecuniary factors that can influence contestants’ behavior in designing a competition. © 2023 Production and Operations Management Society.',\n",
       " '[No abstract available]',\n",
       " \"To accelerate the impact of African genomics on human health, data science skills and awareness of Africa's rich genetic diversity must be strengthened globally. We describe the first African genomics data science workshop, implemented by the African Society of Human Genetics (AfSHG) and international partners, providing a framework for future workshops. © 2023 Elsevier Ltd\",\n",
       " '[No abstract available]',\n",
       " 'The emergence of new prestigious professions in data science and artificial intelligence (AI) provide a rare opportunity to explore the gendered dynamics of technical careers as they are being formed. In this paper, we contribute to the literature on gender inequality in digital work by curating and analysing a unique cross-country data set. We use innovative data science methodology to investigate the nature of work and skills in these under-researched fields. Our research finds persistent disparities in jobs, qualifications, seniority, industry, attrition\\xa0and even self-confidence in these fields. We identify structural inequality in data and AI, with career trajectories of professionals differentiated by gender, reflecting the broader history of computing. Our work is original in illuminating gendering processes within elite high-tech jobs as they are being configured. Paying attention to these nascent fields is crucial if we are to ensure that women take their rightful place at forefront of technological innovation. © 2023 The Authors. New Technology, Work and Employment published by Brian Towers (BRITOW) and John Wiley & Sons Ltd.',\n",
       " '[No abstract available]',\n",
       " 'Theories are the vehicle of cumulative knowledge acquisition. At this time, however, many (developmental) psychological theories are insufficiently precise to derive testable hypotheses. This limits the advancement of our principled understanding of development. This problem cannot be resolved by improving the way deductive (confirmatory) research is conducted (e.g., through preregistration and replication), because theory formation requires inductive (exploratory) research. This paper argues that machine learning can help advance theory formation in (developmental) psychology, because it enables rigorous exploration of patterns in data. The paper discusses specific advantages of machine learning, explains core methodological concepts, introduces relevant methods, and describes how data-driven insights are consolidated into theory. Machine learning automates exploration, and incorporates checks and balances to ensure generalizable results. It can assist in phenomenon detection and offers a more holistic understanding of the phenomena associated with an outcome or process of interest. © 2022 The Author. Infant and Child Development published by John Wiley & Sons Ltd.',\n",
       " 'Wi-Fi is a widely used technology worldwide but can also be a source of insecurity. One of the most common forms of attack is the evil twin, where an attacker creates a fake Wi-Fi network with the same name as a legitimate network. Machine learning algorithms are a promising solution to identify this type of attack. Existing works use databases known as Aegean Wi-Fi Intrusion Datasets (AWID) and detect different types of attacks. This paper focuses on the evil twin attack on AWID3 based on analysis and detection, exploring data science techniques, encompassing the steps of pre-processing, data analysis, data balancing, training and validation, and model testing. The results indicate that the optimized LightGBM reached a False Positive Rate (FPR) of 0.00602 in the test with all columns of the dataset, in addition to an FPR of 0.00898 using a column set, promising for an Intrusion Detection System (IDS). © 2023 ACM.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'Background: The availability of statistical data in the field of football performance analysis is continuously expanding. Despite this abundance, the effective utilization of such data in practical scenarios encounters various challenges. Purpose: This study aimed to explore the potential of integrating visualizations through Power BI to the effectiveness of statistical analysis for soccer coaches. Methodology: We leveraged data from three Excel sheets derived from two surveys published in the \"Journal of Physical Education and Sport\". Employing Power BI, we created six distinct types of graphs. Results: The treemap depicts the distribution of corner kicks and resulting goals based on delivery type and zone. The area chart illustrates the team\\'s season long form. The clustered column chart reveals goals, expected goals, and team efficiency. The radar chart depicts the team\\'s behavior relative to the league\\'s average across 19 tactical situations. The table heatmap presents the percentage of passes in each third of the pitch for the teams. The scatter bubble chart illustrates team performance across four distinct tactical situations during the attacking phase. Conclusions: The derived visualizations provided coaches with invaluable practical insights. Our findings underscore the significance of visualizing statistical data, thereby empowering coaches to analyze their team, assess opponents, and identify trends within their competitive landscape. Visualization proves to be an indispensable tool for making critical decisions and gaining a profound understanding of diverse performance aspects. By harnessing Power BI, coaches can adeptly interpret and capitalize on statistical information, resulting in refined analysis and more informed decision-making processes. © JPES.',\n",
       " '[No abstract available]',\n",
       " 'This study explores the potential of data science software solutions like Customer Relationship Management Software (CRM) for increasing the revenue generation of businesses. We focused on those businesses in the accommodation and food service sector across the European Union (EU). The investigation is contextualized within the rising trend of data-driven decision-making, examining the potential correlation between data science applications and business revenues. By employing a comprehensive evaluation of Eurostat datasets from 2014 to 2021, we used both univariate and multivariate analyses, assessing the percentage of companies that have e-commerce sales across the EU countries, focusing on the usage of big data analytics from any source and the use of CRM tools for marketing purposes or other activities. Big data utilization showed a clear, positive relationship with enhanced e-commerce sales. However, CRM tools exhibited a dualistic impact: while their use in marketing showed no significant effect on sales, their application in non-marketing functions had negative effects on sales. These findings underscore the potential role of CRM and data science solutions in enhancing business performance in the EU’s accommodation and food service industry. © 2023 by the authors.',\n",
       " 'The increasing prevalence of overweight and obesity is a worldwide problem, with several well-known consequences that might start to develop early in life during childhood. The present research based on data from children that have been followed since birth in a previously established cohort study (Generation XXI, Porto, Portugal), taking advantage of State-of-the-Art (SoA) data science techniques and methods, including Neural Architecture Search (NAS), explainable Artificial Intelligence (XAI), and Deep Learning (DL), aimed to explore the hidden value of data, namely on electrocardiogram (ECG) records performed during follow-up visits. The combination of these techniques allowed us to clarify subtle cardiovascular changes already present at 10 years of age, which are evident from ECG analysis and probably induced by the presence of obesity. The proposed novel combination of new methodologies and techniques is discussed, as well as their applicability in other health domains. © 2023 by the authors.',\n",
       " 'Nowadays, railway track monitoring strategies are based on the use of railway inspection vehicles and wayside dynamic monitoring systems. The latter sometimes requires traffic disruption, as well as higher time and cost-consumption activities, and the use of dedicated inspection vehicles is less economical and efficient as the use of in-service vehicles. Furthermore, the use of non-automated algorithms faces challenges when it comes to early damage detection in railway infrastructure, considering operational, environmental, and big data aspects, and may lead to false alarms. To overcome these challenges, the application of artificial intelligence (AI) algorithms for early detection of track defects using accelerations, measured by dynamic monitoring systems in in-service railway vehicles is attracting the attention of railway managers. In this paper, an AI-based methodology based on axle box acceleration signals is applied for the early detection of distributed damage to track in terms of the longitudinal level and lateral alignment. The methodology relies on feature extraction using an autoregressive model, data normalization using principal component analysis, data fusion and feature discrimination using Mahalanobis distance and outlier analysis, considering eight onboard accelerometers. For the numerical simulations, 75 undamaged and 45 damaged track scenarios are considered. The alert limit state defined in the European Standard for assessing track geometry quality is also assumed as a threshold. It was found that the detection accuracy of the AI-based methodology for different sensor layouts and types of damage is greater than 94%, which is acceptable. © 2023 by the authors.',\n",
       " 'The integration of data sciences and smart technologies in the construction industry, particularly in air conditioning project management, is an important area of research. This study employs the Delphi Method to explore this integration, surveying 40 experts in construction management. Participants were meticulously selected based on a set of inclusion criteria related to age, educational qualifications, and field experience. The study tests five hypotheses, each scrutinized through a score-based Delphi analysis. The findings are mixed and shed new light on several dimensions of air conditioning project management. For instance, the study refutes the commonly held belief that the location of air conditioning projects significantly impacts worker safety. It also challenges the assumption that exceeding international safety standards like ISO leads to cost savings. On the other hand, the study validates the significance of global safety standards and the undeniable role of data sciences and smart technologies in enhancing human safety in the air conditioning industry. These findings not only challenge existing models, but also open avenues for further research. They indicate a complex interplay between safety, cost-effectiveness, and technological integration in air conditioning project management. The study suggests a shift towards data-driven decision-making and underscores the need for international safety standards, particularly in an era marked by rapid technological advancements and globalization. © 2023 by the authors.',\n",
       " \"Data Science is a burgeoning area in the iField. But Data Science practices have far outstripped the field's ethical safeguards. We argue that Data Science graduate education programs must address this critical problem. In this theoretical and conceptual paper, we posit an ordinary macroethics that we call data flourishing. We contend that this macroethics is most appropriately developed through a holistic, human-centered data science (HCDS)-based pedagogy that concentrates on cultivating communities of ethical practice (COEPs) through social learning. We favor embedding this macroethics throughout iField programs' graduate data science curricula and by extension, the entire data science education enterprise. This paper aligns with the 2023 ASIS&T annual meeting theme of translating research into practice, particularly the subthemes of “improving decision-making” and “understanding the power of information to develop human happiness, equality, and wellbeing.”. 86th Annual Meeting of the Association for Information Science & Technology | Oct. 27 – 31, 2023 | London, United Kingdom. Author(s) retain copyright, but ASIS&T receives an exclusive publication license.\",\n",
       " 'Data science may be used to determine similarities between musical scores. Programs are written in C++ to capture note progressions from musical scores and to compare progressions from different songs to identify overlapping areas. These tools enable the study of musical borrowing across musical genres and may assist in copyright violation cases. Results indicate that within the Celtic music genre, borrowing occurs across greater than 10% of the songs. 86th Annual Meeting of the Association for Information Science & Technology | Oct. 27 – 31, 2023 | London, United Kingdom. Author(s) retain copyright, but ASIS&T receives an exclusive publication license.',\n",
       " \"Enhanced interaction between solar-wind and Earth's magnetosphere can cause space weather and geomagnetic storms that have the potential to damage critical technologies, such as magnetic navigation, radio communications, and power grids. The severity of a geomagnetic storm is measured using the disturbance-storm-time (Dst) index. The Dst index is calculated by averaging the horizontal component of the magnetic field observed at four near-equatorial observatories and is used to drive geomagnetic disturbance models. As a key specification of the magnetospheric dynamics, the Dst index is used to drive geomagnetic disturbance models such as the High Definition Geomagnetic Model—Real Time. Since 1975, forecasting models have been proposed to forecast Dst solely from solar wind observations at the Lagrangian-1 position. However, while the recent Machine-Learning (ML) models generally perform better than other approaches, many are unsuitable for operational use. Recent exponential growth in data-science research and the democratization of ML tools have opened up the possibility of crowd-sourcing specific problem-solving tasks with clear constraints and evaluation metrics. To this end, National Oceanic and Atmospheric Administration (NOAA)'s National Centers for Environmental Information and the University of Colorado's Cooperative Institute for Research in Environmental Sciences conducted an open data-science challenge called “MagNet: Model the Geomagnetic Field.” The challenge attracted 622 participants, resulting in 1,197 model submissions that used various ML approaches. The top models that met the evaluation criteria are operationally viable and retrainable and suitable for NOAA's operational needs. The paper summarizes the competition results and lessons learned. © 2023. The Authors.\",\n",
       " 'Big data have become a core technology for providing innovative solutions in numerical applications and services in many fields. Embedded in these big data is valuable information and knowledge. This calls for data science and analytics, which has emerged as an important paradigm for driving the new economy and domains (e.g., Internet of Things, social and mobile networks, cloud computing), reforming classic disciplines (e.g., telecommunications, biology, health and social science), as well as upgrading core business and economic activity. In this article, we focus on both theoretical and practical data science and analytics. We summarize and highlight some of its challenges and solutions, which are covered in the eight articles in the current Special Issue on \"theoretical and practical data science and analytics.\" © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.',\n",
       " 'Small-molecule drugs have enabled the practice of precision oncology for genetically defined patient populations since the first approval of imatinib in 2001. Scientific and technology advances over this 20-year period have driven the evolution of cancer biology, medicinal chemistry, and data science. Collectively, these advances provide tools to more consistently design best-in-class small-molecule drugs against known, previously undruggable, and novel cancer targets. The integration of these tools and their customization in the hands of skilled drug hunters will be necessary to enable the discovery of transformational therapies for patients across a wider spectrum of cancers. Significance: Target-centric small-molecule drug discovery necessitates the consideration of multiple approaches to identify chemical matter that can be optimized into drug candidates. To do this successfully and consistently, drug hunters require a comprehensive toolbox to avoid following the “law of instrument” or Maslow’s hammer concept where only one tool is applied regardless of the requirements of the task. Combining our ever-increasing understanding of cancer and cancer targets with the technological advances in drug discovery described below will accelerate the next generation of smallmolecule drugs in oncology. © 2023 The Authors; Published by the American Association for Cancer Research.',\n",
       " 'Objective: To describe the infrastructure, tools, and services developed at Stanford Medicine to maintain its data science ecosystem and research patient data repository for clinical and translational research. Materials and Methods: The data science ecosystem, dubbed the Stanford Data Science Resources (SDSR), includes infrastructure and tools to create, search, retrieve, and analyze patient data, as well as services for data deidentification, linkage, and processing to extract high-value information from healthcare IT systems. Data are made available via self-service and concierge access, on HIPAA compliant secure computing infrastructure supported by in-depth user training. Results: The Stanford Medicine Research Data Repository (STARR) functions as the SDSR data integration point, and includes electronic medical records, clinical images, text, bedside monitoring data and HL7 messages. SDSR tools include tools for electronic phenotyping, cohort building, and a search engine for patient timelines. The SDSR supports patient data collection, reproducible research, and teaching using healthcare data, and facilitates industry collaborations and large-scale observational studies. Discussion: Research patient data repositories and their underlying data science infrastructure are essential to realizing a learning health system and advancing the mission of academic medical centers. Challenges to maintaining the SDSR include ensuring sufficient financial support while providing researchers and clinicians with maximal access to data and digital infrastructure, balancing tool development with user training, and supporting the diverse needs of users. Conclusion: Our experience maintaining the SDSR offers a case study for academic medical centers developing data science and research informatics infrastructure. © The Author(s) 2023. Published by Oxford University Press on behalf of the American Medical Informatics Association.',\n",
       " \"This paper introduces JAX-FEM, an open-source differentiable finite element method (FEM) library. Constructed on top of Google JAX, a rising machine learning library focusing on high-performance numerical computing, JAX-FEM is implemented with pure Python while scalable to efficiently solve problems with moderate to large sizes. For example, in a 3D tensile loading problem with 7.7 million degrees of freedom, JAX-FEM with GPU achieves around 10× acceleration compared to a commercial FEM code depending on platform. Beyond efficiently solving forward problems, JAX-FEM employs the automatic differentiation technique so that inverse problems are solved in a fully automatic manner without the need to manually derive sensitivities. Examples of 3D topology optimization of nonlinear materials are shown to achieve optimal compliance. Finally, JAX-FEM is an integrated platform for machine learning-aided computational mechanics. We show an example of data-driven multi-scale computations of a composite material where JAX-FEM provides an all-in-one solution from microscopic data generation and model training to macroscopic FE computations. The source code of the library and these examples are shared with the community to facilitate computational mechanics research. Program summary: Program Title: JAX-FEM CPC Library link to program files: https://doi.org/10.17632/hgwshjbcw6.1 Developer's repository link: https://github.com/tianjuxue/jax-am/tree/main/jax_am/fem Licensing provisions: GPLv3 Programming language: Python Nature of problem: Implementation of the finite element method (FEM) with several appealing features that classic FEM software typically does not have: realized with pure Python; running on CPU/GPU; differentiable for solving PDE-constrained optimization problems; seamless integration with machine learning. Solution method: Our framework JAX-FEM is based on Google JAX, a high-performance numerical computing library with automatic differentiation features and supporting both CPU/GPU. Unlike classic FEM software written in Fortran or C/C++, JAX-FEM is implemented with pure Python and can easily be installed as a Python package. We demonstrate our software by solving problems including forward PDE prediction, inverse design/optimization, and data-driven analysis. © 2023 Elsevier B.V.\",\n",
       " 'Compound structural identification for non-targeted screening of organic molecules in complex mixtures is commonly carried out using liquid chromatography coupled to tandem mass spectrometry (UHPLC-HRMS/MS and related techniques). Instrumental developments in recent years have increased the quality and quantity of data available; however, using current data analysis methods, structures can be assigned to only a small fraction of compounds present in typical mixtures. We present a new data analysis pipeline, “MSEI”, that harnesses data science methodologies to improve structural identification capabilities from tandem mass spectrometry data. In particular, feature vectors for fingerprint calculation are found directly from tandem mass spectra, strongly reducing computational costs, and fingerprint comparison uses an optimised methodology accounting for uncertainty to improve distinction between matching and non-matching compounds. MSEI builds on the identification of a small number of compounds through current state-of-the-art data analysis on UHPLC-HRMS/MS measurements and uses targeted training and tailored molecular fingerprints to focus identification to a particular molecular space of interest. Initial compound identifications are used as training data for a set of random forests which directly predict a custom 75-digit molecular fingerprint from a vectorised MS/MS spectrum. Kendrick mass defects (KMDs) for peaks as well as “lost”\\xa0fragments removed during fragmentation were found to be useful information for fingerprint prediction. Fingerprints are then compared to potential matches from the PubChem structural database using Euclidean distance, with fingerprint digit weights determined using an SVM to maximise distance between matching and non-matching compounds. Potential matches are additionally filtered for hydrophobicity based on measured retention time, using a newly developed machine learning method for retention time prediction. MSEI was able to correctly assign > 50% of structures in a test dataset and showed > 10% better performance than current state-of-the-art methods, while using an order of magnitude less computational power and a fraction of the training data. © 2023, The Author(s).',\n",
       " 'The aim of the study was to evaluate spatial connectivity and socioeconomic status of African cities using street network datasets and geospatial methods. The drivable street network was collected from OpenStreetMap, and spatial connectivity has developed at the cityscape level and central business districts (CBD). At the cityscape level, almost all studied cities have minimum spatial connectivity as illustrated by metrics like betweenness centrality, average node average and intersection density metrics where maximum values were 0.11, 6.28 and 359 nodes/km2 respectively. The spatial connectivity of CBD was higher compared cityscape level, which indicated the availability unbalanced growth of drivable street network in the sample cities. Moreover, the study has also founded relationship between spatial connectivity and socioeconomic status of cities which in turn have implications to the sustainability of urban areas. © 2023, The Author(s), under exclusive licence to Springer Nature B.V.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " '[No abstract available]',\n",
       " 'Unlike traditional offshore oil and gas projects which generally involve a limited number of structures and are confined to a relatively small seabed footprint, renewable energy projects such as offshore wind farms can comprise many tens or even hundreds of structures dispersed across an extensive area. Driven by the need for offshore wind to reduce costs while also minimising project risk, there is currently a strong focus within the offshore industry on identifying new ways to extract the full benefit from all available site investigation data and carry this benefit through to engineering design. This paper provides a demonstration of two such approaches that utilise geo-data from offshore Western Australia, potentially representative of conditions of future offshore wind farms; the first is an integrated approach combining geophysical and geotechnical data through a seismic inversion process, while the second encompasses statistical analysis of geotechnical cone penetrometer and soil strength test data combined with a Bayesian compressive sampling-based spatial interpolation method. The demonstrations yielded useful findings about the methodologies and associated input requirements. It is envisaged this work will lead to the development of an efficient integrated framework for interpreting geo-data that will inform future offshore site investigation and geotechnical design practice. © 2023 The Authors',\n",
       " 'This comprehensive review provides an in-depth analysis of graph theory, various graph types, and the role of graph visualization in scientific studies. Graphs serve as powerful tools for modeling and analyzing complex systems in diverse disciplines. The introduction highlights the importance of graphs as a visual representation in scientific research, enabling a better understanding of complex data. Infographics and knowledge graphs have gained significant popularity in recent years due to their effectiveness in conveying information. The review starts by exploring the foundations of graph theory, covering key concepts, algorithms, and applications. It discusses the different types of graphs, including directed, undirected, weighted, and bipartite graphs, and their specific use cases in scientific studies. Special attention is given to special graphs, such as complete graphs, trees, and social networks, which have unique properties and play a significant role in various scientific domains. The review showcases their applications and contributions in fields like biology, social sciences, network analysis, and data mining. Graph visualization emerges as a crucial aspect of understanding and interpreting complex data structures. The review emphasizes the challenges and advancements in graph visualization techniques, enabling researchers to effectively communicate and analyze graph-based information. In conclusion, this comprehensive review serves as a valuable resource for researchers in understanding the principles and applications of graph theory in scientific studies. The exploration of graph types, special graphs, and graph visualization techniques provides insights into the diverse uses and potential of graphs in various scientific disciplines. © 2023 Elsevier B.V.',\n",
       " 'Esports domain is growing with an goal to give the player a skill set to improve his skills. The paper, gives detail analysis performed on a dataset which is built by our team. The data used in the dataset is collected from Counter-Strike: Global Offensive (CS:GO) world championship major where top teams from all over the world had participated. The main goal was to create a customized dataset, predict the winning percentage for a team based on various features. It also provides descriptive analysis for a team s performance and prescriptive analysis for any improvements required to made by the team in order to improve their winning percentage and implementing different machine algorithms on the same. Extra features such as assigning roles, ranking and rating were also performed to provide in depth results. © 2023 American Institute of Physics Inc.. All rights reserved.',\n",
       " 'The chapter explores the use of machine learning, data science, and Python in the context of cricket analytics. It highlights the importance of interdisciplinary collaboration and its potential to enhance the accuracy and speed of cricket analytics. It discusses the various data sources that can be used for cricket analytics and how machine learning algorithms can extract valuable insights. The chapter also provides an overview of various Python libraries commonly used in cricket analytics and explains how they can be used for data cleaning, feature engineering, and model building. Additionally, the chapter discusses the challenges and limitations of cricket analytics and provides suggestions for future research directions. © 2023, IGI Global. All rights reserved.',\n",
       " 'The world is approaching a point where big data will start to play a beneficial role in many industries and organizations. Today, analyzing data for new insights has become an everyday norm, increasing the need for data analysts to use efficient and appropriate tools to provide quick and valuable results to clients. Existing research in the field currently lacks a full coverage of all essential algorithms, leaving a knowledge void for practical implementation and code in Python with all needed libraries and links to datasets used. © 2023 by IGI Global. All rights reserved.',\n",
       " \"Python libraries are used in this chapter to create data science models. Data science is the construction of models that can predict and act on data, which is a subset of machine learning. Data science is an essential component of a number of fields because of the exponential growth of data. Python is a popular programming language for implementing machine learning models. The chapter discusses machine learning's role in data science, Python's role in this field, as well as how Python can be utilized. A breast cancer dataset is used as a data source for building machine learning models using Python libraries. Pandas, numpy, matplotlib, seaborn, scikitlearn, and tensorflow are some Python libraries discussed in this chapter, in addition to data preprocessing methods. A number of machine learning models for breast cancer treatment are discussed using this dataset and Python libraries. A discussion of machine learning's future in data science is provided at the conclusion of the chapter. Python libraries for machine learning are very useful for data scientists and researchers in general. © 2023, IGI Global. All rights reserved.\",\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'The proceedings contain 30 papers. The topics discussed include: odML-tables as a metadata standard in microneurography; how to make outpatient healthcare data in Germany available for research in the dynamic course of digital transformation; a unified data architecture for assessing motor symptoms in Parkinson’s disease; reusing biomedical data as agreed – towards structured metadata for data use agreements; adaptability of existing feasibility tools for clinical study research data platforms; clustering on player types of students in health science – trial and data analyses; comparison of German translations of the system usability scale – which to take?; usability analysis of a medication visualization tool for decision support; analysis of the usage context of an mHealth application for equestrians; and understanding human-computer interactions in restricted clinical environments.',\n",
       " 'We might become interested in stock market investment during the course of our lives. But to do that, we first need to have a clear understanding and analysis of the market. We may be unsure of whether to buy or sell stocks; thus, this is vital. It can be challenging to determine whether a stock will rise or decline in value or whether it will be a successful investment. In the event that the stock decreases, we can be unsure about whether to sell or stay on to it. Given the large number of investors who purchase stocks globally and the potential for significant losses, stock market analysis is crucial. The goal is to develop a project that analyzes the stock market and aids in our decision-making when it comes to purchasing and selling stocks. © 2023, IGI Global. All rights reserved.',\n",
       " 'Laboratory automation and data science are valuable new skills for all chemists, but most pedagogical activities involving automation to date have focused on upper-level coursework. Herein, we describe a combined computational and experimental lab suitable for a first-year undergraduate general chemistry course, in which these topics are introduced in the context of determination of the solubility equilibrium constant of lead iodide. Students analyze their data using logistic regression analysis, which has a physical interpretation in terms of the solubility equilibrium expression and its stoichiometric coefficients. In addition to laboratory automation, data visualization, and data fitting skills, students also practice core laboratory skills such as the preparation of stock solutions using a volumetric flask and the use of micropipets. To keep the lab affordable, we demonstrate the use of a low-cost 3D-printed liquid dispensing robot to perform the automated experiment in addition to a commercial liquid-handling robot. Example pre- and post-lab computational notebooks are provided in both Mathematica and Python programming languages. © 2023 American Chemical Society and Division of Chemical Education, Inc.',\n",
       " '[No abstract available]',\n",
       " 'Population-scale biobanks linked to electronic health record data provide vast opportunities to extend our knowledge of human genetics and discover new phenotype-genotype associations. Given their dense phenotype data, biobanks can also facilitate replication studies on a phenome-wide scale. Here, we introduce the phenotype-genotype reference map (PGRM), a set of 5,879 genetic associations from 523 GWAS publications that can be used for high-throughput replication experiments. PGRM phenotypes are standardized as phecodes, ensuring interoperability between biobanks. We applied the PGRM to five ancestry-specific cohorts from four independent biobanks and found evidence of robust replications across a wide array of phenotypes. We show how the PGRM can be used to detect data corruption and to empirically assess parameters for phenome-wide studies. Finally, we use the PGRM to explore factors associated with replicability of GWAS results. © 2023 The Authors',\n",
       " 'This paper presents a case study on using Emacs and Org-mode for literate programming in undergraduate computer and data science courses. Over three academic terms, the author mandated these tools across courses in R, Python, C++, SQL, and more. The onboarding relied on simplified Emacs tutorials and starter configurations. Students gained proficiency after undertaking initial practice. Live coding sessions demonstrated the flexible instruction enabled by literate notebooks. Assignments and projects required documentation alongside functional code. Student feedback showed enthusiasm for learning a versatile IDE, despite some frustration with the learning curve. Skilled students highlighted efficiency gains in a unified environment. However, the uneven adoption of documentation practices pointed to a need for better incorporation into grading. Additionally, some students found Emacs unintuitive, desiring more accessible options. This highlights a need to match tools to skill levels, potentially starting novices with graphical IDEs before introducing Emacs. The key takeaways are as follows: literate programming aids comprehension but requires rigorous onboarding and reinforcement, and Emacs excels for advanced workflows but has a steep initial curve. With proper support, these tools show promise for data science education. © 2023 by the author.',\n",
       " 'One of the most important elements of the reliability structure of a motor vessel is its power subsystem, with the most crucial component being the engine. An engine failure excludes the ship from operation or significantly limits its operation. Therefore, accurate fault diagnosis should be a crucial issue for modern maintenance strategies. In mechanical engineering, the vibration and acoustic signals recorded during the operation of the device are the most meaningful data used to identify the reliability state. In this paper, a novel system-oriented method of reliability state identification is proposed. The method consists of the analysis of the vibration and noise signals collected on each of the engine cylinders using supervised machine learning. The main novelty of this method is data augmentation application and SVM classifier implementation. Due to these aspects, the method becomes robust in the case of poor-quality data or a limited and incomplete learning dataset. The quality of the proposed identification method was evaluated by addressing a new industrial issue (Sulzer 6AL20/24 marine engine reliability state identification). During the tests, the efficiency of the method was analyzed in the case of a complete learning data set (all types of inability states were presented in the learning data set) and an incomplete learning data set (in the testing data set, there were new types of inability states). As a result, in both cases, a very high (100%) identification accuracy of the reliability state and the type of inability state was obtained. This is a significant increase in accuracy (4.6% for the complete and 22% for the incomplete learning data set) in comparison to the efficiency of the same method without the use of machine learning and data science. © 2023 by the authors.',\n",
       " 'The concept of a digital twin (DT) has gained significant attention in academia and industry because of its perceived potential to address critical global challenges, such as climate change, healthcare, and economic crises. Originally introduced in manufacturing, many attempts have been made to present proper definitions of this concept. Unfortunately, there remains a great deal of confusion surrounding the underlying concept, with many scientists still uncertain about the distinction between a simulation, a mathematical model and a DT. The aim of this paper is to propose a formal definition of a digital twin. To achieve this goal, we utilize a data science framework that facilitates a functional representation of a DT and other components that can be combined together to form a larger entity we refer to as a digital twin system (DTS). In our framework, a DT is an open dynamical system with an updating mechanism, also referred to as complex adaptive system (CAS). Its primary function is to generate data via simulations, ideally, indistinguishable from its physical counterpart. On the other hand, a DTS provides techniques for analyzing data and decision-making based on the generated data. Interestingly, we find that a DTS shares similarities to the principles of general systems theory. This multi-faceted view of a DTS explains its versatility in adapting to a wide range of problems in various application domains such as engineering, manufacturing, urban planning, and personalized medicine. © 2023 by the author.',\n",
       " '[No abstract available]',\n",
       " \"This study aims to analyze the multifaceted factors influencing the innovative capabilities of data science professionals in China and assess the impact of simulations on their innovative skills. The sample comprises seventeen experts who actively participated in discussions and provided 36 perspectives on the factors affecting their innovation abilities. The research methodology utilized the Delphi method, involving four rounds of questionnaires distributed to 363 data science professionals to evaluate the factors affecting their innovation capacity. The data was rigorously analyzed using mathematical statistics and SPSS, with a strong emphasis on questionnaire validity and reliability. In the reliability analysis, Cronbach's α was found to be 0.98, indicating a high level of internal consistency. The research results yielded an average score of 4.79, SD = 0.39, IQR = 1, reflecting a strong consensus among experts in agreement with the research findings. Exploratory factor analysis was employed for validity assessment, revealing that the 12th factor accounted for a cumulative variance explanation rate of 76.54%, exceeding the threshold of 60%, signifying the robust structural validity of the questionnaire data. The study also utilized AMOS software to simulate sample data and assess the influence coefficients of individual, organizational, and family characteristics on innovation capacity, resulting in values of 0.53, 0.39, and 0.22, respectively, all greater than 0, indicating favorable influence relationships. Building upon these findings, a comprehensive model of creativity abilities among Chinese data science professionals is proposed. This research critically examines the innovation potential of data science professionals in Chinese academia, with the overarching goal of enhancing their creative skills and competitiveness within the data science field. Additionally, it lays the theoretical groundwork for fostering innovation within the university setting. © 2023, Bright Publisher. All rights reserved.\",\n",
       " '[No abstract available]',\n",
       " 'Data science and material informatics are gaining traction in alloy design. This is due to increasing infrastructure, computational capabilities and established open-source composition-structure-property databases increasingly becoming available. Additionally, the popularization of data science techniques and the drive to reduce overall material life-cycle cost by ∼60% have necessitated increased use of the technique. Alloy design is a multi-optimization problem hence the Edisonian approach is no more viable from cost, labour, and time-to-market perspectives. Although, there have been successful application of data science and material informatics in alloy design, there are drawbacks. This review provides a critical assessment of limitations associated with data science and materials informatics to alloy discovery and property characterization. Among these are cost, false positives, over – and underestimation of properties, lack of experimental data to validate simulated results, lack of state-of-the-art facilities in most developing countries and uncertainty modelling. The implications and areas for future research directions are highlighted. © 2023',\n",
       " '[No abstract available]',\n",
       " \"This paper analyses the human right to science (RtS) in relation to data science (DS) and its applications, particularly, data monetization. It advances an approach that balances three aspects of RtS, namely, protection from harmful science, benefit-sharing and participation in science and derives three corresponding sets of state duties. First, RtS implies the duty to end data monetization in so far as it entails practices harmful to human rights, including unlawful interference with privacy. Second, while data monetization exists, RtS entails the duty to distribute monetary benefits through an RtS-based universal basic income (UBI). Third, RtS entails the duty to facilitate ordinary people's participation in DS and prioritize non-profit pro-social uses of DS as in citizen or community DS. The proposed RtS analysis of DS engages policy responses to artificial intelligence (AI) and material inequality, namely, AI regulation, monetary benefits from data, UBI and the 'data for good' movement. No new data were generated or analysed in support of this research. © 2023 The Author(s). Published by Oxford University Press.\",\n",
       " 'There is a widespread stated desire amongst both public and private organizations worldwide to engage in more significant “evidence-based reasoning” and to be more “data-driven.” We argue that these two goals are proxies for the often-unstated goal of improving the exploration of possible futures as foresights that could lead to better strategic decisions and improved business outcomes. From this perspective, data and analytics hold great promise and are necessary—but not sufficient—for improving strategic decision-making. Something more is needed to realize this potential. We specify how to fill this gap using an integration framework between technology and decision-makers, which is especially appropriate in complex and/or volatile environments. Our solution—which comprises a methodology as well as a software architecture—therefore unifies not only human decision makers to technology but each other and also integrates several disciplines that have been hitherto unnecessarily separated. Thereby, it could help organizations to address increasing challenges better as well as improve the exploration of possible futures. © 2023 Elsevier Ltd',\n",
       " \"As the overdose epidemic overwhelmed medicolegal death investigation offices and toxicology laboratories, the King County Medical Examiner's Office responded with “real-time” fatal overdose surveillance to expedite death certification and information dissemination through assembling a team including a dedicated medicolegal death investigator, an information coordinator, and student interns. In-house testing of blood, urine, and drug evidence from scenes was performed using equipment and supplies purchased for surveillance. Collaboration with state laboratories allowed validation. Applied forensic epidemiology accelerated data dissemination. From 2010 to 2022, the epidemic claimed 5815 lives in King County; the last 4 years accounted for 47% of those deaths. After initiating the surveillance project, in-house testing was performed on blood from 2836 decedents, urine from 2807, and 4238 drug evidence items from 1775 death scenes. Time to complete death certificates decreased from weeks to months to hours to days. Overdose-specific information was distributed weekly to a network of law enforcement and public health agencies. As the surveillance project tracked the epidemic, fentanyl and methamphetamine became dominant and were associated with other indicators of social deterioration. In 2022, fentanyl was involved in 68% of 1021 overdose deaths. Homeless deaths increased sixfold; in 2022, 67% of 311 homeless deaths were due to overdose; fentanyl was involved in 49% and methamphetamine in 44%. Homicides increased 250%; in 2021, methamphetamine was positive in 35% of 149 homicides. The results are relevant to the value of rapid surveillance, its impact on standard operations, selection of cases requiring autopsy, and collaboration with other agencies in overdose prevention. © 2023 American Academy of Forensic Sciences.\",\n",
       " \"An explosion of data available in the life sciences has shifted the discipline toward genomics and quantitative data science research. Institutions of higher learning have been addressing this shift by modifying undergraduate curriculums resulting in an increasing number of bioinformatics courses and research opportunities for undergraduates. The goal of this study was to explore how a newly designed introductory bioinformatics seminar could leverage the combination of in-class instruction and independent research to build the practical skill sets of undergraduate students beginning their careers in the life sciences. Participants were surveyed to assess learning perceptions toward the dual curriculum. Most students had a neutral or positive interest in these topics before the seminar and reported increased interest after the seminar. Students had increases in confidence level in their bioinformatic proficiency and understanding of ethical principles for data/genomic science. By combining undergraduate research with directed bioinformatics skills, classroom seminars facilitated a connection between student's life sciences knowledge and emerging research tools in computational biology. © 2023 The Authors. Biochemistry and Molecular Biology Education published by Wiley Periodicals LLC on behalf of International Union of Biochemistry and Molecular Biology.\",\n",
       " 'The Severe Acute Respiratory Syndrome CoronoVirus2 (SARS-CoV-2) causes the infectious illness Covid-19 (Corona Virus Disease of 2019). The majority of virus-infected persons have mild to moderate respiratory illness and recover on their own. However, some people get serious ailments and need medical attention. Covid-19 can potentially make anyone seriously ill or cause passing away at any age. Nowadays, medical science is based on data science technology to achieve new milestones in genetics, genomics, patient-customer assistance, clinical imaging, drug discovery, and predictive medicine. It has gained due to Covid-19. This paper proposes a novel method: Extended and Compressed Convolution Hybrid Leader-based Optimization (ECCHLO) to differentiate between Covid-19 infected persons and healthy individuals. This outline involves four steps. The initial step is preprocessing the X-ray and Computed Tomography (CT) scan images gathered from the datasets. In the second step, feature extraction is performed. Next, feature fusion is performed to eliminate redundant and unnecessary data. Finally, the Extended Hybrid Leader-Based Optimization (EHLBO) algorithm is utilized to tune the parameters of compressed convolutional neural networks. The ECCHLO model is examined and tested on four datasets, namely SARS-COV-2, Covid-19 radiography database, Covid-CT, and Covid-19 Image datasets. The accuracy of these four datasets is 99.6%, 99.5%, 99.3%, and 100%, respectively, which are higher than the other techniques. This demonstrates that the ECCHLO model is reliable and stable for identifying a person infected with Covid-19. © 2023, The Author(s), under exclusive licence to The Japanese Society for Artificial Intelligence and Springer Nature Japan KK, part of Springer Nature.',\n",
       " \"Plasticity is increasingly recognized as a critical concept in psychiatry and mental health because it allows the reorganization of neural circuits and behavior during the transition from psychopathology to wellbeing. Differences in individual plasticity may explain why therapies, such as psychotherapeutic and environmental interventions, are highly effective in some but not in all patients. Here I propose a mathematical formula to assess plasticity – i.e., the susceptibility to change – to identify, at baseline, which individuals or populations are more likely to modify their behavioral outcome according to therapies or contextual factors. The formula is grounded in the network theory of plasticity so that, when representing a system (e.g., a patient's psychopathology) as a weighed network where the nodes are the system features (e.g., symptoms) and the edges are the connections (i.e., correlations) among them, the network connectivity strength is an inverse measure of the plasticity of the system: the weaker the connectivity, the higher the plasticity and the greater the susceptibility to change. The formula is predicted to be generalizable, measuring plasticity at multiple scales, from the single cell to the whole brain, and can be applied to a wide range of research fields, including neuroscience, psychiatry, ecology, sociology, physics, market and finance. © 2023 Elsevier Ltd\",\n",
       " 'The sustainable phaseout of high global warming potential hydrofluorocarbon (HFC) refrigerant mixtures requires novel solvents, such as ionic liquids (ILs), for new HFC reuse and recycle technologies. Accurate, predictive, and interpretable thermodynamic models for HFC/IL mixtures are essential for multiscale design schemes aiding this phaseout. Still, there is limited guidance regarding the best thermodynamic model for an HFC/IL system. We propose a rigorous thermodynamic model selection and analysis workflow for HFC/IL mixtures which harnesses data science tools – visualization, nonlinear regression, Akaike information criteria, Fischer information matrix (FIM)-based identifiability and uncertainty analyses, and model-based design of experiments methods – to evaluate the accuracy, predictive capability, and interpretability of a thermodynamic model. The open-source IDAES™ platform facilitates training and comparison of sixteen candidate HFC/IL thermodynamic models, including two cubic equations of state, Peng–Robinson and Soave–Redlich–Kwong, and eight variations on temperature dependence within a classical van der Waals mixing rule. We apply this analysis to models for three HFC/IL systems: HFC-32/[emim][TF2N], HFC-125/[emim][TF2N], and HFC-32/[bmim][PF6]. For these mixtures, we observe that models with a temperature dependent mixing rule are consistently ranked higher by Akaike information criteria for model selection. However, these models may still have high parameter uncertainty and correlation, indicating that data at multiple temperatures should be obtained. This result differs from the current practice of generating single isotherm dataset for most new HFC/IL mixtures. Additionally, we find that the most valuable experiments are taken at the bounds of composition, temperature (e.g., 273 and 348 K), and pressure (e.g., 1 MPa) measurements. This analysis guides data generation efforts, showing that optimally selected measurements across multiple temperatures are adequate for regressing thermodynamic models for multiscale process design. © 2023 Elsevier B.V.',\n",
       " '[No abstract available]',\n",
       " 'The implementation of data science in cyber security to help preserve against attacks and improve approach to better conflict cyber warning has many welfares. Honestly, data science has changed cyber security and the reaction has been profound and transformed. Cyber security uses data science to keep digital devices, services, systems, and software Safe from cyberattacks. Here, we talk about cyber security data science, present day uses for the cyber security field and data guide quickwitted managerial systems that can safeguard our system from cyber-attacks. © 2023, Bentham Books imprint.',\n",
       " \"The field of data science relies heavily on mathematical analysis. A solid foundation in certain branches of mathematics is essential for every data scientist already working in the field or planning to enter it in the future. In whatever area we focus on, data science, machine learning engineering, business intelligence development, data architecture, or another area of expertise, it is important to examine the several kinds of mathematical prerequisites and insights and how they're applied in the field of data science. Machine learning algorithms, data analysis and analyzing require mathematics. Mathematics is not the only qualification for a data science education and profession but is often the most significant. Identifying and translating business difficulties into mathematical ones are a crucial phase in a data scientist's workflow. In this study, we describe the different areas of mathematics utilized in data science to understand mathematics and data science together. © 2023, Bentham Books imprint.\",\n",
       " 'The word \" data science\" has become more popular in recent years, with a growing number of people embracing it. Only a small minority of people, on the other hand, are able to offer a clear explanation of what the term refers to when it is used in context. With no defined term to communicate and understand one another, it is difficult for organizations that are devoted to the collaboration, utilization, and application Data Science to communicate and understand one another. As a result of technological advancements, it has become increasingly difficult to define and execute Data Science in a way that is compatible with how it was previously considered and understood in the past. Specifically, we could now set out to develop definitions of Data Science that are representatives of current academic and industrial interpretations and perceptions, map these perspectives to newer domains of Data Science, and then determine whether or not this mapping translates into an effective practical curriculum for academics. Aspects of data science that differentiate it include how it is now used and how it is projected to be used in the future. Data science is also characterized by its ability to forecast the future. © 2023, Bentham Books imprint.',\n",
       " 'Mathematics is the rock-solid foundation of everything that happens when science is present, and it is also extremely important in the field of data science since mathematical ideas assist discover models and facilitate the development of algorithms. But, the concepts they present and the tools they enable are the only reasons statistics and arithmetic are so crucial to data science. There is a particular type of mathematical reasoning that is necessary to grasp data, beyond the fundamentals of calculus, discrete mathematics, and linear algebra. For the implementation of such algorithms in data science, a thorough understanding of the various principles of probability and statistics is essential. Machine learning is one of the many modern data science techniques that has a strong mathematical base. The evidence presented in this chapter backs up our earlier claim that math and statistics are the fields that offer the greatest tools and approaches for extracting structure from data. For newcomers coming from other professions to data science, math proficiency is crucial. © 2023, Bentham Books imprint.',\n",
       " \"Data science is often used as an umbrella term to include various techniques for extracting insights and knowledge from complex structured and unstructured data. It often relies on a large amount of data (big data) and the application of different mathematical methods, including computer vision, NLP (or natural language processing), and data mining techniques. Advances in data science have resulted in a wider variety of algorithms, specialized for different applications and industries, such as healthcare, finance, marketing, supply chain, management, and general administration. Specifically, data science methods have shown promise in addressing key healthcare challenges and helping healthcare practitioners and leaders make datadriven decision-making. This chapter focuses on healthcare issues and how data science can help solve these issues. The chapter will survey different approaches to defining data science and why any organization should use data science. This chapter will also present different skills required for an effective healthcare data scientist and discusses healthcare leaders' behaviors that in impacting their organizational processes. © 2023, Bentham Books imprint.\",\n",
       " \"Advanced Mathematical Applications in Data Science comprehensively explores the crucial role mathematics plays in the field of data science. Each chapter is contributed by scientists, researchers, and academicians. The 13 chapters cover a range of mathematical concepts utilized in data science, enabling readers to understand the intricate connection between mathematics and data analysis. The book covers diverse topics, including, machine learning models, the Kalman filter, data modeling, artificial neural networks, clustering techniques, and more, showcasing the application of advanced mathematical tools for effective data processing and analysis. With a strong emphasis on real-world applications, the book offers a deeper understanding of the foundational principles behind data analysis and its numerous interdisciplinary applications. This reference is an invaluable resource for graduate students, researchers, academicians, and learners pursuing a research career in mathematical computing or completing advanced data science courses. Key features: - Comprehensive coverage of advanced mathematical concepts and techniques in data science - Contributions from established scientists, researchers, and academicians - Real-world case studies and practical applications of mathematical methods - Focus on diverse areas, such as image classification, carbon emission assessment, customer churn prediction, and healthcare data analysis - In-depth exploration of data science's connection with mathematics, computer science, and artificial intelligence - Scholarly references for each chapter - Suitable for readers with high school-level mathematical knowledge, making it accessible to a broad audience in academia and industry. © 2023, Bentham Books imprint. All rights reserved.\",\n",
       " 'Artificial Intelligence and Data Science in Recommendation System: Current Trends, Technologies and Applications captures the state of the art in usage of artificial intelligence in different types of recommendation systems and predictive analysis. The book provides guidelines and case studies for application of artificial intelligence in recommendation from expert researchers and practitioners. A detailed analysis of the relevant theoretical and practical aspects, current trends and future directions is presented. The book highlights many use cases for recommendation systems: - Basic application of machine learning and deep learning in recommendation process and the evaluation metrics - Machine learning techniques for text mining and spam email filtering considering the perspective of Industry 4.0 - Tensor factorization in different types of recommendation system - Ranking framework and topic modeling to recommend author specialization based on content. - Movie recommendation systems - Point of interest recommendations - Mobile tourism recommendation systems for visually disabled persons - Automation of fashion retail outlets - Human resource management (employee assessment and interview screening) This reference is essential reading for students, faculty members, researchers and industry professionals seeking insight into the working and design of recommendation systems. © 2023, Bentham Books imprint. All rights reserved.',\n",
       " 'In this article, the insights of a garment retail store have been studied with respect to the attributes of the dresses and sales information. Mention that each dress in fashion retail has several attributes or features. These features play a critical role in the selection of consumers or customers. This study tries to establish the relationship among these features by which the importance of the attributes is evaluated concerning sales. Furthermore, this paper tries to automate the process of the recommendation of the dresses by using these attributes. It is merely a binary classification but useful for retail sales. Moreover, the demand for sales is estimated over a period. All these objectives are achieved through using one or more data science techniques. The case study shows that the algorithms of data science are helpful in the decision-making of fashion retail. © 2023 Bentham Science Publishers. All rights reserved.',\n",
       " 'The radical growth in the availability of both data and computational resources required for processing data, has led to a corresponding increase in demand for data scientists. Following the universities and the industrial demand, many initiatives to teach data science to high school pupils are emerging. The question we address is: How should data science educators be prepared to teach data science? We describe our experience teaching the Methods of Data Science Teaching (MDST) course and illustrate how we analyzed its design and implementation using the TPACK framework of teachers’ knowledge. © 2023 Association for Computing Machinery. All rights reserved.',\n",
       " 'The study of non-natural biocatalytic transformations relies heavily on empirical methods, such as directed evolution, for identifying improved variants. Although exceptionally effective, this approach provides limited insight into the molecular mechanisms behind the transformations and necessitates multiple protein engineering campaigns for new reactants. To address this limitation, we disclose a strategy to explore the biocatalytic reaction space and garner insight into the molecular mechanisms driving enzymatic transformations. Specifically, we explored the selectivity of an “ene”-reductase, GluER-T36A, to create a data-driven toolset that explores reaction space and rationalizes the observed and predicted selectivities of substrate/mutant combinations. The resultant statistical models related structural features of the enzyme and substrate to selectivity and were used to effectively predict selectivity in reactions with out-of-sample substrates and mutants. Our approach provided a deeper understanding of enantioinduction by GluER-T36A and holds the potential to enhance the virtual screening of enzyme mutants. © 2023 American Chemical Society.',\n",
       " \"In the era of distributed computing, Cloud Computing (CC) has been considered an emerging technology that delivers on-demand services using the worldwide online medium. One of this technology's main challenges is scheduling tasks and allocating resources to achieve the best performance with minimal optimized execution time, and less resource time and usage. Thus, the importance of task priority has been recognized by many researchers since tasks submitted to the cloud can have various sizes, resource utilization, and execution times. Therefore, existing task scheduling algorithms prioritize tasks, where the task with the highest priority is allocated to the available resource. This paper investigated the task priority problem in cloud-based Task Scheduling (TS) algorithms via three different directions centered around the following contributions; first, a fruitful discussion and comparison of some selected cloud priority-based TS algorithms regarding their scheduling method and parameters, algorithm performance, and limitations reflecting how the priority issue is handled in the cloud environment. In the Data Science and Artificial Intelligence age, the interaction and intervention between the Big Data sector and CC are gradually advancing due to mutual development. Cloud environments have become a prominent platform for big data applications, forcing the integration between Data Science and CC techniques to evolve. On this basis, secondly, the paper addresses the directions of potential future research of cloud priority-based TS schemes in Data Science. Finally, as a result, a conceptual framework incorporating a task content-type component is proposed to support the processing and scheduling of data-related tasks submitted to the cloud. © 2023 Little Lion Scientific.\",\n",
       " '[No abstract available]',\n",
       " '\"Data Scientist\"has often been considered as the sexiest job of the 21st century. As a consequence, the spectrum of data science education programs has increased significantly in recent years, and there is a high demand for data scientists at many companies. However, what training is required to become a data scientist? What is the role of data scientists in current enterprises? Is the training well-aligned to the practical needs of a job? In this article, we will address these questions by evaluating a survey of people who were trained in a continuing education program in data science in Switzerland. Our study sheds lights on the practical aspects of the data science education and how this newly-gained knowledge can successfully be applied in an enterprise. One of the highlights from the point of view of the database community is the important role of SQL in data science. © 2023 Copyright is held by the owner/author(s).',\n",
       " \"The intersection of women's health and data science is a field of research that has historically trailed other fields, but more recently it has gained momentum. This growth is being driven not only by new investigators who are moving into this area but also by the significant opportunities that have emerged in new methodologies, resources, and technologies in data science. Here, we describe some of the resources and methods being used by women's health researchers today to meet challenges in biomedical data science. We also describe the opportunities and limitations of applying these approaches to advance women's health outcomes and the future of the field, with emphasis on repurposing existing methodologies for women's health. © 2023 Annual Review of Biomedical Data Science. All rights reserved.\",\n",
       " 'Autism spectrum disorder (autism) is a neurodevelopmental delay that affects at least 1 in 44 children. Like many neurological disorder phenotypes, the diagnostic features are observable, can be tracked over time, and can be managed or even eliminated through proper therapy and treatments. However, there are major bottlenecks in the diagnostic, therapeutic, and longitudinal tracking pipelines for autism and related neurodevelopmental delays, creating an opportunity for novel data science solutions to augment and transform existing workflows and provide increased access to services for affected families. Several efforts previously conducted by a multitude of research labs have spawned great progress toward improved digital diagnostics and digital therapies for children with autism. We review the literature on digital health methods for autism behavior quantification and beneficial therapies using data science. We describe both case-control studies and classification systems for digital phenotyping. We then discuss digital diagnostics and therapeutics that integrate machine learning models of autism-related behaviors, including the factors that must be addressed for translational use. Finally, we describe ongoing challenges and potential opportunities for the field of autism data science. Given the heterogeneous nature of autism and the complexities of the relevant behaviors, this review contains insights that are relevant to neurological behavior analysis and digital psychiatry more broadly. © 2023 Annual Review of Biomedical Data Science. All rights reserved.',\n",
       " 'The term of “data is the new gold” is commonly used now days. To remain competitive in a tight economy, data processing is one of the core elements allowing businesses or organizations to innovate. Consequently, data scientist is nowadays enhanced the most in-demand profession in the industry around the world. Nevertheless, contradict with the high-demand data science careers, in Indonesia there is a shortage of talent in the field. Therefore, the purpose of this research study is to identify the actual condition of qualifications of data scientist in Indonesia. This research was conducted using qualitative method with 15 respondents consist of data scientist in the Jakarta metropolitan area (Jabodetabek). The findings of this study suggest that there are some inadequate qualifications of data scientist in Indonesia. To further our research, we are currently now in the progress of investigating the gap of data scientist qualifications in Indonesia using quantitative research method. © 2023 American Institute of Physics Inc.. All rights reserved.',\n",
       " 'Built on the success of the past five years, KDD DSHealth 2023 will further catalyze the development of links between academic and industrial data science groups. The workshop aims to stimulate discussion on strategic areas for development and to facilitate future cross-disciplinary collaborations. In accordance with the multi-year goal to continue fostering this community via timely topics, this year the workshop will focus on the applications and new development of generative models in healthcare, including the new development and application of LLMs. The workshop invites full papers, as well as work-in-progress on the application of data science in healthcare. The workshop will feature two invited talks from eminent speakers, spanning academia, industry, clinical researchers, and governmental regulatory bodies. In addition, we will invite community members to submit their research works and bring them for discussion. The summary gives a brief description of the half-day workshop to be held on August 7th, 2023. © 2023 Owner/Author.',\n",
       " 'This workshop will bring together researchers and practitioners across different strands of data science research and a wide range of important real-world application domains. The objective is to share the current state of research and practice, explore future work directions, and create collaboration opportunities. In addition, the workshop will emphasize highlighting data science approaches for tackling the United Nations Sustainable Development Goals (see preliminary agenda below). The organizers believe that data science research has an important role to play in providing unique insights about critical challenges faced by marginalized communities around the world; we encourage submissions from both data science researchers as well as social workers, agronomists, epidemiologists, health policy researchers, and other domain experts who are interested in engaging with the SIGKDD community. © 2023 Owner/Author.',\n",
       " 'Artificial Intelligence (AI) has enabled breakthroughs on sports analytics in recent years by allowing the extraction of valuable insights and new understandings from vast amounts of sports data. However, due to the highly interdisciplinary nature of the topic, cross-pollination of ideas and methodologies between AI and sports science as well as across different sports remain limited. To address the gap, we propose the first KDD workshop on Data Science and AI for Sports. We aim to bring together researchers and practitioners working on a broad array of sports-related AI/ML use cases, to exchange research ideas, draw connection among various disciplines, and identify challenges and new research questions for future work. © 2023 Owner/Author.',\n",
       " 'Responsible Geo-AI encourages the design and development of spatial methods, processes, algorithms, and systems to discover spatial patterns (e.g., hotspots, colocations) that reduce adverse impacts on the communities that use them. We propose a vision of Geo-AI which can be found at the intersection of spatial data science and the emerging paradigm of responsible AI. Increasingly, governments and industry are calling for ethical and responsible application of AI research. Through this paper, we hope to encourage engagement with the topic of responsible computing, highlight some of the relevant challenges in the field of spatial data science, and advocate for the inclusion of ethical principles in addressing them. © 2023 ACM.',\n",
       " 'This survey paper is intended to prevent epidemic diseases and pandemic diseases. According to the WHO every year in the world over 17 million people die due to this type of disease. Epidemic diseases have lower transmission rate than pandemic diseases and they spread in a bounded area. On the other hand, pandemic diseases have higher transmission rate and it can easily spread in an immense area. We can control this type of disease in its initial stages before it becomes a fatal disease like covid-19. Lack of knowledge in peoples and inefficient systems used by higher authorities in that region are the main reasons to spread diseases in larger areas. But using data science and the epidemic compartment models it’s possible to control infectious diseases in its initial stages. For different diseases there are different compartment algorithms that are able to estimate the number of cases in the future. These models often use ordinary differential equations for predicting things. Using data science, we are able to find what are key factors responsible for the spreading of that particular disease. © 2023 Auricle Global Society of Education and Research.',\n",
       " 'Management science (MS) uses a variety of scientific researchbased principles and analytical methods, such as mathematical modeling and data analysis, to make decisions and solve complex problems, and has strong connections to management, data, economics, engineering, and other fields. The scientific MS community has grown significantly over the past few decades, particularly in sustainable development, decision support systems, and data science. This paper gives a brief introduction to Volume I of the seventeenth ICMSEM proceedings. First, the key MS research areas are reviewed and the reasons given as to why sustainable development, decision support systems, and data science have been hotspots. Then, the literature in the primary study areas in the seventeenth ICMSEM proceedings Volume I is summarized. Finally, CiteSpace is employed to analyze future MS developments. ICMSEM continues to provide a valuable forum for academic exchanges and communication to promote future innovation in management science and engineering management (MSEM). © ICMSEM 2023',\n",
       " '[No abstract available]',\n",
       " 'SkyPortal is an open-source software package designed to discover interesting transients efficiently, manage follow-up, perform characterization, and visualize the results. By enabling fast access to archival and catalog data, crossmatching heterogeneous data streams, and the triggering and monitoring of on-demand observations for further characterization, a SkyPortal-based platform has been operating at scale for >2 yr for the Zwicky Transient Facility Phase II community, with hundreds of users, containing tens of millions of time-domain sources, interacting with dozens of telescopes, and enabling community reporting. While SkyPortal emphasizes rich user experiences across common front-end workflows, recognizing that scientific inquiry is increasingly performed programmatically, SkyPortal also surfaces an extensive and well-documented application programming interface system. From back-end and front-end software to data science analysis tools and visualization frameworks, the SkyPortal design emphasizes the reuse and leveraging of best-in-class approaches, with a strong extensibility ethos. For instance, SkyPortal now leverages ChatGPT large language models to generate and surface source-level human-readable summaries automatically. With the imminent restart of the next generation of gravitational-wave detectors, SkyPortal now also includes dedicated multimessenger features addressing the requirements of rapid multimessenger follow-up: multitelescope management, team/group organizing interfaces, and crossmatching of multimessenger data streams with time-domain optical surveys, with interfaces sufficiently intuitive for newcomers to the field. This paper focuses on the detailed implementations, capabilities, and early science results that establish SkyPortal as a community software package ready to take on the data science challenges and opportunities presented by this next chapter in the multimessenger era. © 2023. The Author(s). Published by the American Astronomical Society.',\n",
       " '[No abstract available]',\n",
       " 'Data science has been the foundation of recommender systems for a long time. Over the past few decades, various recommender systems have been developed using different data science and machine learning methodologies and techniques. However, no existing work systematically discusses the significant relationships between data science and recommender systems. To bridge this gap, this paper aims to systematically investigate recommender systems from the perspective of data science. Firstly, we introduce the various types of data used for recommendations and the corresponding machine learning models and methods that effectively represent each type. Next, we provide a brief outline of the representative data science and machine learning models utilized in building recommender systems. Subsequently, we share some preliminary thoughts on next-generation recommender systems. Finally, we summarize this special issue on data science for next-generation recommender systems. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG.',\n",
       " 'Agent-based modeling (ABM) has been widely used in numerous disciplines and practice domains, subject to many eulogies and criticisms. This article presents key advances and challenges in agent-based modeling over the last two decades and shows that understanding agents’ behaviors is a major priority for various research fields. We demonstrate that artificial intelligence and data science will likely generate revolutionary impacts for science and technology towards understanding agent decisions and behaviors in complex systems. We propose an innovative approach that leverages reinforcement learning and convolutional neural networks to equip agents with the intelligence of self-learning their behavior rules directly from data. We call for further developments of ABM, especially modeling agent behaviors, in the light of data science and artificial intelligence. © 2023 Elsevier Ltd',\n",
       " '[No abstract available]',\n",
       " 'The data landscape in preclinical safety assessment is fundamentally changing because of not only emerging new data types, such as human systems biology, or real-world data (RWD) from clinical trials, but also technological advancements in data-processing software and analytical tools based on deep learning approaches. The recent developments of data science are illustrated with use cases for the three factors: predictive safety (new in silico tools), insight generation (new data for outstanding questions); and reverse translation (extrapolating from clinical experience to resolve preclinical questions). Further advances in this field can be expected if companies focus on overcoming identified challenges related to a lack of platforms and data silos and assuring appropriate training of data scientists within the preclinical safety teams. © 2023 Elsevier Ltd',\n",
       " 'The purpose of this article is to propose and provide a blueprint for a graduate-level curriculum in clinical data science, devoted to the measurement, acquisition, care, treatment, and inferencing of clinical research data. The curriculum presented here contains a series of five required core courses, five required research courses, and a list of potential electives. The coursework draws from but does not duplicate content from the foundational areas of biostatistics, clinical medicine, biomedical informatics, and regulatory affairs, and may be reproduced by any institution interested in and capable of offering such a program. This new curriculum in “clinical” data science will prepare students for work in academic, industry, and government research settings as well as offer a unifying knowledge base for the profession. © 2023 The Author. Clinical and Translational Science published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics.',\n",
       " \"The importance of involving humans in the data science process has been widely discussed in the literature. However, studies lack details on how to involve humans in the process. Using a design science approach, this paper proposes and evaluates a human-supervised data science framework in the context of local governments. Our findings suggest that the involvement of a stakeholder group, public managers in this case, in the process of data science project enhanced quality of data science outcomes. Public managers' detailed knowledge on both the data and context was beneficial for improving future data science infrastructure. In addition, the study suggests that local governments can harness the value of data-driven approaches to policy and decision making through focalized investments in improving data and data science infrastructure, which includes culture and processes necessary to incorporate data science and analytics into the decision-making process. © 2023 Association for Information Science and Technology.\",\n",
       " 'Health data science systems are becoming key for supporting healthcare decision-making processes. However, these systems should achieve continuous data processing and adapt their behavior to changes arising in real scenarios. Nevertheless, building this type of self-adaptable systems is not trivial, as it requires integrating data analytics and artificial intelligence with cloud computing tools and security and fault tolerance applications. This produces delays in the processing of data, affecting the information delivery in decision-making processes. In this paper, we present the design and implementation of a method to build self-adaptable data science services by using dynamic patterns. This method includes two models: a construction model and a coupling model. The construction model is based on dynamic parallel patterns to create, at design time, in-memory processing structures including as many applications as needed to meet the non-functional requirements (NFRs), such as security and fault tolerance, established by healthcare organizations without affecting the efficiency of a data science system. In turn, the coupling model converts the in-memory processing structures into software blocks that are coupled with the I/O interfaces of data science systems to support the automatic and transparent continuous management of data for facing changes in the incoming workload during execution time. A prototype was implemented to create self-adaptable health data science systems including in-memory processing structures for managing spirometry studies, tomography images, and electrocardiograms. The performance evaluation showed that the dynamic patterns significantly reduced the response time required to prepare contents with NFR characteristics in comparison with solutions from the state-of-the-art such as Nextflow and Makeflow. © 2023 Elsevier B.V.',\n",
       " 'This paper presents Xel, a cloud-agnostic data platform for the design-driven building of high-availability data science services as a support tool for data-driven decision-making. We designed and implemented Xel based on four main components: (a) a high level and driven-design framework for end-users to select analytic and machine learning tools from a service mesh and coupling them into the form of processing pipelines; (b) a new recursive ETL processing model to automatically convert the pipeline designs into infrastructure-agnostic software structures, which are deployed on multiple infrastructures; (c) an orchestration model for transparently managing the data delivery throughout each stage of the processing pipelines used in data science systems; and (d) a data decentralized model to transparently mask service unavailability such as cloud outages and unavailability of either applications or data. Real users created, by means of Xel, data science services such as deep learning analysis of scientific publications, clustering of movie reviews, and a cancer exploratory study. These services were evaluated as case studies that revealed the efficacy of this platform design for enabling end-users to create multiple types of data science pipelines without programming nor making configurations, and automatically masking unavailability of cloud resources and data. This platform is currently used to create a national cancer observatory and big data systems for fusing suicide, mental health, drug consumption and macroeconomic datasets to find spatiotemporal patterns. © 2023 Elsevier B.V.',\n",
       " '[No abstract available]',\n",
       " 'The emergence of Covid-19 pandemic in late 2019 presented daunting challenges for designing and implementing sustainable solutions at both local and global levels. The situation was dire in many developing economies with limited resources and vulnerable healthcare systems especially in Africa.\\xa0Spatial data science (SDS) can be adopted and utilized to assist countries and local communities in understanding and effectively responding to Covid-19 pandemic.\\xa0This article’s study reviewed recent literature with the main goal to assess the application of this data-driven and technology-oriented modern approach in addressing Covid-19 in the African continent.\\xa0Findings indicate that while examples of applications involving traditional geospatial technologies especially geographic information systems are abound, the use of more advanced SDS elements is limited and fragmented. Additionally, various studies leveraged SDS to address one or more complex questions against the backdrop of challenges largely influenced by the digital divide within Africa and across the globe. The article identifies and discusses these challenges as well as opportunities for increased use of SDS in Africa to understand and respond to disasters like Covid-19 and other complex problems. The argument is made for a more complete use of multiple elements of SDS. © 2023, The Author(s), under exclusive licence to Springer Nature B.V.',\n",
       " 'This article discusses the key elements of the Data Science Technology course offered to postgraduate students enrolled in the Master of Data Science program. This course complements the existing curriculum by providing the skills to handle the Big Data platform and tools, in addition to data science activities. We tackle the discussion about this course based on three main requirements, which are related to the need to exploit the key skills from two dimensions, namely, Data Science and Big Data, and the need for a cluster-based computing platform and its accessibility. We address these requirements by presenting the course design and its assessments, the configuration of the computing platform, and the strategy to enable flexible accessibility. In terms of course design, the offered course contributes to several innovative elements and has covered multiple key areas of the data science body of knowledge and multiple quadrants of the job and skills matrix. In the case of the computing platform, a stable deployment of a Hadoop cluster with flexible accessibility, triggered by the pandemic situation, has been established. Furthermore, through our experience with the implementation of the cluster, it has shown the ability of the cluster to handle computing problems with a larger dataset than the one used for the semesters within the scope of the study. We also provide some reflections and highlight future improvements. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.',\n",
       " 'In the fields of engineering and data sciences, the optimization problems arise on regular basis. With the progress in the field of scientific computing and research, the optimization is not a problem for small data sets and lower dimensional problems. The problem arise, when the data is large, stochastic in nature, and/or multidimensional. The basic optimization tools fail for such problems due to the complexity. The genetic algorithms, based on the natural selection hypothesis, play an imperative role to deal with such complex problems. Genetic algorithms are used in the literature to optimize numerous problems. In the field of computational biology, these algorithms have provided cost effective solutions to find optimal values for large data sets. The genetic algorithms have been used for image reconstruction. These algorithms are based on sub-algorithms to improve the accuracy and precision. We will discuss the advanced genetic algorithms and their applications in detail. Genetic algorithm, in hybrid form have attracted interest of researchers from almost all fields, including computer science, applied mathematics, engineering and computational biology. These tools help to analyze the systems in a swift manner. This important feature is discussed with the aid of examples. The time series forecasting and the Bayesian inference, in combination with the genetic algorithms, can prove to be powerful artificial intelligence tools. We will discuss this important aspect in detail with the aid of some examples. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.',\n",
       " 'Modern science depends on computers, but not all scientists have access to the scale of computation they need. A digital divide separates scientists who accelerate their science using large cyberinfrastructure from those who do not, or who do not have access to the compute resources or learning opportunities to develop the skills needed. The exclusionary nature of the digital divide threatens equity and the future of innovation by leaving people out of the scientific process while over-amplifying the voices of a small group who have resources. However, there are potential solutions: recent advancements in public research cyberinfrastructure and resources developed during the open science revolution are providing tools that can help bridge this divide. These tools can enable access to fast and powerful computation with modest internet connections and personal computers. Here we contribute another resource for narrowing the digital divide: scalable virtual machines running on public cloud infrastructure. We describe the tools, infrastructure, and methods that enabled successful deployment of a reproducible and scalable cyberinfrastructure architecture for a collaborative data synthesis working group in February 2023. This platform enabled 45 scientists with varying data and compute skills to leverage 40,000 hours of compute time over a 4-day workshop. Our approach provides an open framework that can be replicated for educational and collaborative data synthesis experiences in any data- and compute-intensive discipline. © 2023 Owner/Author.',\n",
       " 'Educational data scientists often conduct research with the hopes of translating findings into lasting change through policy, civil society, or other channels. However, the bridge from research to practice can be fraught with sociopolitical frictions that impede, or altogether block, such translations-especially when they are contentious or otherwise difficult to achieve. Focusing on one entrenched educational equity issue in US public schools-racial and ethnic segregation-we conduct randomized email outreach experiments and surveys to explore how local school districts respond to algorithmically-generated school catchment areas (\"attendance boundaries\") designed to foster more diverse and integrated schools. Cold email outreach to approximately 4,320 elected school board members across over 800 school districts informing them of potential boundary changes reveals a large average open rate of nearly 40%, but a relatively small click-through rate of 2.5% to an interactive dashboard depicting such changes. Board members, however, appear responsive to different messaging techniques - -particularly those that dovetail issues of racial and ethnic diversity with other top-of-mind issues (like school capacity planning). On the other hand, media coverage of the research drives more dashboard engagement, especially in more segregated districts. A small but rich set of survey responses from school board and community members across several districts identify data and operational bottlenecks to implementing boundary changes to foster more diverse schools, but also share affirmative comments on the potential viability of such changes. Together, our findings may support educational data scientists in more effectively disseminating research that aims to bridge educational inequalities through systems-level change. © 2023 ACM.',\n",
       " 'Conventional 2-D scanning electron microscopy (SEM) is commonly used to rapidly and qualitatively evaluate membrane pore structure. Quantitative 2-D analyses of pore sizes can be extracted from SEM, but without information about 3-D spatial arrangement and connectivity, which are crucial to the understanding of membrane pore structure. Meanwhile, experimental 3-D reconstruction via tomography is complex, expensive, and not easily accessible. Here, we employ data science tools to demonstrate a proof-of-principle reconstruction of the 3-D structure of a membrane using a single 2-D image pulled from a 3-D tomographic data set. The reconstructed and experimental 3-D structures were then directly compared, with important properties such as mean pore radius, mean throat radius, coordination number and tortuosity differing by less than 15%. The developed algorithm could dramatically improve the ability of the membrane community to characterize membranes, accelerating the design and synthesis of membranes with desired structural and transport properties. © 2023 Elsevier B.V.',\n",
       " 'In recent years, ransomware attacks have become a more significant source of computer penetration. Only general-purpose computing systems with sufficient resources have been harmed by ransomware so far. Numerous ransomware prediction strategies have been published, but more practical machine learning ransomware prediction techniques still need to be developed. In order to anticipate ransomware assaults, this study provides a method for obtaining data from artificial intelligence and machine learning systems. A more accurate model for outcome prediction is produced by using the data science methodology. Understanding the data and identifying the variables are essential elements of a successful model. A variety of machine learning algorithms are applied to the pre-processed data, and the accuracy of each technique is compared to determine which approach performed better. Additional performance indicators including recall, accuracy, and f1-score are also taken into account while evaluating the model. It uses machine learning to predict how the ransomware attack would pan out. © 2023 EDP Sciences. All rights reserved.',\n",
       " 'COVID-19 (2019-nCoV, more widely known as 2019 coronavirus disease), brought by SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2), is a rising public health issue compromising the life of over 2.4 million individuals comprehensively. It is a worldwide health crisis of exceptional extents. The WHO (World Health Organization) pronounced the corona-virus 2019 a pandemic on date March 11th, highlighting the more than 118,000 cases in more than 110 nations and regions around the globe. The continuous death toll comprehensively, underscoring the supported danger of further worldwide spread just as explained diversely inside individual countries through communities because of asymptomatic transporters, has produced a requirement for interdisciplinary logical research, reaction, and activity through clear, point-by-point informing to worldwide constituencies. Governments around the globe are forcing different control measures while the social insurance framework is preparing itself for tidal waves of contaminated people that will look for treatment. Since January 2020, a powerful group of research has been rising up out of medical disciplines, for example, virology, the study of disease transmission, immunology, microbiology, radiology, bioinformatics, and phylogeny. Tragically, there is next to zero insightful work rising up out of non-medical disciplines. This work focused on the gaps in the literature by concentrating on the data science deployment to battle against the novel SARS-CoV-2 pandemic. The introduced work looked to decide the utilization of information Science in social insurance frameworks to battle against COVID-19. © 2023 by Apple Academic Press, Inc. All rights reserved.',\n",
       " 'Purpose: Underlying much recent development in data science and artificial intelligence (AI) is a dependence on the labour of precarious crowdworkers via platforms such as Amazon Mechanical Turk. These platforms have been widely critiqued for their exploitative labour relations, and over recent years, there have been various efforts by academic researchers to develop interventions aimed at improving labour conditions. The aim of this paper is to explore US-based crowdworkers’ views on two proposed interventions: a browser plugin that detects automated quality control “Gold Question” (GQ) checks and a proposal for a crowdworker co-operative. Design/methodology/approach: The authors interviewed 20 US-based crowdworkers and undertook a thematic analysis of collected data. Findings: The findings indicate that US-based crowdworkers tend to have negative and mixed feelings about the GQ detector, but were more enthusiastic about the crowdworker co-operative. Originality/value: Drawing on theories of precarious labour, this study suggests an explanation for the findings based on US-based workers’ objective and subjective experiences of precarity. The authors argue that for US-based crowdworkers “constructive” interventions such as a crowdworker co-operative have more potential to improve labour conditions. © 2023, Emerald Publishing Limited.',\n",
       " '[No abstract available]',\n",
       " \"Data's increasing role in society and high profile reproduction of inequalities is in tension with traditional methods of using social data for social justice. Alongside this, ‘intersectionality’ has increased in prominence as a critical social theory and praxis to address inequalities. Yet, there is not a comprehensive review of how intersectionality is operationalized in research data practice. In this study, we examined how intersectionality researchers across a range of disciplines conduct intersectional analysis as a means of unpacking how intersectional praxis may advance an intersectional data science agenda. To explore how intersectionality researchers collect and analyze data, we conducted a critical discourse analysis approach in a review of 172 articles that stated using an intersectional approach in some way. We contemplated whether and how Collins’ three frames of relationality were evident in their approach. We found an over-reliance on the additive thinking frame in quantitative research, which poses limits on the potential for this research to address structural inequality. We suggest ways in which intersectional data science could adopt an articulation mindset to improve on this tendency. © The Author(s) 2023.\",\n",
       " 'Failed to fetch page (status 404)',\n",
       " 'Fixed underwater observatories (FUO), equipped with digital cameras and other sensors, become more commonly used to record different kinds of time series data for marine habitat monitoring. With increasing numbers of campaigns, numbers of sensors and campaign time, the volume and heterogeneity of the data, ranging from simple temperature time series to series of HD images or video call for new data science approaches to analyze the data. While some works have been published on the analysis of data from one campaign, we address the problem of analyzing time series data from two consecutive monitoring campaigns (starting late 2017 and late 2018) in the same habitat. While the data from campaigns in two separate years provide an interesting basis for marine biology research, it also presents new data science challenges, like the the marine image analysis in data form more than one campaign. In this paper, we analyze the polyp activity of two Paragorgia arborea cold water coral (CWC) colonies using FUO data collected from November 2017 to June 2018 and from December 2018 to April 2019. We successfully apply convolutional neural networks (CNN) for the segmentation and classification of the coral and the polyp activities. The result polyp activity data alone showed interesting temporal patterns with differences and similarities between the two time periods. A one month “sleeping” period in spring with almost no activity was observed in both coral colonies, but with a shift of approximately one month. A time series prediction experiment allowed us to predict the polyp activity from the non-image sensor data using recurrent neural networks (RNN). The results pave a way to a new multi-sensor monitoring strategy for Paragorgia arborea behaviour. © 2023 van Kevelaer et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.',\n",
       " 'Data science, bioinformatics, and machine learning are the advent and progression of the fourth paradigm of exploratory science. The need for human-supported algorithms to capture patterns in big data is at the center of personalized healthcare and directly related to translational research. This paper argues that hypothesis-driven and data-driven research work together to inform the research process. At the core of these approaches are theoretical underpinnings that drive progress in the field. Here, we present several exemplars of research on the gut-brain axis that outline the innate values and challenges of these approaches. As nurses are trained to integrate multiple body systems to inform holistic human health promotion and disease prevention, nurses and nurse scientists serve an important role as mediators between this advancing technology and the patients. At the center of person-knowing, nurses need to be aware of the data revolution and use their unique skills to supplement the data science cycle from data to knowledge to insight. © 2022 Wolters Kluwer Health, Inc. All rights reserved.',\n",
       " 'Objectives: As the real-world electronic health record (EHR) data continue to grow exponentially, novel methodologies involving artificial intelligence (AI) are becoming increasingly applied to enable efficient data-driven learning and, ultimately, to advance healthcare. Our objective is to provide readers with an understanding of evolving computational methods and help in deciding on methods to pursue. Target Audience: The sheer diversity of existing methods presents a challenge for health scientists who are beginning to apply computational methods to their research. Therefore, this tutorial is aimed at scientists working with EHR data who are early entrants into the field of applying AI methodologies. Scope: This manuscript describes the diverse and growing AI research approaches in healthcare data science and categorizes them into 2 distinct paradigms, the bottom-up and top-down paradigms to provide health scientists venturing into artificial intelligent research with an understanding of the evolving computational methods and help in deciding on methods to pursue through the lens of real-world healthcare data. © The Author(s) 2023. Published by Oxford University Press on behalf of the American Medical Informatics Association.',\n",
       " 'In this work, we develop novel data science methodologies for ensemble performance data that have the potential to uncover orders of magnitude of performance that is unknowingly being left on the table. Building on years of successful performance tool design and tool integration into million-line codes at Lawrence Livermore National Laboratory (Caliper (Boehme et al. 2016), Hatchet (Bhatele et al. 2019; Brink et al. 2020))—successes highlighted as key deliverables in meeting LLNL’s L1 and L2 milestones (Rieben and Weiss 2020)—we design a data science methodology for integrating multi-dimensional, multi-scale, multi-architecture, and multi-tool performance data, and provide data analytics and interactive visualization capabilities for further analysis and exploration of the data. Our work provides developers with a comprehensive multi-dimensional performance landscape, enabling enhanced capabilities for pinpointing performance bottlenecks on emerging hardware platforms composed of heterogeneous elements. © The Author(s) 2023.',\n",
       " \"Preparing data-literate citizens and supporting future generations to effectively work with data is challenging. Engaging students in Knowledge Building (KB) may be a promising way to respond to this challenge because it requires students to reflect on and direct their inquiry with the support of data. Informed by previous studies, this research explored how an analytics-supported reflective assessment (AsRA)-enhanced KB design influenced 6th graders' KB and data science practices in a science education setting. One intact class with 56 students participated in this study. The analysis of students' Knowledge Forum discourse showed the positive influences of the AsRA-enhanced KB design on students' development of KB and data science practices. Further analysis of different-performing groups revealed that the AsRA-enhanced KB design was accessible to all performing groups. These findings have important implications for teachers and researchers who aim to develop students' KB and data science practices, and general high-level collaborative inquiry skills. Practitioner notes What is already known about this topic Data use becomes increasingly important in the K-12 educational context. Little is known about how to scaffold students to develop data science practices. Knowledge Building (KB) and learning analytics-supported reflective assessment (AsRA) show premises in developing these practices. What this paper adds AsRA-enhanced KB can help students improve KB and data science practices over time. AsRA-enhanced KB design benefits students of different-performing groups. AsRA-enhanced KB is accessible to elementary school students in science education. Implications for practice and/or policy Developing a collaborative and reflective culture helps students engage in collaborative inquiry. Pedagogical approaches and analytic tools can be developed to support students' data-driven decision-making in inquiry learning. © 2023 British Educational Research Association.\",\n",
       " 'Digital journalism has faced a dramatic change and media companies are challenged to use data science algorithms to be more competitive in a Big Data era. While this is a relatively new area of study in the media landscape, the use of machine learning and artificial intelligence has increased substantially over the last few years. In particular, the adoption of data science models for personalization and recommendation has attracted the attention of several media publishers. Following this trend, this paper presents a research literature analysis on the role of Data Science (DS) in Digital Journalism (DJ). Specifically, the aim is to present a critical literature review, synthetizing the main application areas of DS in DJ, highlighting research gaps, challenges, and opportunities for future studies. Through a systematic literature review integrating bibliometric search, text mining, and qualitative discussion, the relevant literature was identified and extensively analyzed. The review reveals an increasing use of DS methods in DJ, with almost 47% of the research being published in the last three years. An hierarchical clustering highlighted six main research domains focused on text mining, event extraction, online comment analysis, recommendation systems, automated journalism, and exploratory data analysis along with some machine learning approaches. Future research directions comprise developing models to improve personalization and engagement features, exploring recommendation algorithms, testing new automated journalism solutions, and improving paywall mechanisms. © 2023 The Author(s)',\n",
       " 'Mass adoption of advanced information technologies is fueling a need for public servants with the skills to manage data-driven public agencies. Public employees typically acquire data skills through graduate research methods courses, which focus primarily on research design and statistical analysis. What data skills are currently taught, and what content should Master of Public Administration (MPA) programs include in their research method courses? We categorized research method course content in 52 syllabi from 31 MPA programs to understand how data skills are taught in public administration. We find that most graduate programs rely on research methods more suited for academic and policy research while lacking the data skills needed to modernize public agencies. Informed by these results, this work presents the Data Science Literacy Framework as a guide for assessing and planning curriculum within MPA programs. © The Author(s) 2022.',\n",
       " 'Rather than presenting Python as Java or C, this textbook focuses on the essential Python programming skills for data scientists and advanced methods for big data analysts. Unlike conventional textbooks, it is based on Markdown and uses full-color printing and a code-centric approach to highlight the 3C principles in data science: creative design of data solutions, curiosity about the data lifecycle, and critical thinking regarding data insights. Q&A-based knowledge maps, tips and suggestions, notes, as well as warnings and cautions are employed to explain the key points, difficulties, and common mistakes in Python programming for data science. In addition, it includes suggestions for further reading. This textbook provides an open-source community via GitHub, and the course materials are licensed for free use under the following license: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). © Publishing House of Electronics Industry, Beijing, China 2023. All rights reserved.',\n",
       " 'The COVID-19 pandemic caused the flaws of traditional grading systems to become even more apparent. In response, a growing number of educators are transitioning their classrooms to focus on alternative methods of assessment. These subversive methods promote more equitable assessments, as they provide a more accurate picture of what a student has learned, cultivate students\\' intrinsic motivation, and do not privilege students from certain backgrounds. This article details how alternative grading, specifically \"ungrading,\"was integrated into an introductory data science course. I detail how the course components align with the principles of alternative grading, students\\' responses to the course structure, and the lessons I learned along the way. Finally, I close with a discussion of how infusing alternative methods of assessment into the classroom stands to cultivate the diversity continually lacking in computer science and data science. © 2023 ACM.',\n",
       " \"Data Science (DS) is an interdisciplinary topic that is applicable to many domains. In this preliminary investigation, we use caselet, a mini-version of a case study, as a learning tool to allow learners to practice data science problem solving (DSPS). Using a dataset collected from a real-world classroom, we performed correlation analysis to reveal the structure of metacognitive and cognitive processes. We also explored the similarity of different DS knowledge components based on students' performance. In addition, we built predictive models to characterize the relationship between metacognition, cognition and learning gain. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\",\n",
       " 'Data science has gained the attention of various industries, educators, parents, and students thinking about their future careers. Statistics departments have traditionally offered data science courses for a long time. The main objective of these courses is to examine the fundamental concepts and theories. However, teaching data science courses has also expanded to other disciplines due to the vast amount of data being collected by numerous modern applications. Also, someone needs to learn how to collect and process data, especially from industrial devices, because of the recent development of Internet of Things (IoT) technologies. Hence, integrating data science into the curricula of different engineering branches becomes a matter of relating the statistics background to the specific discipline. There are several reasons for this transition. Firstly, as the increased computational power and massive availability of the data make the use of statistical theories possible in more engineering applications, there is a growing need for engineering students to build knowledge in data science concepts. Secondly, the wide availability of libraries and models allows for the implementation of diverse solutions to engineering problems. This paper will discuss introducing a new data science curriculum in an Engineering Technology (ET) program with a focus on Electrical Engineering Technology (EET) program. © American Society for Engineering Education, 2023.',\n",
       " \"In the field of data science, advancements in the field of machine learning have led to programs developing high-level reasoning, intricate data understanding, and groundbreaking predictive models. Machine Learning (ML) research aims at making a program 'learn,' that is, develop models and techniques with known information to be able to handle future problems. Traditionally, this is done by increasing the quantity and quality of input data and training the learner in more effective ways to interpret that information. This has a direct parallel to the collegiate classroom, as instructors aim to inspire mastery over a topic to their students through a variety of methods (homework problems, examinations, projects, etc.) and teach them the corresponding skillsets from feedback on these assignments. Machine Teaching (MT) research, on the other hand, aims at making the teacher more productive by using their own cognitive models to improve the quality of the data holistically. Again, this has a corresponding counterpart to current teaching pedagogies; the instructor decides on the details of an assignment from their own knowledge and experience with the end goal of having students retain the information and apply it to future problems. This paper identifies how the various innovations, lessons, and conclusions discovered in the field of artificial intelligence can enhance the quality of a collegiate classroom experience and improve student performance. © American Society for Engineering Education, 2023.\",\n",
       " 'Recent advances in data science algorithms and libraries have impacted approaches and strategies for research and development in the industrial sector, which necessitates the integration of data science into engineering education. Data science courses offered by programs such as mathematics, computer science, and data science in academic institutions normally lack the implementation in solving engineering problems. To fill this gap, we have developed a project-based technical elective course “Machine Learning for Mechanical Engineers” and offered it to undergraduate and graduate students at the University of Arkansas. While this course is received very well by the students and has led to fruitful presentations and publications, it has a low enrollment volume from undergraduate students due to its relatively high programming requirements. A more sophisticated strategy is required to equip mechanical engineering students with data science skills without disturbing the existing curriculum. Inspired by the success of computer-aided design education at the University of Arkansas and the Data Science InFusion into Undergraduate STEM Education (DIFUSE) program at Dartmouth College, we have developed course-specific machine learning modules to be integrated into mechanical engineering core courses rather than dedicated data science courses. This effort includes a nonparametric regression module for Computer Methods in Mechanical Engineering, a generative design module for Computer-Aided Design, and a genetic algorithms module for Thermal Systems Analysis and Design, among others. Through this practice, students will practice programming and machine learning skills every semester from their sophomore year and will be ready for the project-based technical elective machine learning course. © American Society for Engineering Education, 2023.',\n",
       " \"It is important to provide non-computing majors with hands-on experience when teaching them data science topics. Meanwhile, this is challenging since those students typically have limited, or no, computing background. This paper describes our experience in offering two types of hands-on assignments in an entry-level data science course for non-computing majors; one with coding tasks and the other, without. Data sets from various domains were used to diversify the types and requirements of those tasks. We evaluated the two types of hands-on assignments and compared how effectively they helped students understand data science topics and improve students' interest in data science and computer science. © American Society for Engineering Education, 2023.\",\n",
       " \"Data science is an emerging field that encompasses several STEM domains and offers exciting career prospects in a wide range of engineering applications. Given the rapid adoption of data analytics in engineering and the societal importance of many of its applications, there is an urgent need to train the next generation of civil engineers in data science. Several studies have emphasized that the use of data science in the present civil engineering undergraduate curricula is mostly restricted to simple introductory subjects, usually with the use of Microsoft Excel. The curriculum does not encourage students to fully benefit from big data since data science is not extensively used. Additionally, there is a lack of clear guidance on how to strengthen the synergy between common civil engineering courses and data science. In this paper, the authors report on a recently introduced course entitled “Data Science and Engineering Systems Analysis,” at Florida Gulf Coast University (FGCU), incorporating the integration of R programming into data analytics, and systems thinking for the analysis and design of civil engineering applications. The course aims at empowering students with the necessary tools to apply statistics in a civil engineering context, and perform data transformation, data wrangling, visualization, and modeling using R for data science. Students learn how to gather and analyze data as part of the engineering design process, apply systems thinking to an engineering or societal phenomenon, collaborate with peers to find solutions, and effectively present solutions to an audience. Through a careful examination of the course's key features, its applications, and its synergy with the existing curriculum, this paper provides guidance for data science curriculum development, implementation, and evaluation in civil engineering. © American Society for Engineering Education, 2023.\",\n",
       " '[No abstract available]',\n",
       " 'Although evidence suggests that children as young as four years old can develop coding and engineering projects based on data science concepts, data science is often overlooked in early childhood research, and limited resources existed slow its inclusion into this field of study. This paper proposes the Dataying framework to teach data science concepts to young children ages 4-7 years old. The framework development included identifying K-12 data science elements and then validating element suitability for young students. Six cycled steps were identified: identifying a problem, questioning, imagining and planning, collecting, analyzing, and story sharing. This paper also presents examples of data decision problems and demonstrates use of a proposed Insight-Detective method with a plan worksheet for Dataying. © American Society for Engineering Education, 2023.',\n",
       " 'The interdisciplinary nature of materials science and engineering (MSE) asks undergraduate majors in MSE to develop materials science domain knowledge in conjunction with complementary skills such as data science (DS) and scientific writing (SW). With little room to pack additional courses into MSE curricula, better integration of these transferable skills into existing courses will help train our students to succeed in the modern workforce. This Work in Progress details the development of a series of programming-based modules to complement the data analysis in a materials characterization laboratory course. We use the Jupyter Book software to design a scaffolded series of Python-based exercises that focus primarily on data visualization, with additional exercises on tabular data analysis, curve fitting, and image processing. Pre- and post-course surveys suggest that these modules had a positive impact on student learning and that students recognize the importance of these skills in MSE. © American Society for Engineering Education, 2023.',\n",
       " \"This paper describes the roll-out, continuous improvement, and completion of a multi-college interdisciplinary undergraduate program in data science that culminates in a Bachelor of Science degree with its first graduates. The success of graduate-level data science programs across the nation has influenced the creation of undergraduate degrees in data science. There has been tremendous interest by employers in having a workforce that has the skills and techniques, theoretical and practical experience at the undergraduate level to meet their increasing growth and dependency on data to be successful and competitive. This paper presents the results and experiences and continuous improvement of the 36 months of a newly created interdisciplinary program that uniquely integrates business, arts and sciences, and engineering disciplines while meeting the needs of industry stakeholders. The iterative development process, the curriculum, the launch of the program, the first-year data science program, and added years' courses completing the degree, the full-year Practicum and first graduates are described as well as new concentrations developed along the way. Lessons learned are summarized to aid other universities contemplating similar programs. Finally, we discuss student internship readiness, study abroad opportunities, student post-graduation employment offers, industry partner feedback, and the future. © American Society for Engineering Education, 2023.\",\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'This contribution is related to issues on decisions in conformity assessment, especially in testing, inspection, and certification (TIC) predominantly based on measurement data. The digital transformation has started to impact the TIC sector, which is related to intensified utilization of data science in TIC. The quality of the data in big data analytics, is not always sufficiently addressed, especially in sectors with traditionally empirical approaches, such as TIC. This paper conducts a survey of options for deployment of data science in the TIC decision making processes, based on conclusions with complementary usage of empirical \"measurements\" and the \"data science\". The focus of the discussion is centered over a case study where data fusion is applied for determining the TIC instrument calibration frequency, presenting a model to establish recalibration interval with reduced risk in the final decision delivery over the next re-calibration moment. © 2023 International Measurement Confederation (IMEKO). All rights reserved.',\n",
       " 'AU Much: Pleaseconfirmthatallheadinglevelsarerepresentedcorrectly guidance on statistical training in STEM fields has been : focused largely on the undergraduate cohort, with graduate education often being absent from the equation. Training in quantitative methods and reasoning is critical for graduate students in biomedical and science programs to foster reproducible and responsible research practices. We argue that graduate student education should more center around fundamental reasoning and integration skills rather than mainly on listing 1 statistical test method after the other without conveying the bigger context picture or critical argumentation skills that will enable student toPleasechecktheeditsmadetothetitleandprovidecorrectwordingifnecessaryimprove research integrity through rigorous practice.:Herein, we describe the approach we take in a quantitative reasoning course in the R3 program at the Johns Hopkins Bloomberg School of Public Health, with an error-focused lens, based on visualization and communication competencies. Specifically, we take this perspective stemming from the discussed causes of irreproducibility and apply it specifically to the many aspects of good statistical practice in science, ranging from experimental design to data collection and analysis, and conclusions drawn from the data. We also provide tips and guidelines for the implementation and adaptation of our course material to various graduate biomedical and STEM science programs. © 2023 Ciubotariu, Bosch. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'This work applies machine learning to holistically interrogate the influence of metallurgical factors, such as chemical composition, heat treatment, and mechanical properties, on the stress corrosion cracking resistance of corrosion-resistant alloys. Particularly, we explored the effect of nickel in reducing the stress corrosion cracking susceptibility in boiling magnesium chloride, arguably a controversial topic since Copson’s 1959 seminal publication. This paper offers insights into the synergies of nickel with other alloying elements that ultimately impact the resistance to stress corrosion cracking. Furthermore, a more detailed description of statistical patterns in the so-called Copson curve is provided. © 2023 The Author(s). Published on behalf of The Electrochemical Society by IOP Publishing Limited.',\n",
       " 'The Fourth Industrial Revolution, also named Industry 4.0, is leveraging several modern computing fields. Industry 4.0 comprises automated tasks in manufacturing facilities, which generate massive quantities of data through sensors. These data contribute to the interpretation of industrial operations in favor of managerial and technical decision-making. Data science supports this interpretation due to extensive technological artifacts, particularly data processing methods and software tools. In this regard, the present article proposes a systematic literature review of these methods and tools employed in distinct industrial segments, considering an investigation of different time series levels and data quality. The systematic methodology initially approached the filtering of 10,456 articles from five academic databases, 103 being selected for the corpus. Thereby, the study answered three general, two focused, and two statistical research questions to shape the findings. As a result, this research found 16 industrial segments, 168 data science methods, and 95 software tools explored by studies from the literature. Furthermore, the research highlighted the employment of diverse neural network subvariations and missing details in the data composition. Finally, this article organized these results in a taxonomic approach to synthesize a state-of-the-art representation and visualization, favoring future research studies in the field. © 2023 by the authors.',\n",
       " 'Identifying the proxy data and techniques that could yield greater PM2.5estimation accuracy remains challenging due to the lack of general knowledge on the relative performance of covariates and modeling methods on the PM2.5modeling accuracy. To bridge a critical gap in the contemporary literature on how to improve satellite-based PM2.5estimation models, we have systematically reviewed global efforts at PM2.5estimation between 2000 and 2020 using big data analytics, focusing on a critical evaluation of PM2.5modeling accuracy improvements in response to varying methods and covariates, mainly from remote sensing and data science perspectives. Using an automated literature classification deep learning method, we identified 833 publications with fair inter-comparisons among relevant PM2.5estimation models. The inter-compared modeling accuracy metrics were then retrieved and aggregated with respect to each modeling factor to form a global research synthesis database. The synthesized results highlighted that greater PM2.5modeling accuracy (i.e., a mean R2improvement by 6.2%) can be attained by incorporating aerosol optical depth (AOD); this benefit could even be doubled by applying fine-mode AOD. Further enhancements could be achieved by conducting vertical and humidity corrections to AOD (10.41%), as well as including relevant meteorological and socioeconomic factors that are highly associated with PM2.5variations (12%, on average). By contrast, improving spatial resolution of satellite AOD products and temporal resolution of ground PM2.5measurements are unlikely to yield better PM2.5estimation accuracy (improvement of 1.2%). Although advanced machine learning algorithms with better generalization capacity is more cost-effective, the accuracy improvements vary substantially across studies, largely depending on data sources, study regions, and even seasons. The statistical findings underscore the critical importance of domain knowledge and advancements in remote sensing and data science for improving PM2.5modeling accuracy. Finally, with inferred knowledge from this quantitative evidence, we discuss the grand challenges and possible solutions for future research directions toward better PM2.5prediction, mapping, and exposure assessment. © 2023 Elsevier B.V.',\n",
       " 'The vast amount of astronomical information that has become available over this decade has far exceeded that of the last century. The heterogeneity of the data and its overwhelming magnitude have made it impossible to perform manual analysis. As a consequence, new techniques have been developed and different strategies have been amalgamated, such as data science and data mining, in order to carry out more in-depth and exhaustive analyses in search of the extraction of the knowledge contained in data. This paper introduces a data science methodology that consists of successive stages, with the core of this proposal being the step of data preprocessing, with the aim of reducing the complexity of the analysis and enabling hidden knowledge in the data to be uncovered. The proposed methodology was tested on a set of data consisting of artificial light curves that try to mimic the behaviour of the strong gravitational lens phenomenon, as supplied by the Time Delay Challenge 1 (TDC1). Under the data science methodology, diverse statistical methods were implemented for data analysis, and cross-correlation and dispersion methods were applied for the time-delay estimation of strong lensing systems. With this methodology, we obtained time-delay estimations from the TDC1 data set and compared them with earlier results reported by the COSmological MOnitoring of GRAvItational Lenses project (COSMOGRAIL). The empirical evidence leads us to conclude that, with the proposed methodology, we achieve a greater accuracy in estimating time delays compared with estimations made with raw data. © 2023 The Author(s) Published by Oxford University Press on behalf of Royal Astronomical Society.',\n",
       " '[No abstract available]',\n",
       " 'This study proposes and evaluates the OCEL.AI (Open Collaborative Experiential Learning. AI) paradigm that aims at broadening participation in data science education and enhancing undergraduate students’ data literacy. The core of the paradigm is the “Tell Stories” approach. This approach applies the 5W+1H (Who, What, When, Where, Why, and How) conceptual schema of stories as a transdisciplinary language for data science education for STEM and non-STEM majors. Accordingly, this study reported findings from the OCEL.AI project that implemented and evaluated the paradigm. A field experiment, in addition to classroom observations, was conducted to compare the learning outcomes of students in data science competence, appreciation, career motivation, life-long willingness to learn, and self-efficacy in data science between the treatment group and the control group. The results showed that the OCEL.AI paradigm improved undergraduates’ data science competence and career motivation despite majors or gender. © 2023',\n",
       " 'Objectives: The authors’ objective is to present an overarching framework of an analytic ecosystem using diverse data domains and data science approaches that can be used and implemented across the cancer continuum. Analytic ecosystems can improve quality practices and offer enhanced anticipatory guidance in the era of precision oncology nursing. Data Sources: Published scientific articles supporting the development of a novel framework with a case exemplar to provide applied examples of current barriers in data integration and use. Conclusion: The combination of diverse data sets and data science analytic approaches has the potential to extend precision oncology nursing research and practice. Integration of this framework can be implemented within a learning health system where models can update as new data become available across the continuum of the cancer care trajectory. To date, data science approaches have been underused in extending personalized toxicity assessments, precision supportive care, and enhancing end-of-life care practices. Implications for Nursing Practice: Nurses and nurse scientists have a unique role in the convergence of data science applications to support precision oncology across the trajectory of illness. Nurses also have specific expertise in supportive care needs that have been dramatically underrepresented in existing data science approaches thus far. They also have a role in centering the patient and family perspectives and needs as these frameworks and analytic capabilities evolve. © 2023 Elsevier Inc.',\n",
       " '[No abstract available]',\n",
       " 'Pressing environmental research questions demand the integration of increasingly diverse and large-scale ecological datasets as well as complex analytical methods, which require specialized tools and resources. Computational training for ecological and evolutionary sciences has become more abundant and accessible over the past decade, but tool development has outpaced the availability of specialized training. Most training for scripted analyses focuses on individual analysis steps in one script rather than creating a scripted pipeline, where modular functions comprise an ecosystem of interdependent steps. Although current computational training creates an excellent starting place, linear styles of scripting can risk becoming labor- and time-intensive and less reproducible by often requiring manual execution. Pipelines, however, can be easily automated or tracked by software to increase efficiency and reduce potential errors. Ecology and evolution would benefit from techniques that reduce these risks by managing analytical pipelines in a modular, readily parallelizable format with clear documentation of dependencies. Workflow management software (WMS) can aid in the reproducibility, intelligibility and computational efficiency of complex pipelines. To date, WMS adoption in ecology and evolutionary research has been slow. We discuss the benefits and challenges of implementing WMS and illustrate its use through a case study with the targets r package to further highlight WMS benefits through workflow automation, dependency tracking and improved clarity for reviewers. Although WMS requires familiarity with function-oriented programming and careful planning for more advanced applications and pipeline sharing, investment in training will enable access to the benefits of WMS and impart transferable computing skills that can facilitate ecological and evolutionary data science at large scales. © 2023 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society.',\n",
       " '[No abstract available]',\n",
       " 'ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications. © 2023 by the authors.',\n",
       " '[No abstract available]',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'A main factor motivating consumer choice is the packaging: in many cases, the consumer choices are prevalently based on it. Actually, in planning the packaging of a new product on the market, due to the many variables that can influence the result, it is necessary to conduct a high number of preliminary analyses. It is therefore desirable to develop an automated method that allows obtaining information and reduces the analysis time and cost. In this work, we propose the use of data science/machine learning methods to verify, but also to predict, the effectiveness of the packaging in the Food&Beverage sector. As proof of concept, after doing a public survey about some Food&Beverage packaging, we value the ability of a feedforward Multi-Layer Perceptron (MLP) Artificial Neural Network, in predicting the results, i.e. if and how much the consumer likes the packaging. Trained MLP shows a very high correlations coefficient (> 0.98) and low mean square error (7.97) and error percentage (5.76%) values in determining the consumer response. © 2022, The Author(s), under exclusive licence to Springer Nature B.V.',\n",
       " \"Data Science (DS) has emerged from the shadows of its parents—statistics and computer science—into an independent field since its origin nearly six decades ago. Its evolution and education have taken many sharp turns. We present an impressionistic study of the evolution of DS anchored to Kuhn's four stages of paradigm shifts. First, we construct the landscape of DS based on curriculum analysis of the 32 iSchools across the world offering graduate-level DS programs. Second, we paint the “field” as it emerges from the word frequency patterns, ranking, and clustering of course titles based on text mining. Third, we map the curriculum to the landscape of DS and project the same onto the Edison Data Science Framework (2017) and ACM Data Science Knowledge Areas (2021). Our study shows that the DS programs of iSchools align well with the field and correspond to the Knowledge Areas and skillsets. iSchool's DS curriculums exhibit a bias toward “data visualization” along with machine learning, data mining, natural language processing, and artificial intelligence; go light on statistics; slanted toward ontologies and health informatics; and surprisingly minimal thrust toward eScience/research data management, which we believe would add a distinctive iSchool flavor to the DS. © 2022 Association for Information Science and Technology.\",\n",
       " \"This paper focuses on data science in digital cultural heritage in Brazil, where there is a lack of systematized information and curated databases for the integrated organization of documentary knowledge. Thus, the aim was to systematically map the different forms of information organization and representation applied to objects from collections belonging to institutions affiliated with the federal government's Special Department of Culture. This diagnosis is then used to discuss the requirements of devising strategies that favor a better data science information infrastructure to reuse information on Brazil's cultural heritage. Content analysis was used to identify analytical categories and obtain a broader understanding of the documentary sources of these institutions in order to extract, analyze, and interpret the data involved. A total of 215 hyperlinks that can be considered cultural collections of the institutions studied were identified, representing 2,537,921 cultural heritage items. The results show that the online publication of Brazil's digital cultural heritage is limited in terms of technology, copyright licensing, and established information organization practices. This paper provides a conceptual and analytical view to discuss the requirements for formulating strategies aimed at building a data science information infrastructure of Brazilian digital cultural collections that serves as future projects. © 2022 Association for Information Science and Technology.\",\n",
       " 'The information schools, also referred to as iField schools, are leaders in data science education. This study aims to develop a data science graduate curriculum model from an information science perspective to support iField schools in developing data science graduate education. In June 2020, information about 96 data science graduate programs from iField schools worldwide was collected and analyzed using a mixed research method based on inductive content analysis. A wide range of data science competencies and skills development and 12 knowledge topics covered by the curriculum were obtained. The humanistic model is further taken as the theoretical and methodological basis for course model construction, and 12 course knowledge topics are reconstructed into 4 course modules, including (a) data-driven methods and techniques; (b) domain knowledge; (c) legal, moral, and ethical aspects of data; and (d) shaping and developing personal traits, and human-centered data science graduate curriculum model is formed. At the end of the study, the wide application prospect of this model is discussed. © 2022 Association for Information Science and Technology.',\n",
       " 'Context: Wireline Formation Testers are common operations in well testing and evaluation, as they provide key information for exploration and production activities, such as reservoir pressure and permeability. The operation is conducted by following a line graph, where the X and Y axes are time and pressure, respectively. Problem: The decision on whether to end or not the operation is based on this graph. Unfortunately, there is no consensus on the ideal method to conduct these operations, and that is precisely the objective of this work. Solution: Using mathematical modeling, optimization and data science techniques, this work aims to increase the efficiency of wireline formation testing, defining objective metrics for operations conductions, in order to reduce time and guarantee data quality.IS Theory: This work is associated with the Theory of Computational Learning, which aims to understand the fundamental principles of learning and design better-automated methods. Method: The research has a prescriptive character, following machine learning model building best-practices and using a quantitative approach in analyzing results. Summary of Results: The results obtained show that there is a clear potential for reducing operating time and, therefore, costs, if the proposed methodology is used in routine operations. Contributions and Impact in the IS area: This article shows that, through mathematical modeling and the application of data science techniques, it is possible to significantly reduce the time of Wireline Formation Test operations, without any relevant loss of information, which can be a significant gain for oil and gas companies. © 2023 Copyright held by the owner/author(s).',\n",
       " 'The application of data science resources can enhance our ability to reduce health disparities affecting communities across the country. In this paper, we use the Preferred Reporting Items for Structured Review and Meta-Analyses methodology to conduct a thematic analysis of selected studies and identify micro and macro themes within the literature. The authors of this analysis reviewed articles that included data science software applications and their role in focusing efforts to address health disparities affecting the health of citizens in specific communities across the country. Abstracts were reviewed for research relevance after duplicates were removed, and documents were eliminated from consideration during the screening based on the exclusion criteria. At the conclusion of the structured literature review, the results emphasized the strong utility of geographically directed assessment. This highlights the need for the integration of geo- and statistical methodologies in the use of data science software for healthcare applications, which can be a useful means to identify locations where health disparities are present and can focus efforts to resolve these health disparities. Nurses, physicians, and health educators can be dispatched to the geographical areas where health disparities are most prevalent. Copyright © 2022 Wolters Kluwer Health, Inc. All rights reserved.',\n",
       " \"Employee turnover has turn out to be a large venture for data technological know-how companies. The departure of key software program builders would possibly reason large loss an IT business enterprise in view that they additionally leave with essential commercial enterprise understanding and integral technical skills. It is fundamental for IT companies to apprehend developer turnover in order to keep certified builders and reduce injury due to developer exit. In this research, monthly self-report of the software developers includes developer's activities, working hours, no of projects they have been assigned etc. will been taken into account for analysis for doing the prediction with the help of data science algorithm.By the usage of NB algorithm, KNN algorithm and SVM algorithm, prediction mannequin hasbeen in contrast on the experimental groundwork and supply the end result of which algorithmis performing better. Then, this fantastic mannequin will be given to HR managers to predict whether or not the worker will depart the corporation or not. © 2023 Little Lion Scientific.\",\n",
       " 'Big data and data scientific applications in the modern agriculture are rapidly evolving as the data technology advances and more computational power becomes available. The adoption of big data has enabled farmers and producers to optimize their agricultural activities sustainably with cutting-edge technologies, resulting in eco-friendly and efficient farming. Wireless sensor networks and machine learning have had a direct impact on smart and precision agriculture, with deep learning techniques applied to data collected via sensor nodes. Additionally, internet of things, drones, and robotics are being incorporated into farming techniques. Digital data handling has amplified the information wave, and information and communication technology have been used to deliver benefits to both farmers and consumers. This work highlights the technological implications and challenges that arise in data-driven agricultural practices as well as the research problems that need to be solved. © 2023 Owner/Author.',\n",
       " 'As data science and instrumentation become key practices in common careers ranging from medicine to agriscience, chemistry as a core introductory course must introduce such topics to students early and at an accessible level. Advanced data acquisition and data science generally require expensive precision instrumentation and massive computation, often out-of-reach even for upper-level undergraduate laboratory courses. At the same time, a new generation of affordable do-it-yourself instruments presents an opportunity for incorporation of curricula focused on instrument design and computation into freshman-level courses. We present a new lab for integration into existing courses that starts with hands-on spectrometer building, moves to data collection, and finally introduces an advanced data science technique, singular value decomposition, at an appropriate level with minimal computing requirements. The hardware and software used are modular and inexpensive. The lab was tested in three community college general chemistry sections over two semesters. Previously, students taking these courses did not typically see advanced quantitative chemistry curricula before deciding whether to pursue a bachelor’s degree. This lab allowed students to practice data collection and organization skills, use prewritten Jupyter notebooks that perform advanced data analysis, and gain presentation skills. A multiwave assessment completed by students highlights both successes and difficulties associated with incorporating multiple advanced topics involving instrument design, data collection, and analysis techniques in a single lab. © 2023 American Chemical Society and Division of Chemical Education, Inc.',\n",
       " 'The fast development of wireless location acquisition technologies has led to a significant increase in the availability of mobility data, specifically spatio-temporal trajectory data, which includes information about the movements (locations) of objects over time. This data has proven valuable for a wide range of applications, including predicting travel patterns, discovering routes, analyzing social interactions, and managing resources in urban environments. However, using trajectory datasets can also be challenging, particularly in different countries. This article aims to explore the challenges of using trajectory datasets, specifically using focus group discussions. Focus groups are a qualitative method of gaining a deeper understanding of a particular topic and have not been previously used to examine the challenges related to trajectory datasets. The information gathered through these discussions is augmented by a review of existing literature for a comprehensive understanding of data challenges. © 2023 ACM.',\n",
       " 'Abstract: Current networks (such as 4G and the forthcoming 5G networks) may not be capable of fully congregating quickly emerging traffic strains due to the proliferation of smart fatal, infrastructures and the explosion of diverse applications with varying necessities. As a result, 6G network research has already seen participation from both the private sector and the academic community. Recently, an innovative paradigm has emerged for the intelligent design and optimization of 6G networks based on the combination of artificial intelligence (AI) and data science (DS). Therefore, this article proposes an AI-enabled architecture for 6G networks, which is alienated into four layers: intelligent sensing, data analytics, intelligent control, and smart application, to realize patterns sighting, smart resource management, automatic network adjustment, and intelligent service provisioning. We go over the uses of DS&AI methods in 6G networks, such as AI-enhanced mobile edge computing, intelligent mobility, and smart-spectrum management, and how to implement these methods to maximize the network’s performance. We also emphasize key areas for future study and clarifications for AI-enabled 6G networks, together with computational efficiency, algorithm resilience, hardware development, and energy management. © Allerton Press, Inc. 2023.',\n",
       " '[No abstract available]',\n",
       " '[No abstract available]',\n",
       " 'Modeling the failure times of processors and memories in computers is crucial for ensuring the reliability and robustness of data science workflows. By understanding the failure characteristics of the hardware components, data scientists can develop strategies to mitigate the impact of failures on their computations, and design systems that are more fault-tolerant and resilient. In particular, failure time modeling allows data scientists to predict the likelihood and frequency of hardware failures, which can help inform decisions about system design and resource allocation. In this paper, we aimed to model the failure times of processors and memories of computers; this was performed by formulating a new type of bivariate model using the copula function. The modified extended exponential distribution is the suggested lifetime of the experimental units. It was shown that the new bivariate model has many important properties, which are presented in this work. The inferential statistics for the distribution parameters were obtained under the assumption of a Type-II censored sampling scheme. Therefore, point and interval estimation were observed using the maximum likelihood and the Bayesian estimation methods. Additionally, bootstrap confidence intervals were calculated. Numerical analysis via the Markov Chain Monte Carlo method was performed. Finally, a real data example of processors and memories failure time was examined and the efficiency of the new bivariate distribution of fitting the data sample was observed by comparing it with other bivariate models. © 2023 by the authors.',\n",
       " 'Understanding how post- acute COVID-19 syndrome (PACS or long COVID) manifests among underserved populations, who experienced a disproportionate burden of acute COVID-19, can help providers and policymakers better address this ongoing crisis. To identify clinical sequelae of long COVID among underserved populations treated in the primary care safety net, we conducted a causal impact analysis with electronic health records (EHR) to compare symptoms among community health center patients who tested positive (n=4,091) and negative (n=7,118) for acute COVID-19. We found 18 sequelae with statistical significance and causal dependence among patients who had a visit after 60 days or more following acute COVID-19. These sequelae encompass most organ systems and include breathing abnormalities, malaise and fatigue, and headache. This study adds to current knowledge about how long COVID manifests in a large, underserved population. © 2023 Meharry Medical College.',\n",
       " '[No abstract available]',\n",
       " 'The perceived importance of data in society has led to a surge in interest towards data science education. This article seeks to build on existing literature concerned with the sociopolitical, cultural, and ethical dimensions of data science education by considering the salience of two interrelated concepts discussed in Asian and Asian American studies and postcolonial studies: techno-Orientalism and the Filipino mimicry stereotype. These concepts describe the positioning of Asians and Asian Americans, and Filipinos in particular, as performance machines who are adept at copying the ideas of dominant groups but incapable of innovation themselves. This article situates techno-Orientalism and the Filipino mimicry stereotype within broader conversations around postcolonial data politics and then argues for the possibility of including these concepts in data science education curricula. Finally, this article discusses Gaskins’s techno-vernacular creativity as one path for resisting techno-Orientalist and mimicry discourses. © 2023, Association for Educational Communications & Technology.',\n",
       " 'Over recent years, positive airway pressure (PAP) remote monitoring has transformed the management of OSA and produced a large amount of data. Accumulated PAP data provide valuable and objective information regarding patient treatment adherence and efficiency. However, the majority of studies that have analyzed longitudinal PAP remote monitoring have summarized data trajectories in static and simplistic metrics for PAP adherence and the residual apnea-hypopnea index by the use of mean or median values. The aims of this article are to suggest directions for improving data cleaning and processing and to address major concerns for the following data science applications: (1) conditions for residual apnea-hypopnea index reliability, (2) lack of standardization of indicators provided by different PAP models, (3) missing values, and (4) consideration of treatment interruptions. To allow fair comparison among studies and to avoid biases in computation, PAP data processing and management should be conducted rigorously with these points in mind. PAP remote monitoring data contain a wealth of information that currently is underused in the field of sleep research. Improving the quality and standardizing data handling could facilitate data sharing among specialists worldwide and enable artificial intelligence strategies to be applied in the field of sleep apnea. © 2022 American College of Chest Physicians',\n",
       " \"Based on the assumption that the success of an organization is largely determined by the knowledge and skills of its employees, human resource (HR) departments invest considerable resources in the employee recruitment process with the aim of selecting the best, most suitable employees. Due to the high cost of the recruitment process along with its high rate of uncertainty, HR recruiters utilize a variety of methods and instruments to improve the efficiency and effectiveness of this process. Thus far, however, neurological methods, in which neurobiological signals from an examined person are analyzed, have not been utilized for this purpose. This study is the first to propose a neuro-based decision support system to classify cognitive functions into levels, whose target is to enrich the information and indications regarding the candidate along the employee recruitment processes. We first measured relevant functional and cognitive abilities of 142 adult participants using traditional computer-based assessment, which included a battery of four tests regarding executive functions and intelligence score, consistent with actual recruitment processes. Second, using electroencephalogram (EEG) technology, which is one of the dominant measurement tools in NeuroIS research, we collected the participants' brain signals by administering a resting state EEG (rsEEG) on each participant. Finally, using advanced machine and deep learning algorithms, we leveraged the collected rsEEG to classify participants' levels of executive functions and intelligence score. Our empirical analyses show encouraging results of up to 72.6% accuracy for the executive functions and up to 71.2% accuracy for the intelligence score. Therefore, this study lays the groundwork for a novel, generic (non-stimuli based) system that supports the current employee recruitment processes, that is based on psychological theories of assessing executive functions. The proposed decision support system could contribute to the development of additional medium of assessing employees remotely which is especially relevant in the current Covid-19 pandemic. While our method aims at classification rather than at explanation, our intriguing findings have the potential to push forward NeuroIS research and practice. © 2023 Elsevier B.V.\",\n",
       " '[No abstract available]',\n",
       " 'In the recent years, fire departments started to build databases containing detailed information about their interventions during fires, road accidents, and other types of incidents. Their goal is to invest this information using data analysis methods in order to better understand the trends of certain events. This could help them enhance the management of their allocated resources, which leads to a reduction in the operational costs, increase in efficiency and the overall intervention speed. Therefore, in this research paper, we investigate the possibility of predicting future incidents using machine learning algorithms that are trained on a set of data containing information on almost 200,000 interventions that happened during the last 6 years. These data, provided by the fire department in the region of Doubs, France, were not sufficient to detect patterns. Thus, we have imported additional information from external resources that we thought it would improve the accuracy of the predictions. Finally, we tested multiple machine learning algorithms and we compared their results, aiming to determine which algorithm performs better. The results look promising as we were able to predict the number of interventions for each 3 hours block for a whole year, with an acceptable error margin. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.',\n",
       " 'In recent times, the healthcare industry has been generating a significant amount of data in distinct formats, such as electronic health records (EHR), clinical trials, genetic data, payments, scientific articles, wearables, and care management databases. Data science is useful for analysis (pattern recognition, hypothesis testing, risk valuation) and prediction. The major, primary usage of data science in the healthcare domain is in medical imaging. At the same time, lung cancer diagnosis has become a hot research topic, as automated disease detection poses numerous benefits. Although numerous approaches have existed in the literature for lung cancer diagnosis, the design of a novel model to automatically identify lung cancer is a challenging task. In this view, this paper designs an automated machine learning (ML) with data science-enabled lung cancer diagnosis and classification (MLDS-LCDC) using computed tomography (CT) images. The presented model initially employs Gaussian filtering (GF)-based pre-processing technique on the CT images collected from the lung cancer database. Besides, they are fed into the normalized cuts (Ncuts) technique where the nodule in the pre-processed image can be determined. Moreover, the oriented FAST and rotated BRIEF (ORB) technique is applied as a feature extractor. At last, sunflower optimization-based wavelet neural network (SFO-WNN) model is employed for the classification of lung cancer. In order to examine the diagnostic outcome of the MLDS-LCDC model, a set of experiments were carried out and the results are investigated in terms of different aspects. The resultant values demonstrated the effectiveness of the MLDS-LCDC model over the other state-of-The-Art methods with the maximum sensitivity of 97.01%, specificity of 98.64%, and accuracy of 98.11%. © 2023 World Scientific Publishing Company.',\n",
       " \"With the emergence of smart technology and automated systems in today's world, big data is being incorporated into many applications. Trends in data can be detected and objects can be tracked based on the real-time data that is utilized in everyday life. These connected sensor devices and objects will provide a large amount of data that is to be analyzed quickly, as it can accelerate the transformation of smart technology. The accuracy of prediction of artificial intelligence (AI) systems is drastically increasing by using machine learning and other probability and statistical approaches. Big data and geospatial data help to solve complex issues and play a vital role in future applications. Emerging Trends, Techniques, and Applications in Geospatial Data Science provides an overview of the basic concepts of data science, related tools and technologies, and algorithms for managing the relevant challenges in real-time application domains. The book covers a detailed description for readers with practical ideas using AI, the internet of things (IoT), and machine learning to deal with the analysis, modeling, and predictions from big data. Covering topics such as field spectra, high-resolution sensing imagery, and spatiotemporal data engineering, this premier reference source is an excellent resource for data scientists, computer and IT professionals, managers, mathematicians and statisticians, health professionals, technology developers, students and educators of higher education, librarians, researchers, and academicians. © 2023 by IGI Global. All rights reserved.\",\n",
       " 'Sensemaking is the iterative process of identifying, extracting, and explaining insights from data, where each iteration is referred to as the \"sensemaking loop.\"However, little is known about how sensemaking behavior evolves from exploration and explanation during this process. This gap limits our ability to understand the full scope of sensemaking, which in turn inhibits the design of tools that support the process. We contribute the first mixed-method to characterize how sensemaking evolves within computational notebooks. We study 2,574 Jupyter notebooks mined from GitHub by identifying data science notebooks that have undergone significant iterations, presenting a regression model that automatically characterizes sensemaking activity, and using this regression model to calculate and analyze shifts in activity across GitHub versions. Our results show that notebook authors participate in various sensemaking tasks over time, such as annotation, branching analysis, and documentation. We use our insights to recommend extensions to current notebook environments. © 2023 Owner/Author.',\n",
       " 'The work involved in gathering, wrangling, cleaning, and otherwise preparing data for analysis is often the most time consuming and tedious aspect of data work. Although many studies describe data preparation within the context of data science workflows, there has been little research on data preparation in data journalism. We address this gap with a hybrid form of thematic analysis that combines deductive codes derived from existing accounts of data science workflows and inductive codes arising from an interview study with 36 professional data journalists. We extend a previous model of data science work to incorporate detailed activities of data preparation. We synthesize 60 dirty data issues from 16 taxonomies on dirty data and our interview data, and we provide a novel taxonomy to characterize these dirty data issues as discrepancies between mental models. We also identify four challenges faced by journalists: diachronic, regional, fragmented, and disparate data sources. © 2023 ACM.',\n",
       " 'Across academia, government, and industry, data stewards are facing increasing pressure to make datasets more openly accessible for researchers while also protecting the privacy of data subjects. Differential privacy (DP) is one promising way to offer privacy along with open access, but further inquiry is needed into the tensions between DP and data science. In this study, we conduct interviews with 19 data practitioners who are non-experts in DP as they use a DP data analysis prototype to release privacy-preserving statistics about sensitive data, in order to understand perceptions, challenges, and opportunities around using DP. We find that while DP is promising for providing wider access to sensitive datasets, it also introduces challenges into every stage of the data science workflow. We identify ethics and governance questions that arise when socializing data scientists around new privacy constraints and offer suggestions to better integrate DP and data science. © 2023 ACM.',\n",
       " 'Within the last few decades, data science has risen towards the top of agendas in public services such as adult social care. Concomitant with this ever-increasing appetite for data science is an expanding catalogue of challenges associated with developing, deploying, and maintaining data science software: choosing the appropriate, if any, analytical technique to use; balancing competing conceptions of success; and sustaining user adoption. In my thesis, I argue that these are sociotechnical challenges: issues that arise in the tension between what people do, and how data science software supports, limits, and, crucially, changes what they do. Through design science research, this thesis will develop, demonstrate, and evaluate a design approach for data science software that attempts to address such sociotechnical challenges. The empirical sites in which these research activities will take place are live data science projects with local government organisations responsible for adult social care services in England. The resultant approach will include a conceptual model for sociotechnical data science, process guidance and methods for applying the design approach, and results from applying the approach in a real-world data science project in collaboration with an industry partner. © 2023 Owner/Author.',\n",
       " 'Data science workflows are human-centered processes involving on-demand programming and analysis. While programmable and interactive interfaces such as widgets embedded within computational notebooks are suitable for these workflows, they lack robust state management capabilities and do not support user-defined customization of the interactive components. The absence of such capabilities hinders workflow reusability and transparency while limiting the scope of exploration of the end-users. In response, we developed Magneton, a framework for authoring interactive widgets within computational notebooks that enables transparent, reusable, and customizable data science workflows. The framework enhances existing widgets to support fine-grained interaction history management, reusable states, and user-defined customizations. We conducted three case studies in a real-world knowledge graph construction and serving platform to evaluate the effectiveness of these widgets. Based on the observations, we discuss future implications of employing Magneton widgets for general-purpose data science workflows. © 2023 Owner/Author.',\n",
       " \"Managing error has become an increasingly central and contested arena within data science work. While recent scholarship in artificial intelligence and machine learning has focused on limiting and eliminating error, practitioners have long used error as a site of collaboration and learning vis-à-vis labelers, domain experts, and the worlds data scientists seek to model and understand. Drawing from work in CSCW, STS, HCML, and repair studies, as well as from multi-sited ethnographic fieldwork within a government institution and a non-profit organization, we move beyond the notion of error as an edge case or anomaly to make three basic arguments. First, error discloses or calls to attention existing structures of collaboration unseen or underappreciated under 'working' systems. Second, error calls into being new forms and sites of collaboration (including, sometimes, new actors). Third, error redeploys old sites and actors in new ways, often through restructuring relations of hierarchy and expertise which recenter or devalue the position of different actors. We conclude by discussing how an artful living with error can better support the creative strategies of negotiation and adjustment which data scientists and their collaborators engage in when faced with disruption, breakdown, and friction in their work. © 2023 ACM.\",\n",
       " \"Discrete mathematics is a core discipline basic course in data science and big data technology. In the process of teaching discrete mathematics, teacher pay more attention to theoretical explanation, and neglect the correlation and connection with the professional course groups of data science and big data technology, so that students do not have enough understanding of the core basic position of discrete mathematics courses and insufficient learning motivation. Build the knowledge map of each related course in the discrete mathematics and data science and technology major course group to realize the cultivation of students' logical thinking, abstract thinking, computer thinking, modeling thinking, engineering thinking, divergent thinking and critical thinking. © 2023 ACM.\",\n",
       " 'There are significant predictive tool usages by design engineers in automotive industry to capture material composition and manufacturing process-induced variables. In specific, an accurate modeling of material behavior to predict the mechanical performance of a thermoplastic part is an evolving subject in this field as one needs to consider multiple factors and steps to achieve the right prediction accuracies. The variability in prediction comes from different factors such as polymer type (filled vs. unfilled, amorphous vs semi crystalline etc.), design and manufacturing features (weldline, gate locations, thickness, notches etc.), operating conditions (temperature, moisture etc.) and finally load states (tension, compression, flexural, impact etc.). Using traditional numerical simulation-based modelling to study and validate all these factors requires significant computational time and effort. An alternative method by using data science and AI-ML models is proposed to reduce the overall validation time needed for simulation. To validate this methodology, extensive part level experiments were done on a representative cylindrical geometry to accommodate all these factors using different ULTEM™ Resin materials (PEI). The results show that by using neural network ML model, it is possible to accurately predict the structural response like maximum displacement and force. The ML model results were compared to the CAE based approaches and results overlapped with each other well within the 95% scatter band. By combining both the CAE modelling and ML modelling it is possible to accurately predict the critical structural response of applications more efficiently and economically. © 2023 SAE International. All Rights Reserved.',\n",
       " 'This paper attempts to conceptualise the concept of data analytics for both MSMEs and large corporations. Data analytics enable organisations to learn how customers think and behave in real time, and make changes to their business as a consequence, using real-time insights about the customer base. The various misconceptions on data analytics, especially in organisational context, warrants the writing of this paper. Present day organisations are already under immense pressure due to increasingly competitive and hostile environments, and by utilizing data analytics may give them that extra edge over competitors in order to survive and grow. The objective of this chapter is to provide an overview of data analytics, and propose a conceptual framework to showcase the data science process as well as some of the most well-known and useful features of data analytics. This chapter also attempts to synthesise the various jargons for organisations to easily comprehend step-by-step, from data collection to producing useful data in order for them to enable informed decision-makings. © 2023, IGI Global.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " '[No abstract available]',\n",
       " 'Improving life expectancy, which is falling and declining globally, is getting harder, especially given the limited economic resources in most parts of the world. This study seeks to identify the most significant factors that positively and negatively affect life expectancy, separating them from less significant factors. This way, specific areas can be identified to channel scarce economic resources towards increasing life expectancy. Using data science techniques on a dataset comprising economic, immunological, health, personal, and social attributes, we have been able to achieve this. Furthermore, four machine learning tree regression algorithms were employed using the identified attributes to develop and evaluate life expectancy prediction models. The extremely randomized tree model performed best using evaluation matrices: MAE, RMSE, R2, and CV score. This research can help governments, especially in low-income, developing countries, make better decisions and investments, as well as help digital health experts develop technologies that could improve life expectancy. © 2023 The Author(s)',\n",
       " 'Continuous developments in data science have brought forth an exponential increase in complexity of machine learning models. Additionally, data scientists have become ubiquitous in the private market and academic environments. All of these trends are on a steady rise, and are associated with an increase in power consumption and associated carbon footprint. The increasing carbon footprint of large-scale advanced data science has already received attention, but the latter trend has not. This work aims to estimate the contribution of the increasingly popular “common” data science to the global carbon footprint. To this end, the power consumption of several typical tasks in the aforementioned common data science tasks are measured and compared to: large-scale “advanced” data science, common computer-related tasks, and everyday non-computer related tasks. An automated data science project is also run on various hardware architectures. To assess its sustainability in terms of carbon emission, the measurements are converted to gCO2eq and an equivalent unit of “km driven by car”. Our main findings are: “common” data science consumes 2.57 more power than regular computer usage, but less than some common everyday power-consuming tasks such as lighting or heating; advanced data science consumes substantially more power than common data science, and can be either on par or vastly surpass common everyday power-consuming tasks, depending on the scale of the project. In addition to the reporting of these results, this work also aims to inspire researchers to include power usage and estimated carbon emission as a secondary result in their work. © 2023',\n",
       " '[No abstract available]',\n",
       " 'When there are differences in research objects and methodology between data science and information science, there are also linkages between data science and information science, based on the DIKW hierarchy to the concept chain, namely data – information – knowledge – wisdom. While knowledge metrics provides a quantitative linkage of data – information – knowledge – wisdom, information is the logarithm of data and knowledge is the logarithm of information, on which the mechanism of Brookes’ basic equation of information science is revealed. We suggest to maintain similar principles of data science and information science, including the principle of order, the principle of correlation, the principle of reorganized transformation, the principle of scatter distribution, the principle of logarithmic perspective, and the principle of least effort. Also, we extend to discuss a few issues on knowledge science. © 2023',\n",
       " 'Information science and data science are closely related, but the relationships and synergies between them may not be sufficiently addressed in educational curricula for information professionals. Consequently, information professionals are at risk of being perceived as having little relevance in data-intensive settings and may fail to demonstrate the unique contribution that they can make in such environments. The knowledge and skills that information professionals can bring relate to the social, cultural, and ethical dimensions of data that are essential to recognise for successful data governance. If information professionals are not actively engaged with data initiatives, then they may be ceding their professional jurisdiction to other occupations. © 2023 The Author(s)',\n",
       " 'While data science and information science emerged as two separate disciplines with different roots, in the recent past, they have been getting integrated and intertwined in interesting and impactful ways. The traditional distinction between data and information does not easily explain the differences and overlaps between the two sciences named after them. If one claims, for instance, that information is ‘meaningful data’ then it is important to note that a main objective of data science is indeed to derive meaningful information out of data. Information science is not necessarily a superset or a higher level of data science. Both of these disciplines have earned their place in sciences through different pasts, paths, and possibilities. Keeping that in mind, they are discussed here while tracing their origins and understanding their positionalities in the current context. More than the past and the present, what becomes then important is where they are heading next. Several suggestions are provided to keep data science a meaningful offering within information science – as a uniqueness for the former with the strengths of the latter. © 2023 The Author',\n",
       " 'The rapid emergence of data science as a field has made it a rival or replacement for information science from an industry perspective. In particular, the “big data” meme in data science and a heavy reliance on “black box” technology emphasize the quantity of data used in a project and asks, “what data do we have” rather than “what data do we need to solve our business problems.” This perspective also undermines the perceived importance of domain expertise, user research, data semantics and provenance, and other considerations valued in information science. This article uses a composite (and somewhat caricatured) case study of a data science project and discusses seven ways in which it is destined to fail, and then explains how “good information science” would have prevented or ameliorated them. Data science and information science need to recognize that together they can accomplish more than they can accomplish separately. © 2023 The Author',\n",
       " 'Addressing societal issues in civil and environmental engineering increasingly requires skills in data science and programming. To date, there is not much known about the extent students are learning these skills in current civil and environmental engineering curricula. We conducted a survey of accredited civil and environmental engineering departments in the US regarding their current curricula with respect to programming and data science. Our response rate was 41% (power of 0.89 at 0.05 significance level). The results show limited incorporation of the modern data science languages (Java, Python, and R) into civil and environmental curricula, and significant use of MATLAB and spreadsheets. Although department chairs see the value of incorporating modern data science languages and facilitating student experience in the broader data science skills (e.g., understanding privacy issues, etc.) into their curricula, barriers such as faculty knowledge and lack of space in the curricula remain significant hurdles. © 2022 American Society of Civil Engineers.',\n",
       " 'Tools supporting the teaching and learning of programming may help professors in correcting assignments and students in receiving immediate feedback, thus improving the solution before the final submission. This paper describes the rDSA tool, which was designed, developed, and evaluated to support students in completing assignments concerning (i) the execution of statistical analyses in the R language and (ii) commenting on the results in natural language. The paper focuses on the feedback provided by the tool to students and how it was designed/evaluated/improved over the years. The paper also presents the results of two studies that indicate the advantages of using the tool in terms of engagement and learning outcomes. To conclude, we provide a discussion on the characteristics of the tool, its use in similar courses, and the scope for future work. © 2022, The Author(s).',\n",
       " 'Artificial intelligence (AI) has been widely adopted in higher education. However, the current research on AI in higher education is limited lacking both breadth and depth. The present study fills the research gap by exploring faculty members’ perception on teaching AI and data science related courses facilitated by an open experiential AI platform. Specifically, two focus groups are conducted among computer science and non-computer science faculty members to gauge their perception on the integration of AI in an experiential learning platform to teach data science, as well as their perception on AI powered data science curriculum in higher education. Findings reveal three major themes which are defining data science, assembling interdisciplinary teams, and building platform for connection. The study has both theoretical and practical implications. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.',\n",
       " 'The data science era is characterized by data-driven automated decision systems\\xa0(ADS) enabling, through data analytics and machine learning, automated decisions in many contexts, deeply impacting our lives. As such, their downsides and potential risks are becoming more and more evident: technical solutions, alone, are not sufficient and an interdisciplinary approach is needed. Consequently, ADS should evolve into\\xa0data-informed ADS,\\xa0which take\\xa0humans in the loop\\xa0in all the data processing steps. Data-informed ADS should deal with data responsibly, guaranteeing nondiscrimination\\xa0with respect to protected groups of individuals. Nondiscrimination can be characterized in terms of different types of properties, like fairness and diversity. While fairness, i.e., absence of bias against minorities, has been widely investigated in machine learning, only more recently this issue has been tackled by considering all the steps of data processing pipelines at the basis of ADS, from data acquisition to analysis. Additionally, fairness is just one point of view of nondiscrimination to be considered for guaranteeing equity: other issues, like diversity, are raising interest from the scientific community due to their relevance in society. This paper aims at critically surveying how nondiscrimination has been investigated in the context of complex data science pipelines at the basis of data-informed ADS, by focusing on the specific data processing tasks for which nondiscrimination solutions have been proposed. © 2022, The Author(s).',\n",
       " 'The paper considers the nature of input data used by Data Science algorithms of modern-day application domains. It then proposes three algorithms designed to remove statistical anomalies from datasets as a part of the Data Science pipeline. The main advantages of given algorithms are their relative simplicity and a small number of configurable parameters. Parameters are determined by machine learning with respect to the properties of input data. These algorithms are flexible and have no strict dependency on the nature and origin of data. The efficiency of the proposed approaches is verified with a modeling experiment conducted using algorithms implemented in Python. The results are illustrated with plots built using raw and processed datasets. The algorithms application is analyzed, and results are compared. © O. Pysarchuk, D. Baran, Yu. Mironov, I. Pysarchuk, 2023.',\n",
       " 'A question is present nowadays in academic and research arenas: as we manage data, adopting several different processes and methods, as this management relates to other fields, so different and identifiable, can data science be considered as a scientific field? The chapter develops a study around this question, observing how, potentially, this field was formed and continues to evolve, gaining new definitions and practical views, pressured by the massive adoption of technological resources to manage data and a progressive level of demand of products and services which are obtained from data, as those regarded as information or knowledge outcome. The study was conducted to appreciate the possible characteristic of multi-, inter-, and trans-disciplinary aspects for this scientific field, aiming to understand this perspective as a mature characteristic of a scientific field. These arguments were checked, which form the data science context, determining a final level of perception for this study, that considered data science as a new, innovative, in formation scientific field. Copyright © 2023, IGI Global.',\n",
       " 'In this chapter, concepts of data science definition and market business agility with strong support of digital transformation paradigm are discussed. The intention is to provide the reader with a consistent, proposedly not finished view of this powerful, actual, and potential association, which presents a new fashion for entrepreneurship, leading also to organizational changes which imply, in their way, a redefinition of professional works, technology application, risk and project management, and strategic planning. The chapter describes these concepts, develops their relationships, and leaves an \"open door\" for the reader to perceive not only the conceptual application for competitive scenario description, but also as an associative and integrative methodology to be used, beyond other perspectives, to appreciate the evolution of the context developed by the chapter itself, updating it in the time ahead. In the final part of the text, study cases are discussed as to affirm chapter objectives, findings, and proposed integrative base. Copyright © 2023, IGI Global.',\n",
       " 'Data science in marketing has become critical in gaining sustained competitive advantage in a rapidly changing business environment. It involves using advanced analytics and scientific principles to extract valuable information from large volumes of data gathered from multiple sources, such as social media platforms. There are multiple benefits to using data science in marketing, including proper data-based planning, enhanced customization, enhanced forecasting through predictive analytics, effective ROI measuring, and improved pricing models. The research explains how companies can turn the potential and opportunities of these advanced analytics techniques into real company performance in a competitive marketing environment. This research aims to explore how firms can use marketing analytics and big data to improve capabilities and performance. Specifically, the study argues that big data and marketing analytics can be used to extract valuable and meaningful marketing information and insights that can be integrated to improve marketing effectiveness and performance. Copyright © 2023, IGI Global.',\n",
       " 'This chapter consists of an introduction of the study of enhancing business communication and collaboration through data science applications in the current context. Organizations are highly complex and ever-changing environments that need to be supported by solid and reliable information. With rapid technological and scientific advances, it is imperative that organizations adopt a policy of using technological methods to ensure the prosperity and continuity of their business. Copyright © 2023, IGI Global.',\n",
       " 'Data science applications have the potential to offer improvements in business communication and collaboration through data science applications. In view of this premise, this chapter analyzes the behavioral changes of the drivers resulting from the implantation of a telemetry tool. A case study was carried out in a medium-sized company in the logistics and cargo transportation segment in the Belo Horizonte, Brazil. The technique used for data collection was documental. This implementation allowed identifying a radical change in drivers\\' behavior, resulting in reduced average speed, abrupt acceleration, abrupt braking, excessive g-force cornering, and the use of \"banguela\" (coasting). In this way, the tool proved to be a competitive advantage since this change in behavior increased employee safety, eliminated costs, and improved the company\\'s image before the population. Copyright © 2023, IGI Global.',\n",
       " 'Digital evolution has become increasingly present in our lives, whether on cellphones, computers, watches, or other appliances. As a result of the wide access we have to the digital world, the amount of data generated daily is vast. This density of information generated at every moment can be the insight needed for the success of an organization. Much is said about data-based decision-making to generate the best results. The new capabilities of data intelligence unleashed by the emergence of cloud computing and artificial intelligence make it one of the most promising areas of digital transformation change management. Enhancing Business Communications and Collaboration Through Data Science Applications provides relevant theoretical frameworks and the latest empirical research findings in the area. It is written for professionals who wish to improve their understanding of the strategic role of trust at different levels of the information and knowledge society. Covering topics such as data science, online business communication, and user-centered design, this premier reference source is an ideal resource for business managers and leaders, entrepreneurs, data scientists, data analysts, sociologists, students and educators of higher education, librarians, researchers, and academicians. © 2023 by IGI Global. All rights reserved.',\n",
       " '[No abstract available]',\n",
       " 'Underwater environments are emerging as a new frontier for data science thanks to an increase in deployments of underwater sensor technology. Challenges in operating computing underwater combined with a lack of high-speed communication technology covering most aquatic areas means that there is a significant delay between the collection and analysis of data. This in turn limits the scale and complexity of the applications that can operate based on these data. In this article, we develop underwater fog computing support using low-cost micro-clouds and demonstrate how they can be used to deliver cost-effective support for data-heavy underwater applications. We develop a proof-of-concept micro-cloud prototype and use it to perform extensive benchmarks that evaluate the suitability of underwater micro-clouds for diverse underwater data science scenarios. We conduct rigorous tests in both controlled and field deployments, using river and sea waters. We also address technical challenges in enabling underwater fogs, evaluating the performance of different communication interfaces and demonstrating how accelerometers can be used to detect the likelihood of communication failures and determine which communication interface to use. Our work offers a cost-effective way to increase the scale and complexity of underwater data science applications, and demonstrates how off-the-shelf devices can be adopted for this purpose. © 2023 Association for Computing Machinery.',\n",
       " '[No abstract available]',\n",
       " 'Interest in data science has been growing across industries - both STEM and non-STEM. Non-STEM students often have difficulties with programming and data analysis tools. These entry barriers can be minimized, and these concepts can be easily absorbed when using visual tools. Thus, for this specific audience, the use of visual tools has been essential for teaching data science. Several of these tools are available, but they all have limitations. This work presents Blockly-DS: a new tool capable of assisting in teaching data science to a non-STEM audience. The Blockly-DS tool is being tested in two Brazilian higher education institutions, one, IBMEC, a business undergraduate university, and the other, FIAP, a STEM school that offers an MBA as well as corporate and undergraduate courses. The preliminary results presented in this article refers to a validation with two groups of training sessions for junior financial analysts of a major Brazilian bank in partnership with FIAP. © 2023 ACM.',\n",
       " 'The Data Science Academy (DSA) is an extra-curricular program for 7-12th graders that has evolved over the last four years with valuable lessons learned along the way. DSA was created in part because the K-12 curriculum is already packed, so deploying this informal learning environment and outreach program was one way to meet the challenge of the CS4ALL initiative and broaden participation in computing. DSA serves multiple purposes in that regard: it teaches teachers, it gives undergraduates mentoring experience, and it provides a platform for educational research and development. Currently, the DSA comprises five teaching modules, which have been repackaged and delivered as quarter or semester long weekend sessions, or shorter intensive summer programs. That is to say, the DSA accommodates flexible formats, including virtual or in-person ones and soon asynchronous options as well. The DSA provides an opportunity to develop and test lesson modules, including those derived from research projects in partnership with DS-PATH participants (an NSF project to create DS Pathways). Ultimately, the goal is to apply our experience with the DSA in order to expand the K-12 curriculum with new Data Science courses (grades 9-12), modules and pallets (grades 6-8), which is the next phase of our project. © 2022 Owner/Author.',\n",
       " 'Data Science is a growing, interdisciplinary field that intersects with multiple academic, professional, and industry domains. Universities are increasingly offering data science programs across a wide variety of departments and program areas. It remains unclear, however, whether any standardization of missions, learning outcomes, or course offerings have emerged. Our research provides a systematic review of 788 data science program offerings, focusing on how these programs frame their missions and objectives; the types of courses offered within their programs; and whether issues of the ethical and social dimensions of data are addressed. © 2022 Owner/Author.',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " 'The study of plant metabolism is one of the most complex tasks, mainly due to the huge amount and structural diversity of metabolites, as well as the fact that they react to changes in the environment and ultimately influence each other. Metabolic profiling is most often carried out using tools that include mass spectrometry (MS), which is one of the most powerful analytical methods. All this means that even when analyzing a single sample, we can obtain thousands of data. Data science has the potential to revolutionize our understanding of plant metabolism. This review demonstrates that machine learning, network analysis, and statistical modeling are some techniques being used to analyze large quantities of complex data that provide insights into plant development, growth, and how they interact with their environment. These findings could be key to improving crop yields, developing new forms of plant biotechnology, and understanding the relationship between plants and microbes. It is also necessary to consider the constraints that come with data science such as quality and availability of data, model complexity, and the need for deep knowledge of the subject in order to achieve reliable outcomes. © 2023 by the authors.',\n",
       " 'This article proposes a novel approach that leverages graph theory, machine learning, and graph embedding to evaluate research groups comprehensively. Assessing the performanceand impact of research groups is crucial for funding agencies and research institutions, but many traditional methods often fail to capture the complex relationships between the evaluated elements.In this sense, our methodology transforms publication data into graph structures, allowing the visualization and quantification of relationships between researchers, publications, and institutions.By incorporating symmetry properties, we offer a more in-depth evaluation of research groups cohesiveness and structure over time. This temporal evaluation methodology bridges the gap between unstructured scientometrics networks and the evaluation process, making it a valuable tool for decision-making procedures. A case study is defined to demonstrate the potential to providevaluable insights into the dynamics and limitations of research groups, which ultimately reinforces the feasibility of the proposed approach when supporting decision making for funding agencies andresearch institutions. © 2023 by the authors.',\n",
       " 'As data grows exponentially across diverse fields, the ability to effectively leverage big data has become increasingly crucial. In the field of data science, however, minority groups, including African Americans, are significantly underrepresented. With the strategic role of minority-serving institutions to enhance diversity in the data science workforce and apply data science to health disparities, the National Institute for Minority Health Disparities (NIMHD) provided funding in September 2021 to six Research Centers in Minority Institutions (RCMI) to improve their data science capacity and foster collaborations with data scientists. Meharry Medical College (MMC), a historically Black College/University (HBCU), was among the six awardees. This paper summarizes the NIMHD-funded efforts at MMC, which include offering mini-grants to collaborative research groups, surveys to understand the needs of the community to guide project implementation, and data science training to enhance the data analytics skills of the RCMI investigators, staff, medical residents, and graduate students. This study is innovative as it addressed the urgent need to enhance the data science capacity of the RCMI program at MMC, build a diverse data science workforce, and develop collaborations between the RCMI and MMC’s newly established School of Applied Computational Science. This paper presents the progress of this NIMHD-funded project, which clearly shows its positive impact on the local community. © 2023 by the authors.',\n",
       " '[No abstract available]',\n",
       " ...]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_extract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>publicationName</th>\n",
       "      <th>cover_date</th>\n",
       "      <th>scopus_id</th>\n",
       "      <th>cited_by_count</th>\n",
       "      <th>open_access</th>\n",
       "      <th>eid</th>\n",
       "      <th>aggregationType</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>link</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analysing trends of computational urban scienc...</td>\n",
       "      <td>Kumar D.</td>\n",
       "      <td>Computational Urban Science</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>SCOPUS_ID:85209789532</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2-s2.0-85209789532</td>\n",
       "      <td>Journal</td>\n",
       "      <td>[{'name': 'State University of New York Albany...</td>\n",
       "      <td>https://www.scopus.com/inward/record.uri?partn...</td>\n",
       "      <td>Urban computing with a data science approaches...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A landmark federal interagency collaboration t...</td>\n",
       "      <td>Justice A.C.</td>\n",
       "      <td>JAMIA Open</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>SCOPUS_ID:85208963031</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2-s2.0-85208963031</td>\n",
       "      <td>Journal</td>\n",
       "      <td>[{'name': 'VA Connecticut Healthcare System', ...</td>\n",
       "      <td>https://www.scopus.com/inward/record.uri?partn...</td>\n",
       "      <td>Objectives: In 2016, the Department of Veteran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Regional planning: A failed or flawed project ...</td>\n",
       "      <td>Chirisa I.</td>\n",
       "      <td>Regional Science Policy and Practice</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>SCOPUS_ID:85208099811</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2-s2.0-85208099811</td>\n",
       "      <td>Journal</td>\n",
       "      <td>[{'name': 'University of the Free State', 'cit...</td>\n",
       "      <td>https://www.scopus.com/inward/record.uri?partn...</td>\n",
       "      <td>This paper explores regional planning as an ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science and Model Predictive Control:: A ...</td>\n",
       "      <td>Morato M.M.</td>\n",
       "      <td>Journal of Process Control</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>SCOPUS_ID:85207933325</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2-s2.0-85207933325</td>\n",
       "      <td>Journal</td>\n",
       "      <td>[{'name': 'Université Grenoble Alpes', 'city':...</td>\n",
       "      <td>https://www.scopus.com/inward/record.uri?partn...</td>\n",
       "      <td>Model Predictive Control (MPC) is an establish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Assessment of the relationship between central...</td>\n",
       "      <td>Akabane S.</td>\n",
       "      <td>Scientific Reports</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>SCOPUS_ID:85207210995</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2-s2.0-85207210995</td>\n",
       "      <td>Journal</td>\n",
       "      <td>[{'name': 'The University of Tokyo Hospital', ...</td>\n",
       "      <td>https://www.scopus.com/inward/record.uri?partn...</td>\n",
       "      <td>Purpose The relationship between the height of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        author  \\\n",
       "0  Analysing trends of computational urban scienc...      Kumar D.   \n",
       "1  A landmark federal interagency collaboration t...  Justice A.C.   \n",
       "2  Regional planning: A failed or flawed project ...    Chirisa I.   \n",
       "3  Data Science and Model Predictive Control:: A ...   Morato M.M.   \n",
       "4  Assessment of the relationship between central...    Akabane S.   \n",
       "\n",
       "                        publicationName  cover_date              scopus_id  \\\n",
       "0           Computational Urban Science  2024-12-01  SCOPUS_ID:85209789532   \n",
       "1                            JAMIA Open  2024-12-01  SCOPUS_ID:85208963031   \n",
       "2  Regional Science Policy and Practice  2024-12-01  SCOPUS_ID:85208099811   \n",
       "3            Journal of Process Control  2024-12-01  SCOPUS_ID:85207933325   \n",
       "4                    Scientific Reports  2024-12-01  SCOPUS_ID:85207210995   \n",
       "\n",
       "   cited_by_count  open_access                 eid aggregationType  \\\n",
       "0               0         True  2-s2.0-85209789532         Journal   \n",
       "1               0         True  2-s2.0-85208963031         Journal   \n",
       "2               0         True  2-s2.0-85208099811         Journal   \n",
       "3               0        False  2-s2.0-85207933325         Journal   \n",
       "4               0         True  2-s2.0-85207210995         Journal   \n",
       "\n",
       "                                        affiliations  \\\n",
       "0  [{'name': 'State University of New York Albany...   \n",
       "1  [{'name': 'VA Connecticut Healthcare System', ...   \n",
       "2  [{'name': 'University of the Free State', 'cit...   \n",
       "3  [{'name': 'Université Grenoble Alpes', 'city':...   \n",
       "4  [{'name': 'The University of Tokyo Hospital', ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.scopus.com/inward/record.uri?partn...   \n",
       "1  https://www.scopus.com/inward/record.uri?partn...   \n",
       "2  https://www.scopus.com/inward/record.uri?partn...   \n",
       "3  https://www.scopus.com/inward/record.uri?partn...   \n",
       "4  https://www.scopus.com/inward/record.uri?partn...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Urban computing with a data science approaches...  \n",
       "1  Objectives: In 2016, the Department of Veteran...  \n",
       "2  This paper explores regional planning as an ap...  \n",
       "3  Model Predictive Control (MPC) is an establish...  \n",
       "4  Purpose The relationship between the height of...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstract'] = batch_extract\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly pick articles\n",
    "def pick_random_articles(articles, total=1000):\n",
    "    if len(articles) < total:\n",
    "        print(f\"Warning: Only {len(articles)} articles available. Returning all.\")\n",
    "        return articles\n",
    "    return random.sample(articles, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 11)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Example text\n",
    "text = \"Data Science in Supporting Hotel Management: An Application of Deep Learning Techniques in 2024!\"\n",
    "\n",
    "# Define a regular expression pattern to extract words\n",
    "words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "# Remove stopwords by checking if each word is not in the stopword list\n",
    "filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "# Display the filtered words\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_title  \\\n",
      "0    [Data, Science, Supporting, Hotel, Management,...   \n",
      "1    [Data, science, Analysis, Using, Deep, Learnin...   \n",
      "2    [Data, science, based, reconstruction, D, memb...   \n",
      "3    [Data, science, sustainable, entrepreneurship,...   \n",
      "4    [Applying, data, science, methodologies, artif...   \n",
      "..                                                 ...   \n",
      "995  [Data, Science, literacy, future, security, in...   \n",
      "996  [Predictive, modeling, heat, formation, sulfur...   \n",
      "997  [Data, Science, Model, Predictive, Control, su...   \n",
      "998  [Machine, learning, data, science, techniques,...   \n",
      "999  [Harnessing, Data, Science, Produced, Water, E...   \n",
      "\n",
      "                               cleaned_publicationName  \n",
      "0           [Smart, Innovation, Systems, Technologies]  \n",
      "1    [International, Conference, Knowledge, Enginee...  \n",
      "2                         [Journal, Membrane, Science]  \n",
      "3         [Technological, Forecasting, Social, Change]  \n",
      "4               [American, Journal, Medical, Genetics]  \n",
      "..                                                 ...  \n",
      "995  [Journal, Policing, Intelligence, Counter, Ter...  \n",
      "996                [European, Physical, Journal, Plus]  \n",
      "997                        [Journal, Process, Control]  \n",
      "998  [Machine, Learning, Data, Science, Techniques,...  \n",
      "999  [Society, Petroleum, Engineers, SPE, Water, Li...  \n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Extract words (alphabetic characters only)\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
    "    return filtered_words # Join words back into a string\n",
    "\n",
    "# Apply the clean_text function to the 'title' and 'name' columns\n",
    "df['cleaned_title'] = df['title'].apply(lambda x: clean_text(str(x)))\n",
    "df['cleaned_publicationName'] = df['publicationName'].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "print(df[['cleaned_title', 'cleaned_publicationName']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"abstract_draft1.csv\"\n",
    "df.to_csv(output_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
